# Run Single Node GRPO for llama3.1-8b workloads on A4 GKE Node pools with Nvidia NeMo RL Framework

This recipe outlines the steps for running a llama3.1-8b GRPO workload
on [A4 GKE Node pools](https://cloud.google.com/kubernetes-engine) by using the
[NVIDIA NeMo framework](https://github.com/NVIDIA-NeMo/RL).

## Orchestration and deployment tools

For this recipe, the following setup is used:

- Orchestration - [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)
- NemoRL configuration and deployment - A Helm chart is used to configure and deploy
  the [Kubernetes Jobset](https://kubernetes.io/blog/2025/03/23/introducing-jobset)
  resource which manages the execution of the
  [NeMo RL workload](https://github.com/NVIDIA-NeMo/RL).

## Test environment

This recipe has been tested with the following configuration:

- GKE cluster
    - [A regional standard cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview) version: 1.31.7-gke.1265000 or later.
    - A GPU node pool at least 1
    [a4-highgpu-8g](https://cloud.google.com/compute/docs/accelerator-optimized-machines#a4-high-vms) provisioned using the DENSE deployment type.
    - [Workload Identity Federation for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity) enabled.
    - [Cloud Storage FUSE CSI driver for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/cloud-storage-fuse-csi-driver) enabled.
    - [DCGM metrics](https://cloud.google.com/kubernetes-engine/docs/how-to/dcgm-metrics) enabled.
    - [Kueue](https://kueue.sigs.k8s.io/docs/reference/kueue.v1beta1/) and [JobSet](https://jobset.sigs.k8s.io/docs/overview/) APIs installed.
    - Kueue configured to support [Topology Aware Scheduling](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/).
- A regional Google Cloud Storage (GCS) bucket to store logs generated by the recipe runs.

To prepare the required environment, see
[GKE environment setup guide](../../../../docs/configuring-environment-gke-a4.md).

## Training dataset

This recipe uses nvidia/OpenMathInstruct2Dataset dataset

## Docker container image

This recipe uses the following docker image:
`us-central1-docker.pkg.dev/deeplearning-images/reproducibility/pytorch-gpu-nemo-nccl:nemo-rl-nemo25.04`.

This image is based on NVIDIA NeMo 25.04 and contains the NCCL gIB plugin
v1.0.5, bundling all NCCL binaries validated for use with A4 GPUs.

## Run the recipe

From your client workstation, complete the following steps:

## Install KubeRay Operator
Nemo RL use Ray as an orchestrator on top of GKE. In your terminal, run the following to install kuberay operator:
```
helm repo add kuberay https://ray-project.github.io/kuberay-helm/
helm repo update
helm install kuberay-operator kuberay/kuberay-operator
```

To check if your kuberay operator is installed, run the following command:

```
kubectl get pods | grep kuberay-operator
```

You should see a pod (e.g., kuberay-operator-...) in the Running state.

### Configure environment settings

Set the environment variables to match your environment:

 ```bash
 export PROJECT_ID=<PROJECT_ID>
 export CLUSTER_REGION=<CLUSTER_REGION>
 export CLUSTER_NAME=<CLUSTER_NAME>
 ```

Replace the following values:

 - `<PROJECT_ID>`: your Google Cloud project ID.
 - `<CLUSTER_REGION>`: the region where your cluster is located.
 - `<CLUSTER_NAME>`: the name of your GKE cluster.

Set the default project:

 ```bash
 gcloud config set project $PROJECT_ID
 ```

### Get the recipe

Clone the `gpu-recipes` repository and set a reference to the recipe folder.

```
git clone https://github.com/ai-hypercomputer/gpu-recipes.git
cd gpu-recipes
```

### Get cluster credentials

```
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION
```

### Configure and submit a NemoRL GRPO job

#### Start a ray cluster

Nemo RL requires ray as an orchestrator. To start a ray cluster, refer to launch-ray-cluster.sh.
This launcher uses the template files in this folder to start a ray cluster.

The replica can be adapted to fit your workloads scale.

```
source launch-ray-cluster.sh
```

Just hold on a bit while the Ray cluster starts up. To check the status, run this command:

```
kubectl get pods | grep ray-cluster
```

Once everything is correctly set up, you should see '2/2 Running' for all the pods (head and workers), like this example:

```
ray-cluster-kuberay-head-l7bw6                          2/2     Running     0          106s
ray-cluster-kuberay-worker-grp-0-worker-r264t           2/2     Running     0          106s
ray-cluster-kuberay-worker-grp-0-worker-xwpln           2/2     Running     0          106s
```
#### Launch a 4 node node llama 3.1 8b GRPO workload 

Once your Ray cluster is totally up and running, you can start submitting your jobs.
Check out the submit_llama3.1-8b.sh script for example commands, fill in your actual HF and WandB keys

```
source submit_llama3.1-8b.sh
```

You can customize the job by directly modifying the configurations in the submission script. For example, just change cluster.num_nodes to 2 to use two nodes.


### Monitor the job

To check the status of pods in your job, run the following command:

```
kubectl get pods | grep JOB_NAME_PREFIX
```

Replace the following:
- JOB_NAME_PREFIX - your job name prefix. For example ray-cluster.

To get the logs for one of the pods, run the following command:

```
kubectl logs POD_NAME
```

Information about the training job's progress, including crucial details such as loss,
step count, and step time, is generated by the rank 0 process.
This process runs on the pod whose name begins with `JOB_NAME_PREFIX-workload-0-0`.
For example: `ray-cluster-kuberay-worker-grp-0-worker-xwpln `.

### Analyze results

The output will be visible directly in your pod's logs. It is also sent to wandb if logger.wandb_enabled is set to True in the submission script.

The logs in your pod should look like this for each step: 

```
▶ Processing rewards...
▶ Computing advantages...
▶ Preparing for logprob inference...
▶ Computing logprobs...
▶ Preparing for training...
▶ Training policy...
Logged data to logs/exp_001/train_data_step3.jsonl

📊 Training Results:
  • Loss: 0.0502
  • Avg Reward: 0.0879
  • Mean Generation Length: 314.4863

⏱️  Timing:
  • Total step time: 10.82s
  • generation: 5.61s (51.9%)
  • prepare_for_generation: 1.95s (18.1%)
  • policy_training: 1.36s (12.6%)
  • policy_and_reference_logprobs: 0.80s (7.4%)
  • logprob_inference_prep: 0.55s (5.1%)
  • training_prep: 0.45s (4.2%)
  • data_processing: 0.06s (0.6%)
  • reward_calculation: 0.01s (0.1%)

========================= Step 5/29687 =========================
▶ Preparing batch...
▶ Generating responses for batch of size 512...
```

### Troubleshooting

This section provides guidance on troubleshooting issues with the training job.

To check the status of the job's pods, use the following command:

```bash
kubectl get pods | grep JOB_NAME_PREFIX
```

Replace `JOB_NAME_PREFIX` with the prefix of your job name. For example, `$USER-mixtral-8x7b-nemo`. This command will list all pods associated with the specified job, along with their current status.


To get the logs from a specific pod, use the following command:

```bash
kubectl logs POD_NAME
```

Replace `POD_NAME` with the name of the pod you want to inspect.

In this recipe, the training job is orchestrated by the [Kubernetes JobSet](https://jobset.sigs.k8s.io/docs/overview/). If the JobSet encounters a fatal failure, it removes all pods, making it impossible to inspect their logs directly. To analyze logs from a failed job, retrieve them from Cloud Logging using the following filter:

```
resource.type="k8s_container"
resource.labels.project_id="PROJECT_ID"
resource.labels.location="CLUSTER_REGION"
resource.labels.cluster_name="CLUSTER_NAME"
resource.labels.namespace_name="default"
resource.labels.pod_name=~"^JOB_NAME_PREFIX.*"
severity>=DEFAULT
```

Replace the following:
- `PROJECT_ID`: your Google Cloud project ID.
- `CLUSTER_REGION`: the region where your cluster is located.
- `CLUSTER_NAME`: the name of your GKE cluster.
- `JOB_NAME_PREFIX`: the prefix of your job name (e.g., `ray-cluster`).

This filter will retrieve logs from all containers within pods that match the job with the specified name prefix.


### Uninstall the Helm release

You can delete the job and other resources created by the Helm chart. To
uninstall Helm, run the following command from your client:

```bash
helm uninstall <your workload>
```
