# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ $timestamp := now | date "2006-01-02-15-04-05" }}
{{ $jobSuffix := randAlphaNum 4 | lower }}
{{ $jobuuid := uuidv4 }}
{{ $nodes := div .Values.workload.gpus 8 | max 1 }}
{{ $gpusPerNode := min .Values.workload.gpus 8 }}
{{ $jobName := printf "%s-%s" .Release.Name $jobSuffix }}
{{- $root := . -}}

apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: "{{ $jobName }}"
  namespace: default
  labels:
  {{- if $root.Values.queue }}
    kueue.x-k8s.io/queue-name: "{{ $root.Values.queue }}"
  {{- end }}
spec:
  {{- if $root.Values.queue }}
  suspend: true
  {{- end }}
  failurePolicy:
    maxRestarts: {{ default 0 $root.Values.workload.max_workload_restarts }}
  replicatedJobs:
  - name: workload
    replicas: 1
    template:
      spec:
        parallelism: {{ $nodes }}
        completions: {{ $nodes }}
        backoffLimit: 0
        completionMode: Indexed
        ttlSecondsAfterFinished: 43200
        template:
          metadata:
            annotations:
              kubectl.kubernetes.io/default-container: workload
              {{- if $root.Values.volumes.gcsVolumes }}
              gke-gcsfuse/volumes: "true"
              gke-gcsfuse/cpu-limit: "0"
              gke-gcsfuse/memory-limit: "0"
              gke-gcsfuse/ephemeral-storage-limit: "0"
              {{- end }}
              {{- if $root.Values.volumes.psVolumes }}
              gke-parallelstore/volumes: "true"
              gke-parallelstore/cpu-limit: "0"
              gke-parallelstore/memory-limit: "0"
              {{- end }}
              {{- if and $root.Values.queue $root.Values.tasSettings.topologyRequest }}
              {{- toYaml .Values.tasSettings.topologyRequest | nindent 14 }}
              {{- end }}
              {{- if and $root.Values.queue $root.Values.dwsSettings.maxRunDurationSeconds }}
              provreq.kueue.x-k8s.io/maxRunDurationSeconds: "{{ $root.Values.dwsSettings.maxRunDurationSeconds }}"
              {{- end }}
              {{- if not $root.Values.network.hostNetwork }}
              networking.gke.io/default-interface: "eth0"
              networking.gke.io/interfaces: |
              {{- if $root.Values.network.subnetworks }}
                [
                  {{- range $i, $subnetwork := $root.Values.network.subnetworks }}
                  {"interfaceName":"eth{{ $i }}","network":"{{ $subnetwork }}"}{{ eq $i 9 | ternary "" ","}}
                  {{- end }}
                ]
              {{- else }}
                [
                  {"interfaceName":"eth0","network":"default"},
                  {"interfaceName":"eth1","network":"gvnic-1"},
                  {{- range  $i := until 8 }}
                  {"interfaceName":"eth{{ add 2 $i }}","network":"rdma-{{ $i }}"}{{ eq $i 7 | ternary "" ","}}
                  {{- end }}
                ]
              {{- end }}
              {{- end }}
          spec:
            {{- if $root.Values.network.hostNetwork }}
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            {{- end }}
            restartPolicy: Never
            {{ if $root.Values.targetNodes }}
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      {{- range $hostname := $root.Values.targetNodes }}
                      - {{ $hostname }}
                      {{- end }}
            {{ end }}
            tolerations:
            - operator: "Exists"
              key: nvidia.com/gpu
            - operator: "Exists"
              key: cloud.google.com/impending-node-termination

            volumes:
            {{ if $root.Values.network.gibVersion }}
            - name: gib
              emptyDir: {}
            {{ end }}

            - name: workload-configuration
              configMap:
                name: "{{.Release.Name}}-config"
                items:
                - key: workload-configuration
                  path: {{ $root.Values.workload.configFile | default "workload-configuration" }}

            - name: workload-launcher
              configMap:
                name: "{{.Release.Name}}-launcher"

            - name: shared-memory
              emptyDir:
                medium: "Memory"
                sizeLimit: 500Gi

            {{- range $pvc := $root.Values.volumes.pvcMounts }}
            - name: "{{ $pvc.claimName }}"
              persistentVolumeClaim:
                claimName: "{{ $pvc.claimName }}"
            {{- end }}

            {{- range $gcs := $root.Values.volumes.gcsMounts }}
            - name: "{{ $gcs.bucketName }}"
              csi:
                driver: gcsfuse.csi.storage.gke.io
                volumeAttributes:
                  bucketName: "{{ $gcs.bucketName }}"
                  {{- if $gcs.mountOptions }}
                  mountOptions: "{{ $gcs.mountOptions }}"
                  {{- end }}
            {{- end}}

            {{- if $root.Values.volumes.ssdMountPath }}
            - name: local-ssd
              hostPath:
                path: /mnt/stateful_partition/kube-ephemeral-ssd
            {{- end }}

            initContainers:
            {{ if $root.Values.network.gibVersion }}
            - name: nccl-plugin-installer
              image: {{ $root.Values.network.gibVersion }}
              imagePullPolicy: Always
              args:
              - |
                set -ex
                /scripts/container_entry.sh install --install-nccl
                cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
                cp -R /var/lib/gib/. /target/usr/local/gib
              command:
              - /bin/sh
              - -c
              volumeMounts:
              - mountPath: /target/usr/local/gib
                name: gib
            {{ end}}

            containers:
            {{- if $root.Values.workload.gcsSidecarImage }}
            - name: gke-gcsfuse-sidecar
              image: {{ $root.Values.workload.gcsSidecarImage }}
            - name: gke-gcsfuse-metadata-prefetch
              image: {{ $root.Values.workload.gcsSidecarImage }}
            {{- end }}
            {{- if $root.Values.workload.psSidecarImage }}
            - name: gke-parallelstore-sidecar
              image: {{ $root.Values.workload.psSidecarImage }}
            {{- end }}

            - name: workload
              image: "{{ $root.Values.workload.image }}"
              imagePullPolicy: Always
              {{- if $root.Values.network.hostNetwork }}
              securityContext:
                privileged: true
              {{- end }}
              env:
              - name: JOB_IDENTIFIER
                value: "{{ $jobName }}"
              - name: JOB_TIMESTAMP
                value: "{{ $timestamp }}"
              - name: JOB_UUID
                value: "{{ $jobuuid }}"
              - name: JOB_ORCHESTRATOR
                value: "gke"
              # Add RANK based on the pod's index provided by the Indexed Job
              # This is crucial for torch.distributed initialization.
              - name: JOB_COMPLETION_INDEX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: REPLICATED_JOB_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
              - name: JOBSET_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: MASTER_ADDR
                value: "{{$jobName}}-workload-0-0.{{$jobName}}"
              - name: MASTER_PORT
                value: "6002"
              - name: WORLD_SIZE
                value: "{{ $root.Values.workload.gpus }}"
              - name: NNODES
                value: "{{ $nodes }}"
              - name: GPUS_PER_NODE
                value: "{{ $gpusPerNode }}"

              - name: NCCL_PLUGIN_PATH
                value: /usr/local/gib/lib64

              {{ if $root.Values.network.gibVersion }}
              - name: NCCL_INIT_SCRIPT
                value: "/usr/local/gib/scripts/set_nccl_env.sh"
              {{ end }}

              {{ if $root.Values.network.ncclSettings }}
              {{- toYaml .Values.network.ncclSettings | nindent 14 }}
              {{ end }}

              {{ if $root.Values.workload.envs }}
              {{- toYaml .Values.workload.envs | nindent 14 }}
              {{ end }}

              command:
              - bash
              - -c
              - |
                echo "Pod on $(hostname --fqdn) is running"
                echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"

                if [[ -n "${NCCL_INIT_SCRIPT}" ]]; then
                  echo "Running NCCL init script: ${NCCL_INIT_SCRIPT}"
                  source ${NCCL_INIT_SCRIPT}
                fi

                echo "Launching workload with the following arguments:"
                {{- range $root.Values.workload.defaultArguments }}
                echo "  {{ . }}"
                {{- end }}
                {{- range $root.Values.workload.arguments }}
                echo "  {{ . }}"
                {{- end }}
                echo ""

                sleep 10

                bash /workload/launcher/launch-workload.sh \
                {{- range $root.Values.workload.defaultArguments }}
                {{ . }} \
                {{- end }}
                {{- range $root.Values.workload.arguments }}
                {{ . }} \
                {{- end }}


              volumeMounts:
                {{ if $root.Values.network.gibVersion }}
                - name: gib
                  mountPath: /usr/local/gib
                {{ end }}

                - name: workload-configuration
                  mountPath: {{ $root.Values.workload.configPath | default "/workload/configs" }}

                - name: workload-launcher
                  mountPath: /workload/launcher

                - name: shared-memory
                  mountPath: /dev/shm

                {{- range $pvc := $root.Values.volumes.pvcMounts }}
                - name: "{{ $pvc.claimName }}"
                  mountPath: "{{ $pvc.mountPath }}"
                {{- end }}

                {{- range $gcs := $root.Values.volumes.gcsMounts }}
                - name: "{{ $gcs.bucketName }}"
                  mountPath: "{{ $gcs.mountPath }}"
                {{- end }}

                {{- if $root.Values.volumes.ssdMountPath }}
                - name: local-ssd
                  mountPath: "{{ $root.Values.volumes.ssdMountPath }}"
                {{- end }}

              resources:
                limits:
                  nvidia.com/gpu: {{ $gpusPerNode }}
