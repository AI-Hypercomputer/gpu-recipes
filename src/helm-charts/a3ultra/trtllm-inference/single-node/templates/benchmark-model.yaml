# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ $nodes := div .Values.job.gpus 8 | max 1 }}
{{ $gpusPerNode := min .Values.job.gpus 8 }}

{{- $root := . -}}

apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}"
  labels:
    app: "{{ .Release.Name }}"
spec:
  ttlSecondsAfterFinished: {{ .Values.job.ttlSecondsAfterFinished }}
  parallelism: {{ $nodes }}
  completions: {{ $nodes }}
  backoffLimit: 0

  template:
    metadata:
      labels:
        app: "{{ .Release.Name }}-inference-benchmark"
      annotations:
        kubectl.kubernetes.io/default-container: inference-benchmark
        {{- if .Values.volumes.gcsMounts }}
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "0"
        gke-gcsfuse/memory-limit: "0"
        gke-gcsfuse/ephemeral-storage-limit: "0"
        {{- end}}
        {{- if not $root.Values.network.hostNetwork }}
        networking.gke.io/default-interface: "eth0"
        networking.gke.io/interfaces: |
        {{- if $root.Values.network.subnetworks }}
          [
            {{- range $i, $subnetwork := $root.Values.network.subnetworks }}
            {"interfaceName":"eth{{ $i }}","network":"{{ $subnetwork }}"}{{ eq $i 9 | ternary "" ","}}
            {{- end }}
          ]
        {{- else }}
          [
            {"interfaceName":"eth0","network":"default"},
            {"interfaceName":"eth1","network":"gvnic-1"},
            {{- range  $i := until 8 }}
            {"interfaceName":"eth{{ add 2 $i }}","network":"rdma-{{ $i }}"}{{ eq $i 7 | ternary "" ","}}
            {{- end }}
          ]
        {{- end }}
        {{- end }}

    spec:
      {{- if $root.Values.network.hostNetwork }}
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      {{- end }}
      restartPolicy: OnFailure

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
      - key: cloud.google.com/impending-node-termination
        operator: Exists
      volumes:
        - name: library-dir-host
          hostPath:
            path: /home/kubernetes/bin/nvidia
        - name: gib
          hostPath:
            path: /home/kubernetes/bin/gib
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        - name: local-ssd
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd
        - name: benchmark-script
          configMap:
            name: {{ .Release.Name }}-benchmark-script
            defaultMode: 0700
        {{- range $gcs := .Values.volumes.gcsMounts }}
        - name: "{{ $gcs.bucketName }}"
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: "{{ $gcs.bucketName }}"
        {{- end }}

      containers:
      - name: inference-benchmark
        image: "{{ .Values.job.image.repository }}:{{ .Values.job.image.tag }}"
        imagePullPolicy: Always
        securityContext:
          privileged: true
        resources:
          requests:
            nvidia.com/gpu: {{ $gpusPerNode }}
          limits:
            nvidia.com/gpu: {{ $gpusPerNode }}

        env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: "{{ .Values.huggingface.secretName }}"
                key: "{{ .Values.huggingface.secretData.token }}"

          # Enable faster downloads of model weights from HuggingFace
          - name: HF_HUB_ENABLE_HF_TRANSFER
            value: "1"
          - name: LD_LIBRARY_PATH
            value: "/usr/local/nvidia/lib64"
          - name: NCCL_DEBUG
            value: "INFO"
          # Workload specific environment variables
          - name: MODEL_DOWNLOAD_DIR
            value: "/ssd/{{ .Values.model.name }}"

          {{- range $gcs := $root.Values.volumes.gcsMounts }}
          - name: GCS_FUSE_BUCKET
            value: "{{ $gcs.bucketName }}"
          {{- end }}

        workingDir: /workspace/tensorrtllm_backend/tensorrt_llm
        command: ["/bin/bash", "-c"]
        args:
          - |
            #!/bin/bash

            mkdir -p /gcs/benchmark_logs

            # Set recommended NCCL environment variables
            source /usr/local/gib/scripts/set_nccl_env.sh

            # Download the model from HuggingFace and skip the meta original ckpt weights
            huggingface-cli download {{ .Values.model.name }} --exclude="*original*" --local-dir ${MODEL_DOWNLOAD_DIR} --token ${HF_TOKEN} --local-dir-use-symlinks False

            # List contents in LD_LIBRARY_PATH
            echo "Contents of ${LD_LIBRARY_PATH}:"
            for path in ${LD_LIBRARY_PATH//:/ }; do
              echo "Files in $path:"
              ls -l "$path"
            done

            # Fix symbolic links - remove libnccl-net.so
            mv /usr/local/nvidia/lib64/libnccl-net.so /usr/local/nvidia/lib64/libnccl-net.so.1
            ln -sf /usr/local/nvidia/lib64/libnccl-net.so /usr/local/nvidia/lib64/libnccl-net.so.1

            export LD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/torch/lib:/usr/local/lib/python3.12/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/tensorrt/lib

            # Update shared library cache, links
            ldconfig

            # Copy and execute the benchmark script
            cp /scripts/run_trtllm_bench.sh .
            chmod +x run_trtllm_bench.sh
            ./run_trtllm_bench.sh

        volumeMounts:
          - name: library-dir-host
            mountPath: /usr/local/nvidia
          - name: gib
            mountPath: /usr/local/gib
          - name: shared-memory
            mountPath: /dev/shm
          - name: local-ssd
            mountPath: {{ .Values.volumes.ssdMountPath }}
          - name: benchmark-script
            mountPath: /scripts
          {{- range $gcs := .Values.volumes.gcsMounts }}
          - name: "{{ $gcs.bucketName }}"
            mountPath: "{{ $gcs.mountPath }}"
          {{- end }}