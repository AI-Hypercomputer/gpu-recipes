# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ $timestamp := now | unixEpoch }}
{{ $jobSuffix := randAlphaNum 4 | lower }}
{{ $jobuuid := uuidv4 }}

{{ $nodes := div .Values.workload.gpus 8 | max 1 }}
{{ $gpusPerNode := min .Values.workload.gpus 8 }}

{{- $root := . -}}

apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}"
  namespace: default
  labels:
  {{- if $root.Values.queue }}
    kueue.x-k8s.io/queue-name: "{{ $root.Values.queue }}"
  {{- end }}
spec:
  {{- if $root.Values.queue }}
  suspend: true
  {{- end }}
  parallelism: {{ $nodes }}
  completions: {{ $nodes }}
  backoffLimit: 0
  completionMode: Indexed
  ttlSecondsAfterFinished: 43200
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: megatron
        {{- if $root.Values.volumes.gcsMounts }}
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "0"
        gke-gcsfuse/memory-limit: "0"
        gke-gcsfuse/ephemeral-storage-limit: "0"
        {{- end}}
    spec:
      schedulingGates:
      - name: "gke.io/topology-aware-auto-scheduling"
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      subdomain: "{{.Release.Name}}"
      restartPolicy: Never
      {{ if $root.Values.targetNodes }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                {{- range $hostname := $root.Values.targetNodes }}
                - {{ $hostname }}
                {{- end }}
      {{ end }}
      tolerations:
      - operator: "Exists"
        key: nvidia.com/gpu
      - operator: "Exists"
        key: cloud.google.com/impending-node-termination
      volumes:
      - name: library-dir-host
        hostPath:
          path: /home/kubernetes/bin/nvidia
      - name: gib
        hostPath:
          path: /home/kubernetes/bin/gib
      - name: workload-configuration
        configMap:
          name: "{{.Release.Name}}"
      - name: workload-terminated-volume
        emptyDir: {}
      - name: local-ssd
        hostPath:
          path: /mnt/stateful_partition/kube-ephemeral-ssd
      - name: shared-memory
        emptyDir:
          medium: "Memory"
          sizeLimit: 250Gi

      {{- range $gcs := $root.Values.volumes.gcsMounts }}
      - name: "{{ $gcs.bucketName }}"
        csi:
          driver: gcsfuse.csi.storage.gke.io
          volumeAttributes:
            bucketName: "{{ $gcs.bucketName }}"
      {{- end}}

      initContainers:

      {{ if $root.Values.gcsDownload }}
      - name: training-data-downloader
        image: gcr.io/google.com/cloudsdktool/google-cloud-cli
        volumeMounts:
        - name: local-ssd
          mountPath: "{{ $root.Values.volumes.ssdMountPath }}"
        {{- range $gcs := $root.Values.volumes.gcsMounts }}
        - name: "{{ $gcs.bucketName }}"
          mountPath: "{{ $gcs.mountPath }}"
        {{- end }}
        env:
        - name: GCS_DATA_SOURCE
          value: "{{ $root.Values.gcsDownload.source }}"
        - name: GCS_DATA_TARGET
          value: "{{ $root.Values.gcsDownload.target }}"
        command:
          - /bin/sh
          - -c
          - |
            echo "Caching training data from $GCS_DATA_SOURCE to $GCS_DATA_TARGET"
            mkdir -p $GCS_DATA_TARGET
            SECONDS=0
            gcloud storage rsync \
              --recursive \
              $GCS_DATA_SOURCE $GCS_DATA_TARGET
            duration=$SECONDS
            echo "Transferred or synchronized $GCS_DATA_SOURCE to $GCS_DATA_TARGET in $duration seconds."
      {{ end }}

      containers:
      - name: megatron
        image: "{{ $root.Values.workload.image }}"
        imagePullPolicy: Always
        securityContext:
          privileged: true
        env:
        - name: JOB_IDENTIFIER
          value: "{{ .Release.Name }}-{{ $timestamp }}-{{ $jobSuffix }}"
        - name: JOB_TIMESTAMP
          value: "{{ $timestamp }}"
        - name: JOB_UUID
          value: "{{ $jobuuid }}"
        - name: JOB_ORCHESTRATOR
          value: "gke"

        - name: SSD_MOUNT_PATH
          value: "{{ $root.Values.volumes.ssdMountPath }}"

        # The following settings are specific to the Torch distributed launcher:
        {{- range $gcs := $root.Values.volumes.gcsMounts }}
        - name: GCS_FUSE_BUCKET
          value: "{{ $gcs.bucketName }}"
        {{- end }}
        - name: TORCH_DISTRIBUTED_TARGET
          value: "{{ $root.Values.workload.torchDistributedTarget }}"
        - name: TORCH_DISTRIBUTED_TRACING
          value: "ALL"
        - name: MASTER_ADDR
          value: "{{.Release.Name}}-0.{{.Release.Name}}.default.svc.cluster.local"
        - name: MASTER_PORT
          value: "6002"
        - name: WORLD_SIZE
          value: "{{ $root.Values.workload.gpus }}"
        - name: NNODES
          value: "{{ $nodes }}"
        - name: GPUS_PER_NODE
          value: "{{ $gpusPerNode }}"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        # RDMA-specific NCCL environment variables
        - name: NCCL_SOCKET_IFNAME
          value: "eth0,eth1"
        - name: NCCL_CROSS_NIC
          value: "0"
        - name: NCCL_NET_GDR_LEVEL
          value: "PIX"
        - name: NCCL_P2P_NET_CHUNKSIZE
          value: "131072"
        - name: NCCL_P2P_PCI_CHUNKSIZE
          value: "131072"
        - name: NCCL_P2P_NVL_CHUNKSIZE
          value: "524288"
        - name: NCCL_NVLS_CHUNKSIZE
          value: "524288"
        - name: NCCL_IB_GID_INDEX
          value: "3"
        - name: NCCL_IB_ADAPTIVE_ROUTING
          value: "1"
        - name: NCCL_IB_QPS_PER_CONNECTION
          value: "4"
        - name: NCCL_IB_TC
          value: "52"
        - name: NCCL_IB_FIFO_TC
          value: "84"
        - name: NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE
          value: "/usr/local/gib/configs/guest_config.txtpb"
        - name: NCCL_TUNER_CONFIG_PATH
          value: "/usr/local/gib/configs/tuner_config.txtpb"
        - name: NCCL_ALGO
          value: "Ring,Tree"
        - name: NCCL_MIN_NCHANNELS
          value: "4"

        # The following is needed to prevent send-receive stalling execution
        - name: NVTE_FWD_LAYERNORM_SM_MARGIN
          value: "8"
        - name: NVTE_BWD_LAYERNORM_SM_MARGIN
          value: "8"

        {{- range $environment_variable := $root.Values.network.ncclSettings }}
        - name: {{ $environment_variable.name }}
          value: "{{ $environment_variable.value }}"
        {{- end }}

        command:
        - bash
        - -c
        - |
          function on_script_completion {
            touch /semaphore/workload_terminated
          }
          trap on_script_completion EXIT
          echo "Pod on $(hostname --fqdn) is running"
          echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"
          echo "Job ID is $JOB_IDENTIFIER"

          echo "Running nvidia-smi"
          nvidia-smi

          mkdir -p /gcs/index_mapping_dir

          echo "Warning: Set LD_LIBRARY_PATH=$LD_LIBRARY_PATH to override the NCCL library"
          ldconfig $LD_LIBRARY_PATH
          echo "Added $LD_LIBRARY_PATH to ldconfig:"
          ldconfig -p | grep libcuda | sed 's/^/  /'

          touch $SSD_MOUNT_PATH/hello-from-$HOSTNAME.txt
          echo "Local SSD contents (path $SSD_MOUNT_PATH):"; ls $SSD_MOUNT_PATH | sed 's/^/  /'

          echo "Downloading GPT vocabulary files"
          wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json &&\
          wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt

          echo "NeMo configuration file:"
          cat /etc/workload-configuration/nemo-configuration.yaml | sed 's/^/| /'
          echo ""

          echo "Detected the following additional workload arguments:"
          {{- range $root.Values.workload.arguments }}
          echo "{{ . }}"
          {{- end }}


          sleep 10 # <- Hack to allow some time for service to boot

          echo "Checking for presence of nsys:"
          which nsys

          echo "Job logs will go to /gcs/nemo-experiments/$JOB_IDENTIFIER/."
          mkdir -p /gcs/nemo-experiments/$JOB_IDENTIFIER/

          export NODE_RANK=$JOB_COMPLETION_INDEX
          echo "Launching Torch distributed as node rank $NODE_RANK out of $NNODES nodes"

          if [ "$NODE_RANK" -eq "1" ]; then
            echo "Launching nvidia-smi in daemon mode with (20 sec delay)"
            nvidia-smi dmon -d 20 -s pum &
          fi

          OMP_NUM_THREADS=12 torchrun \
          --nproc-per-node="$GPUS_PER_NODE" \
          --nnodes="$NNODES" \
          --node_rank="$NODE_RANK" \
          --rdzv_id="$JOB_IDENTIFIER" \
          --master_addr="$MASTER_ADDR" \
          --master_port="$MASTER_PORT" \
          ${TORCH_DISTRIBUTED_TARGET} \
          --config-path="/etc/workload-configuration" \
          --config-name="nemo-configuration.yaml" \
          +trainer.num_nodes="$NNODES" \
          +exp_manager.version="$JOB_IDENTIFIER" \
          +exp_manager.dllogger_logger_kwargs.json_file="/gcs/nemo-experiments/$JOB_IDENTIFIER/dllogger/rank-$NODE_RANK/dllogger.json" \
          {{- range $root.Values.workload.arguments }}
          {{ . }} \
          {{- end }}

          echo "Copying log files"
          cp -r /workspace/nemo_experiments/megatron_gpt/$JOB_IDENTIFIER/* /gcs/nemo-experiments/$JOB_IDENTIFIER/
          echo "Pod on $(hostname --fqdn) is exiting"

        volumeMounts:
          - name: library-dir-host
            mountPath: /usr/local/nvidia
          - name: gib
            mountPath: /usr/local/gib
          - name: workload-terminated-volume
            mountPath: /semaphore
          - name: workload-configuration
            mountPath: /etc/workload-configuration
          - name: shared-memory
            mountPath: /dev/shm
          - name: local-ssd
            mountPath: "{{ $root.Values.volumes.ssdMountPath }}"

          {{- range $gcs := $root.Values.volumes.gcsMounts }}
          - name: "{{ $gcs.bucketName }}"
            mountPath: "{{ $gcs.mountPath }}"
          {{- end }}

        resources:
          limits:
            nvidia.com/gpu: {{ $gpusPerNode }}