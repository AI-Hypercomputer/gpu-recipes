# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ $nodes := div .Values.workload.gpus 4 | max 1 }}
{{ $gpusPerNode := min .Values.workload.gpus 4 }}

{{ $root := . }}

apiVersion: apps/v1
kind: Deployment
metadata:
  name: "{{ .Release.Name }}"
  namespace: default
  labels:
    app: {{ .Release.Name }}-serving
    {{- if $root.Values.queue }}
    kueue.x-k8s.io/queue-name: "{{ $root.Values.queue }}"
    {{- end }}
spec:
  replicas: {{ $nodes }}
  selector:
    matchLabels:
      app: {{ .Release.Name }}-serving
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}-serving
      annotations:
        kubectl.kubernetes.io/default-container: serving
        {{- if or $root.Values.volumes.gcsVolumes }}
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "0"
        gke-gcsfuse/memory-limit: "0"
        gke-gcsfuse/ephemeral-storage-limit: "0"
        {{- end }}
        {{- if and $root.Values.queue $root.Values.dwsSettings.maxRunDurationSeconds }}
        provreq.kueue.x-k8s.io/maxRunDurationSeconds: "{{ $root.Values.dwsSettings.maxRunDurationSeconds }}"
        {{- end }}
        {{- if not $root.Values.network.hostNetwork }}
        networking.gke.io/default-interface: "eth0"
        {{- end }}
    spec:
      {{- if $root.Values.network.hostNetwork }}
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      {{- end }}
      subdomain: "{{.Release.Name}}"
      restartPolicy: Always
      {{- if $root.Values.targetNodes }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                {{ range $hostname := $root.Values.targetNodes }}
                - {{ $hostname }}
                {{ end }}
      {{- end }}
      tolerations:
      - operator: "Exists"
        key: nvidia.com/gpu
      - operator: "Exists"
        key: cloud.google.com/impending-node-termination
      - key: "kubernetes.io/arch"
        operator: "Equal"
        value: "arm64"
        effect: "NoSchedule"
      volumes:
        {{- if $root.Values.network.gibVersion }}
        - name: gib
          emptyDir: {}
        {{- end }}
        - name: serving-configuration
          configMap:
            name: "{{.Release.Name}}-config"
            items:
            - key: serving-configuration
              path: {{ $root.Values.workload.configFile | default "serving-args" }}
        - name: serving-launcher
          configMap:
            name: "{{.Release.Name}}-launcher"
            defaultMode: 0700
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        {{- range $gcs := $root.Values.volumes.gcsMounts }}
        - name: "{{ $gcs.bucketName }}"
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: "{{ $gcs.bucketName }}"
              {{- if $gcs.mountOptions }}
              mountOptions: "{{ $gcs.mountOptions }}"
              {{- end }}
        {{- end }}
        {{ if (gt (len $root.Values.volumes.pvcMounts) 0) }}
        - name: "{{ (index $root.Values.volumes.pvcMounts 0).claimName }}"
          persistentVolumeClaim:
            claimName: "{{ (index $root.Values.volumes.pvcMounts 0).claimName }}"
        {{- end }}
        {{- if $root.Values.volumes.ssdMountPath }}
        - name: local-ssd
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd
        {{- end }}

      initContainers:
      {{- if $root.Values.network.gibVersion }}
      - name: nccl-plugin-installer
        image: {{ $root.Values.network.gibVersion }}
        imagePullPolicy: Always
        args:
        - |
          set -ex
          /scripts/container_entry.sh install --install-nccl
          cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
          cp -R /var/lib/gib/. /target/usr/local/gib
        command:
        - /bin/sh
        - -c
        volumeMounts:
        - mountPath: /target/usr/local/gib
          name: gib
      {{- end }}

      containers:
        {{- if $root.Values.workload.gcsSidecarImage }}
        - name: gke-gcsfuse-sidecar
          image: {{ $root.Values.workload.gcsSidecarImage }}
        - name: gke-gcsfuse-metadata-prefetch
          image: {{ $root.Values.workload.gcsSidecarImage }}
        {{- end }}
        - name: serving
          image: "{{ $root.Values.workload.image }}"
          imagePullPolicy: Always
          {{- if $root.Values.network.hostNetwork }}
          securityContext:
            privileged: true
          {{- end }}
          env:
            {{- if $root.Values.network.ncclSettings }}
            {{- toYaml .Values.network.ncclSettings | nindent 12 }}
            {{- end }}
            - name: NCCL_PLUGIN_PATH
              value: /usr/local/gib/lib64
            - name: LD_LIBRARY_PATH
              value: /usr/local/gib/lib64:/usr/local/nvidia/lib64
            {{- if $root.Values.network.gibVersion }}
            - name: NCCL_INIT_SCRIPT
              value: "/usr/local/gib/scripts/set_nccl_env.sh"
            {{- end }}
            # Workload specific environment variables
            - name: MODEL_NAME
              value: "{{ $root.Values.workload.model.name }}"
            - name: TRTLLM_DIR
              value: "/app/tensorrt_llm"
            {{- if $root.Values.workload.envs }}
            {{- toYaml .Values.workload.envs | nindent 12 }}
            {{- end }}

          workingDir: /workload
          command: ["/bin/bash", "-c"]
          args:
            - |
              #!/bin/bash

              if [ ! -f "$LAUNCHER_SCRIPT" ]; then
                echo "Error: Launcher script $LAUNCHER_SCRIPT not found!"
                exit 1
              fi

              ARGS=()
              EXTRA_ARGS_FILE="/tmp/extra_llm_api_args.yaml"

              # Use Python to parse the main config file, extract llm_api_args,
              # and generate the command-line arguments.
              python -c "
              import yaml
              import sys

              args = []
              llm_api_args = {}
              config_file = sys.argv[1]
              extra_args_file = sys.argv[2]

              try:
                with open(config_file, 'r') as f:
                  config = yaml.safe_load(f)

                if 'llm_api_args' in config:
                  llm_api_args = config.pop('llm_api_args')
                  with open(extra_args_file, 'w') as f:
                    yaml.dump(llm_api_args, f)

                for key, value in config.items():
                  if value is True:
                    args.append(f'--{key}')
                  elif value is not False:
                    args.append(f'--{key}')
                    args.append(str(value))

                # Print the arguments for the shell script to capture
                print(' '.join(args))

              except Exception as e:
                print(f'Error parsing config file: {e}', file=sys.stderr)
                sys.exit(1)
              " "$SERVER_ARGS_FILE" "$EXTRA_ARGS_FILE" > /tmp/launcher_args.txt

              # Read the generated arguments into the ARGS array
              mapfile -t ARGS < <(tr ' ' '\n' < /tmp/launcher_args.txt)
              rm /tmp/launcher_args.txt

              {{ if eq $root.Values.workload.framework "trtllm" }}
              {{- range $root.Values.workload.benchmarks.experiments }}
              echo "Running: $LAUNCHER_SCRIPT --model_name $MODEL_NAME --isl {{ .isl }} --osl {{ .osl }} --num_requests {{ .num_requests }} -- ${ARGS[@]}"
              exec "$LAUNCHER_SCRIPT" --model_name $MODEL_NAME --isl {{ .isl }} --osl {{ .osl }} --num_requests {{ .num_requests }} -- "${ARGS[@]}"
              {{- end }}
              {{ else }}
              echo "Running: $LAUNCHER_SCRIPT ${ARGS[@]}"
              exec "$LAUNCHER_SCRIPT" "${ARGS[@]}"
              {{- end }}

          volumeMounts:
            {{- if $root.Values.network.gibVersion }}
            - name: gib
              mountPath: /usr/local/gib
            {{- end }}
            - name: serving-configuration
              mountPath: {{ $root.Values.workload.configPath | default "/workload/configs" }}
            - name: serving-launcher
              mountPath: /workload/launcher
            - name: shared-memory
              mountPath: /dev/shm
            {{- range $gcs := $root.Values.volumes.gcsMounts }}
            - name: "{{ $gcs.bucketName }}"
              mountPath: "{{ $gcs.mountPath }}"
            {{- end }}
            {{ if (gt (len $root.Values.volumes.pvcMounts) 0) }}
            - name: "{{ (index $root.Values.volumes.pvcMounts 0).claimName }}"
              mountPath: "{{ (index $root.Values.volumes.pvcMounts 0).mountPath }}"
              subPath: "{{ (index $root.Values.volumes.pvcMounts 0).subPath }}"
            {{- end }}
            {{- if $root.Values.volumes.ssdMountPath }}
            - name: local-ssd
              mountPath: "{{ $root.Values.volumes.ssdMountPath }}"
            {{- end }}

          resources:
            requests:
              nvidia.com/gpu: {{ $gpusPerNode }}
            limits:
              nvidia.com/gpu: {{ $gpusPerNode }}
