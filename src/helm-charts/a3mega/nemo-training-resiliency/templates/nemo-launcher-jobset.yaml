# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{- if eq $.Values.infrastructure.gpu_gen "a3m" }}
{{- $num_nodes := int .Values.infrastructure.num_nodes }}

{{- $jobname := "" }}
{{- if $.Values.workload.job_name_include_date }}
{{- $datetime := now | date "01021504" }}
{{- $jobname = printf "%s-goodput-n%d-%s-%s-%s" $.Release.Name $num_nodes (.Values.workload.model_size | lower) .Values.infrastructure.gpu_gen $datetime }}
{{- else }}
{{- $jobname = printf "%s-goodput-n%d-%s-%s" $.Release.Name $num_nodes (.Values.workload.model_size | lower) .Values.infrastructure.gpu_gen }}
{{- end }}

apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: {{ $jobname }}
  labels:
    xpk.google.com/workload: {{ $jobname }}
spec:
  failurePolicy:
    maxRestarts: {{ $.Values.infrastructure.max_restarts }}
  network:
    enableDNSHostnames: true
  replicatedJobs:
  - name: slice-job
    replicas: {{ $.Values.infrastructure.replicas }}
    template:
      spec:
        backoffLimit: 0
        completions: {{ $.Values.infrastructure.num_nodes }}
        parallelism: {{ $.Values.infrastructure.num_nodes }}
        template:
          metadata:
            annotations:
              {{- if $.Values.infrastructure.enable_gcsfuse }}
              gke-gcsfuse/cpu-limit: "0"
              gke-gcsfuse/ephemeral-storage-limit: "0"
              gke-gcsfuse/memory-limit: "0"
              gke-gcsfuse/volumes: "true"
              {{- end }}
              kubectl.kubernetes.io/default-container: megatron
            labels:
              xpk.google.com/workload: {{ $jobname }}
          spec:
            containers:
            - command:
              - bash
              - -c
              - |
                function on_script_completion {
                  # Note: This semaphore is used to terminate the TCPx side-car
                  touch /semaphore/workload_terminated
                  exit_code=$?
                  if [ $exit_code -ne 0 ]; then
                    # Force fail the pod if megatron failed
                    exit $exit_code
                  fi
                }
                trap on_script_completion EXIT
                trap "" SIGPROF
                echo "[goodput] Pod on $(hostname --fqdn) is running"
                echo "[goodput] Pod is assigned job index of $JOB_COMPLETION_INDEX"
                echo "Job ID is $JOB_IDENTIFIER"

                mkdir -p /gcs/index_mapping_dir

                export LD_LIBRARY_PATH="/usr/local/nccl-plugin/lib64:/usr/local/nvidia/lib64/:${LD_LIBRARY_PATH}"
                echo "Warning: Set LD_LIBRARY_PATH=$LD_LIBRARY_PATH to override the NCCL library"

                ldconfig /usr/local/nvidia/lib64/
                echo "Added /usr/local/nvidia/lib64/ to ldconfig:"
                ldconfig -p | grep libcuda | sed 's/^/  /'

                echo "Contents of /usr/local/nccl-plugin/lib64:"
                ls /usr/local/nccl-plugin/lib64 | sed 's/^/  /'

                touch $SSD_MOUNT_PATH/hello-from-$HOSTNAME.txt
                echo "Local SSD contents (path $SSD_MOUNT_PATH):"; ls $SSD_MOUNT_PATH | sed 's/^/  /'

                {{- if $.Values.workload.delete_old_ckpt }}
                  {{- if $.Values.infrastructure.enable_gcsfuse }}
                    if [[ "${NODE_RANK}" == "0" ]]; then
                      bash resiliency/scripts/rm_old_ckpts.sh &
                    fi
                  {{- else }}
                    bash resiliency/scripts/rm_old_ckpts.sh &
                  {{- end }}
                {{- end }}

                {{- if .Values.infrastructure.use_supervisor }}
                  CMD="exec python3 resiliency/launcher.py --use-supervisor"
                {{- else }}
                  CMD="exec ft_launcher"
                {{- end }}
                CMD="$CMD \
                --ft-param-initial_rank_heartbeat_timeout="${FT_PARAM_INITIAL_RANK_HEARTBEAT_TIMEOUT}" \
                --ft-param-rank_heartbeat_timeout="${FT_PARAM_RANK_HEARTBEAT_TIMEOUT}" \
                --nproc-per-node="${GPUS_PER_NODE}" \
                --nnodes="${NNODES}" \
                --node_rank="${NODE_RANK}" \
                --rdzv_id="${JOB_IDENTIFIER}" \
                --master_addr="${MASTER_ADDR}" \
                --master_port="${MASTER_PORT}" \
                resiliency/examples/train.py ${FLAGS}"

                eval echo "Launching command: $CMD"
                eval "$CMD"
              env:
              - name: REPLICATED_JOB_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
              - name: JOBSET_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: JOB_IDENTIFIER
                value: {{ $jobname }}
              - name: SSD_MOUNT_PATH
                value: /ssd
              - name: GCS_FUSE_BUCKET
                value: {{ $.Values.infrastructure.gcsfuse_bucket }}
              - name: TORCH_DISTRIBUTED_TRACING
                value: ALL
              - name: TORCH_NCCL_DUMP_ON_TIMEOUT
                value: "1"
              - name: TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN
                value: "1"
              - name: MASTER_ADDR
                value: $(JOBSET_NAME)-$(REPLICATED_JOB_NAME)-0-0.$(JOBSET_NAME)
              - name: NNODES
                value: "{{ $.Values.infrastructure.num_nodes }}"
              - name: GPUS_PER_NODE
                value: "{{ $.Values.infrastructure.gpus_per_node }}"
              {{- if $.Values.infrastructure.use_supervisor }}
              - name: HOST_DAEMON_PORT
                value: "{{ $.Values.infrastructure.host_daemon_port }}"
              {{- end }}
              {{- range $key, $value := $.Values.workload }}
              - name: {{ $key | upper }}
                value: "{{ $value }}"
              {{- end }}
              - name: GLOO_SOCKET_IFNAME
                value: eth0
              - name: NODE_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: WORKLOAD_model.data.index_mapping_dir
                value: /gcs/index_mapping_dir
              - name: WORKLOAD_model.data.data_prefix
                value: '[1.0,/ssd/.cache/wikipedia-tokenized-for-llama2]'
              - name: WORKLOAD_model.tokenizer.model
                value: /ssd/.cache/llama-2-7b-megatron-checkpoint/tokenizer.model
              - name: NVTE_FWD_LAYERNORM_SM_MARGIN
                value: "8"
              - name: NVTE_BWD_LAYERNORM_SM_MARGIN
                value: "8"
              - name: NCCL_BUFFSIZE
                value: "8388608"
              - name: NCCL_FASTRAK_CTRL_DEV
                value: eth0
              - name: NCCL_FASTRAK_IFNAME
                value: eth1,eth2,eth3,eth4,eth5,eth6,eth7,eth8
              - name: NCCL_SOCKET_IFNAME
                value: eth0
              - name: NCCL_TUNER_PLUGIN
                value: libnccl-tuner.so
              - name: NCCL_TUNER_CONFIG_PATH
                value: /usr/local/nccl-plugin/lib64/a3plus_tuner_config.textproto
              - name: NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE
                value: /usr/local/nccl-plugin/lib64/a3plus_guest_config.textproto
              - name: NCCL_DYNAMIC_CHUNK_SIZE
                value: "524288"
              - name: NCCL_P2P_NET_CHUNKSIZE
                value: "524288"
              - name: NCCL_P2P_PCI_CHUNKSIZE
                value: "524288"
              - name: NCCL_P2P_NVL_CHUNKSIZE
                value: "1048576"
              - name: NCCL_FASTRAK_NUM_FLOWS
                value: "2"
              - name: NCCL_FASTRAK_USE_SNAP
                value: "1"
              - name: NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS
                value: "600000"
              - name: NCCL_FASTRAK_ENABLE_CONTROL_CHANNEL
                value: "0"
              - name: NCCL_NET_GDR_LEVEL
                value: PIX
              - name: NCCL_FASTRAK_ENABLE_HOTPATH_LOGGING
                value: "0"
              - name: NCCL_FASTRAK_USE_LLCM
                value: "1"
              - name: NCCL_CROSS_NIC
                value: "0"
              - name: NCCL_PROTO
                value: Simple
              - name: IMAGE_VERSION
                value: "24.05"
              - name: NCCL_DEBUG
                value: VERSION # WARN
              - name: NCCL_DEBUG_SUBSYS
                value: INIT,NET,ENV,COLL,GRAPH
              - name: NCCL_ALGO
                value: Ring,Tree
              - name: NCCL_MIN_NCHANNELS
                value: "4"
              - name: NCCL_NVLS_ENABLE
                value: "0"
              image: {{ $.Values.images.megatron }}
              imagePullPolicy: Always
              name: megatron
              ports:
              - containerPort: 6002
              - containerPort: 60061
              - containerPort: 60062
              - containerPort: 60063
              - containerPort: 60064
              - containerPort: 60065
              - containerPort: 60066
              - containerPort: 60067
              - containerPort: 60068
              resources:
                limits:
                  nvidia.com/gpu: 8
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /usr/local/nvidia
                name: nvidia-install-dir-host
              - mountPath: /usr/local/nccl-plugin
                name: nccl-plugin-volume
              - mountPath: /tmp
                name: tcpx-daemon-socket
              - mountPath: /semaphore
                name: workload-terminated-volume
              - mountPath: /dev/shm
                name: shared-memory
              - mountPath: /ssd
                name: local-ssd
              {{- if $.Values.infrastructure.enable_gcsfuse }}
              - mountPath: /gcs
                name: gcs-hns-vol
              {{- end }}
            dnsPolicy: ClusterFirstWithHostNet
            hostNetwork: true
            initContainers:
            - command:
              - /bin/sh
              - -c
              - |
                mkdir -p /var/lib/tcpxo
                ln -s /var/lib/tcpxo /var/lib/tcpx
                /scripts/container_entry.sh install --install-nccl
                # cp -r /var/lib/tcpxo/lib64/. /usr/local/nccl-plugin/lib64
                cp -r /var/lib/tcpxo/* /usr/local/nccl-plugin/
                echo "Installed NCCL plugin to pod-wide, shared NCCL plug-in volume"
                echo "Contents (mounted at /usr/local/nccl-plugin/lib64):"
                ls /usr/local/nccl-plugin/lib64 | sed 's/^/  /'
                echo "Contents (mounted at /usr/local/nccl-plugin/):"
                ls /usr/local/nccl-plugin/ | sed 's/^/  /'
              image: {{ $.Values.images.nccl_plugin_installer }}
              imagePullPolicy: Always
              name: nccl-plugin-installer
              volumeMounts:
              - mountPath: /usr/local/nccl-plugin
                name: nccl-plugin-volume
            - command:
              - bash
              - -c
              - |
                /fts/entrypoint_rxdm_container.sh --num_hops 2 --num_nics 8 --uid=  --alsologtostderr &
                while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
                pkill -e "^"entrypoint_rxdm_container.sh || true
                sleep 15
              env:
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64
              image: {{ $.Values.images.network_rx_daemon }}
              imagePullPolicy: Always
              name: network-rx-daemon
              restartPolicy: Always
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /tmp
                name: tcpx-daemon-socket
              - mountPath: /semaphore
                name: workload-terminated-volume
              - mountPath: /usr/local/nvidia
                name: nvidia-install-dir-host
            # priorityClassName: high
            restartPolicy: Never
            terminationGracePeriodSeconds: 0
            tolerations:
            - key: nvidia.com/gpu
              operator: Exists
            volumes:
            - hostPath:
                path: /home/kubernetes/bin/nvidia
              name: nvidia-install-dir-host
            - emptyDir: {}
              name: nccl-plugin-volume
            - hostPath:
                path: /run/tcpx
              name: tcpx-daemon-socket
            - emptyDir:
                medium: Memory
                sizeLimit: 600Gi
              name: shared-memory
            - emptyDir: {}
              name: workload-terminated-volume
            - hostPath:
                path: /mnt/stateful_partition/kube-ephemeral-ssd
              name: local-ssd
            {{- if $.Values.infrastructure.enable_gcsfuse }}
            - name: gcs-hns-vol
              persistentVolumeClaim:
                claimName: {{ $.Values.infrastructure.pvc }}
            {{- end }}
{{- end }}