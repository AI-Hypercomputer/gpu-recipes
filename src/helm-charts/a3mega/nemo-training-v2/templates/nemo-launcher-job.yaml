# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ $timestamp := now | date "2006-01-02-15-04-05" }}
{{ $jobSuffix := randAlphaNum 4 | lower }}
{{ $jobuuid := uuidv4 }}
{{ $nodes := div .Values.workload.gpus 8 | max 1 }}
{{ $gpusPerNode := min .Values.workload.gpus 8 }}
{{- $root := . -}}

apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}"
  namespace: default
  labels:
  {{- if $root.Values.queue }}
    kueue.x-k8s.io/queue-name: "{{ $root.Values.queue }}"
  {{- end }}
spec:
  {{- if $root.Values.queue }}
  suspend: true
  {{- end }}
  parallelism: {{ $nodes }}
  completions: {{ $nodes }}
  completionMode: Indexed
  ttlSecondsAfterFinished: 43200
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: nemo-workload
        {{- if $root.Values.volumes.gcsVolumes }}
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "500m"
        gke-gcsfuse/memory-limit: "1Ti"
        gke-gcsfuse/ephemeral-storage-limit: "2Ti"
        {{- end }}
        {{- if $root.Values.volumes.psVolumes }}
        gke-parallelstore/volumes: "true"
        gke-parallelstore/cpu-limit: "0"
        gke-parallelstore/memory-limit: "0"
        {{- end }}
        {{- if and $root.Values.queue $root.Values.tasSettings.topologyRequest }}
        {{- toYaml .Values.tasSettings.topologyRequest | nindent 8 }}
        {{- end }}
        {{- if and $root.Values.queue $root.Values.dwsSettings.maxRunDurationSeconds }}
        provreq.kueue.x-k8s.io/maxRunDurationSeconds: "{{ $root.Values.dwsSettings.maxRunDurationSeconds }}"
        {{- end }}
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      subdomain: "{{.Release.Name}}"
      restartPolicy: Never
      {{ if $root.Values.targetNodes }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                {{- range $hostname := $root.Values.targetNodes }}
                - {{ $hostname }}
                {{- end }}
      {{ end }}

      tolerations:
      - operator: "Exists"
        key: nvidia.com/gpu
      - operator: "Exists"
        key: cloud.google.com/impending-node-termination

      volumes:
      - name: nvidia-install-dir-host
        hostPath:
          path: /home/kubernetes/bin/nvidia
      {{- if $root.Values.network.pluginVersion }}
      - name: nccl-plugin-volume
        emptyDir: {}
      {{- end }}
      - name: workload-configuration
        configMap:
          name: "{{.Release.Name}}"
      - name: shared-memory
        emptyDir:
          medium: "Memory"
          sizeLimit: 250Gi

      {{- range $pvc := $root.Values.volumes.pvcMounts }}
      - name: "{{ $pvc.claimName }}"
        persistentVolumeClaim:
          claimName: "{{ $pvc.claimName }}"
      {{- end }}

      {{- range $gcs := $root.Values.volumes.gcsMounts }}
      - name: "{{ $gcs.bucketName }}"
        csi:
          driver: gcsfuse.csi.storage.gke.io
          volumeAttributes:
            bucketName: "{{ $gcs.bucketName }}"
            {{- if $gcs.mountOptions }}
            mountOptions: "{{ $gcs.mountOptions }}"
            {{- end }}
      {{- end}}

      {{- if $root.Values.volumes.ssdMountPath }}
      - name: local-ssd
        hostPath:
          path: /mnt/stateful_partition/kube-ephemeral-ssd
      {{- end }}

      initContainers:
      {{- if $root.Values.network.pluginVersion }}
      - name: nccl-plugin-installer
        image: "{{ $root.Values.network.pluginVersion }}"
        imagePullPolicy: Always
        volumeMounts:
        - name: nccl-plugin-volume
          mountPath: /usr/local/nccl-plugin
        command:
          - /bin/sh
          - -c
          - |
            set -ex
            chmod 755 /scripts/container_entry.sh
            /scripts/container_entry.sh install --install-nccl
            cp -r /var/lib/tcpxo/* /usr/local/nccl-plugin/
            echo "Installed NCCL plugin to pod-wide, shared NCCL plug-in volume"
            echo "Contents (mounted at /usr/local/nccl-plugin/lib64):"
            ls /usr/local/nccl-plugin/lib64 | sed 's/^/  /'
            echo "Contents (mounted at /usr/local/nccl-plugin/):"
            ls /usr/local/nccl-plugin/ | sed 's/^/  /'
      {{- end }}

      - name: network-rx-daemon
        image: {{ $root.Values.network.daemonVersion }}
        imagePullPolicy: Always
        restartPolicy: Always
        securityContext:
          privileged: true
        volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        command:
        - bash
        - -c
        - |
          cleanup() {
            echo "Received SIGTERM, exiting RxDM"
            if [ -n "$child_pid" ]; then
              echo "Sending SIGTERM to child process"
              kill -TERM "$child_pid"
            fi
            exit 0
          }
          trap cleanup SIGTERM

          chmod 755 /fts/entrypoint_rxdm_container.sh
          /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8  --uid= --alsologtostderr & child_pid=$!

          wait "$child_pid"

      containers:
      {{- if $root.Values.workload.gcsSidecarImage }}
      - name: gke-gcsfuse-sidecar
        image: {{ $root.Values.workload.gcsSidecarImage }}
      - name: gke-gcsfuse-metadata-prefetch
        image: {{ $root.Values.workload.gcsSidecarImage }}
      {{- end }}
      {{- if $root.Values.workload.psSidecarImage }}
      - name: gke-parallelstore-sidecar
        image: {{ $root.Values.workload.psSidecarImage }}
      {{- end }}

      - name: nemo-workload
        image: "{{ $root.Values.workload.image }}"
        imagePullPolicy: Always
        securityContext:
          privileged: true
        env:
        - name: JOB_IDENTIFIER
          value: "{{ .Release.Name }}-{{ $timestamp }}"
        - name: TORCH_DISTRIBUTED_TARGET
          value: "{{ $root.Values.workload.torchDistributedTarget }}"
        - name: EXPERIMENT_NAME
          value: "{{ $root.Values.workload.experimentName }}"
        - name: EXPERIMENT_ROOT_DIR
          value: "{{ $root.Values.workload.experimentRootDir }}"
        {{- if $root.Values.workload.checkpointsRootDir }}
        - name: CHECKPOINTS_ROOT_DIR
          value: "{{ $root.Values.workload.checkpointsRootDir }}"
        {{- end }}
        {{- if $root.Values.workload.tokenizerPath }}
        - name: TOKENIZER_PATH
          value: "{{ $root.Values.workload.tokenizerPath }}"
        {{- end }}
        - name: MASTER_ADDR
          value: "{{.Release.Name}}-0.{{.Release.Name}}.default.svc.cluster.local"
        - name: MASTER_PORT
          value: "6002"
        - name: WORLD_SIZE
          value: "{{ $root.Values.workload.gpus }}"
        - name: NNODES
          value: "{{ $nodes }}"
        - name: GPUS_PER_NODE
          value: "{{ $gpusPerNode }}"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"

        # The following is needed to prevent send-receive stalling execution
        - name: NVTE_FWD_LAYERNORM_SM_MARGIN
          value: "8"
        - name: NVTE_BWD_LAYERNORM_SM_MARGIN
          value: "8"

        {{- if $root.Values.network.pluginVersion }}
        - name: CONFIGURE_NCCL_PLUGIN
          value: "true"
        - name: NCCL_LIB_DIR
          value: "/usr/local/nccl-plugin/lib64"
        {{- end }}

        {{- range $environment_variable := $root.Values.network.ncclSettings }}
        - name: {{ $environment_variable.name }}
          value: "{{ $environment_variable.value }}"
        {{- end }}

        - name: NEMO_CONFIG_OVERRIDES
          value: {{ join " " $root.Values.workload.nemoConfigOverrides | quote }}

        {{- if $root.Values.workload.enableNemoDebug }}
        - name: NEMO_TESTING
          value: "true"
        {{- end }}

        command:
        - bash
        - -c
        - |

          trap "" SIGPROF

          sleep 10

          echo "Pod on $(hostname --fqdn) is running"
          echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"
          echo "Job ID is $JOB_IDENTIFIER"

          echo "Running nvidia-smi"
          nvidia-smi

          ldconfig /usr/local/nvidia/lib64/
          echo "Added /usr/local/nvidia/lib64/ to ldconfig:"
          ldconfig -p | grep libcuda | sed 's/^/  /'

          export LD_LIBRARY_PATH="/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}"
          if [[ -n "${CONFIGURE_NCCL_PLUGIN}" ]]; then
            echo "Setting NCCL environment variables from ${NCCL_LIB_DIR}/nccl-env-profile.sh"
            source ${NCCL_LIB_DIR}/nccl-env-profile.sh
          fi
          echo "LD_LIBRARY_PATH=$LD_LIBRARY_PATH"

          dllogger_json_file="${EXPERIMENT_ROOT_DIR}/${EXPERIMENT_NAME}/${JOB_IDENTIFIER}/dllogger/rank-${JOB_COMPLETION_INDEX}/dllogger.json"
          explicit_log_dir="${EXPERIMENT_ROOT_DIR}/${EXPERIMENT_NAME}/${JOB_IDENTIFIER}"
          echo "Logging to ${explicit_log_dir}"
          echo "DLLogger logging to ${dllogger_json_file}"
          if [[ -n "${CHECKPOINTS_ROOT_DIR}" ]]; then
            checkpoints_dir="${CHECKPOINTS_ROOT_DIR}/${EXPERIMENT_NAME}/${JOB_IDENTIFIER}"
            echo "Checkpoints will be saved to ${checkpoints_dir}"
            NEMO_CONFIG_OVERRIDES="${NEMO_CONFIG_OVERRIDES} ++exp_manager.checkpoint_callback_params.dirpath=${checkpoints_dir}"
          fi

          echo "NeMo configuration file:"
          cat /etc/workload-configuration/nemo-configuration.yaml | sed 's/^/| /'
          echo ""

          echo "NeMo configuration overrides:"
          echo "${NEMO_CONFIG_OVERRIDES}"
          echo ""

          if [[ -n "${TOKENIZER_PATH}" ]]; then
            echo "Getting tokenizer files"
            cp ${TOKENIZER_PATH}/* .
            echo ""
          fi

          echo "Launching Torch distributed on the node rank $JOB_COMPLETION_INDEX out of $NNODES nodes"

          OMP_NUM_THREADS=12 torchrun \
          --nproc-per-node="${GPUS_PER_NODE}" \
          --nnodes="${NNODES}" \
          --node_rank="${JOB_COMPLETION_INDEX}" \
          --rdzv_id="${JOB_IDENTIFIER}" \
          --master_addr="${MASTER_ADDR}" \
          --master_port="${MASTER_PORT}" \
          ${TORCH_DISTRIBUTED_TARGET} \
          --config-path="/etc/workload-configuration" \
          --config-name="nemo-configuration.yaml" \
          ++trainer.num_nodes="$NNODES" \
          ++exp_manager.name="$EXPERIMENT_NAME" \
          ++exp_manager.version="$JOB_IDENTIFIER" \
          ++exp_manager.explicit_log_dir="${explicit_log_dir}" \
          ++exp_manager.create_tensorboard_logger=true \
          ++exp_manager.create_dllogger_logger=true \
          ++exp_manager.dllogger_logger_kwargs.verbose=true \
          ++exp_manager.dllogger_logger_kwargs.stdout=true \
          ++exp_manager.dllogger_logger_kwargs.json_file="${dllogger_json_file}" \
          ${NEMO_CONFIG_OVERRIDES}

          echo "Training completed"
          echo "Pod on $(hostname --fqdn) is exiting"

        volumeMounts:
          - name: nvidia-install-dir-host
            mountPath: /usr/local/nvidia
          {{- if $root.Values.network.pluginVersion }}
          - name: nccl-plugin-volume
            mountPath: /usr/local/nccl-plugin
          {{- end }}
          - name: workload-configuration
            mountPath: /etc/workload-configuration
          - name: shared-memory
            mountPath: /dev/shm

          {{- range $pvc := $root.Values.volumes.pvcMounts }}
          - name: "{{ $pvc.claimName }}"
            mountPath: "{{ $pvc.mountPath }}"
          {{- end }}

          {{- range $gcs := $root.Values.volumes.gcsMounts }}
          - name: "{{ $gcs.bucketName }}"
            mountPath: "{{ $gcs.mountPath }}"
          {{- end }}

          {{- if $root.Values.volumes.ssdMountPath }}
          - name: local-ssd
            mountPath: "{{ $root.Values.volumes.ssdMountPath }}"
          {{- end }}

        resources:
          limits:
            nvidia.com/gpu: {{ $gpusPerNode }}