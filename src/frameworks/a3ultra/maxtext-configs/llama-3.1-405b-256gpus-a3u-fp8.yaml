hardware: gpu
dcn_fsdp_parallelism: 32
ici_fsdp_parallelism: 8
per_device_batch_size: 2
max_target_length: 8192
learning_rate: 0.001
model_name: llama3.1-405b
enable_checkpointing: false
quantization: fp8
attention: cudnn_flash_te
remat_policy: full
use_iota_embed: true
dataset_type: synthetic
logits_dot_in_fp32: false
enable_goodput_recording: false
monitor_goodput: false
save_config_to_gcs: true
