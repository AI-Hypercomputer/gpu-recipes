# Pretrain Llama-3.1-8B workloads on a4x GKE Node pools with Nvidia NeMo Framework

This recipe outlines the steps for running a Llama-3.1-8B pretraining workload
on [a4x GKE Node pools](https://cloud.google.com/kubernetes-engine) by using the
[NVIDIA NeMo framework](https://github.com/NVIDIA/nemo).

## Orchestration and deployment tools

For this recipe, the following setup is used:

- Orchestration - [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)
- Pretraining job configuration and deployment - A Helm chart is used to configure and deploy
  the [Kubernetes Jobset](https://kubernetes.io/blog/2025/03/23/introducing-jobset)
  resource which manages the execution  of the
  [NeMo pretraining workload](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_pretraining.py).

## Test environment

This recipe has been optimized for and tested with the following configuration:

- GKE cluster
Please follow Cluster Toolkit [instructions] (https://github.com/GoogleCloudPlatform/cluster-toolkit/tree/main/examples/gke-a4x) to create your A4x GKE cluster.

## Training dataset

This recipe uses a mock pretraining dataset provided by the NeMo framework

## Docker container image

This recipe uses the following docker image:
- `nvcr.io/nvidia/nemo:25.07`
- `us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-arm64:v1.0.6`

This image is based on NVIDIA NeMo 25.07 and contains the NCCL gIB plugin
v1.0.6, bundling all NCCL binaries validated for use with a4x GPUs.


## Run the recipe

From your client workstation, complete the following steps:

### Configure environment settings

Set the environment variables to match your environment:

 ```bash
 export PROJECT_ID=<PROJECT_ID>
 export CLUSTER_REGION=<CLUSTER_REGION>
 export CLUSTER_NAME=<CLUSTER_NAME>
 export GCS_BUCKET=<GCS_BUCKET> #You don't need to add gs://
 export KUEUE_NAME=<KUEUE_NAME>
 ```

Replace the following values:

 - `<PROJECT_ID>`: your Google Cloud project ID.
 - `<CLUSTER_REGION>`: the region where your cluster is located.
 - `<CLUSTER_NAME>`: the name of your GKE cluster.
 - `<GCS_BUCKET>`: the name of your Cloud Storage bucket. Don't include the `gs://` prefix.
 - `<KUEUE_NAME>`: the name of the Kueue local queue. The default queue created by the cluster toolkit is `a4x`. Make sure to verify the name of the local queue in your cluster.

Set the default project:

 ```bash
 gcloud config set project $PROJECT_ID
 ```


### Get the recipe

Clone the `gpu-recipes` repository and set a reference to the recipe folder.

```
git clone https://github.com/ai-hypercomputer/gpu-recipes.git
cd gpu-recipes
export REPO_ROOT=`git rev-parse --show-toplevel`
export RECIPE_ROOT=$REPO_ROOT/training/a4x/llama3-1-8b/nemo-pretraining-gke/1node
cd $RECIPE_ROOT
```

### Get cluster credentials

```
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION
```

### Configure and submit a pretraining job

#### Using 1 node (4 GPUs) BF16 precision

The default job setting is 20 training steps and bf16 precision. To execute the
job with the default settings, run the following command from your client:

    ```bash 
    cd $RECIPE_ROOT
    export WORKLOAD_NAME=$USER-a4x-llama31-8b-bf16-1node
    helm install $WORKLOAD_NAME . -f values.yaml \
    --set-file workload_launcher=launcher.sh \
    --set-file workload_config=llama31-8b.py \
    --set workload.image=nvcr.io/nvidia/nemo:25.07 \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set volumes.gcsMounts[0].mountPath=/job-logs \
    --set workload.envs[0].value=/job-logs/$WORKLOAD_NAME \
    --set queue=${KUEUE_NAME}
    ```

**Examples**

-   To set the number of training steps to 100, run the following command from
    your client:

    ```bash 
    cd $RECIPE_ROOT
    export WORKLOAD_NAME=$USER-a4x-llama31-8b-bf16-1node33
    helm install $WORKLOAD_NAME . -f values.yaml \
    --set-file workload_launcher=launcher.sh \
    --set-file workload_config=llama31-8b.py \
    --set workload.image=nvcr.io/nvidia/nemo:25.07 \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set volumes.gcsMounts[0].mountPath=/job-logs \
    --set workload.envs[0].value=/job-logs/$WORKLOAD_NAME \
    --set queue=${KUEUE_NAME} \
    --set workload.arguments[0]="trainer.max_steps=100"
    ```

### Monitor the job

To check the status of pods in your job, run the following command:

```
kubectl get pods | grep $USER-a4x-llama31-8b-bf16-1node
```

Replace the following:
- JOB_NAME_PREFIX - your job name prefix. For example $USER-a4x-llama31-8b-bf16-1node.

To get the logs for one of the pods, run the following command:

```
kubectl logs POD_NAME
```

Information about the training job's progress, including crucial details such as loss,
step count, and step time, is generated by the rank 0 process.
This process runs on the pod whose name begins with `JOB_NAME_PREFIX-workload-0-0`.
For example: `user-llama-3-1-8b-nemo-fp8-workload-0-0-s9zrv`.

### Uninstall the Helm release

You can delete the job and other resources created by the Helm chart. To
uninstall Helm, run the following command from your client:

```bash
helm uninstall $USER-a4x-llama31-8b-bf16-1node
```
