# Pretrain Llama-3.1-70B workloads on A4 GKE Node pools using MaxText

This recipe outlines the steps for running a Llama-3.1-70B pretraining workload
on [A4 GKE Node pools](https://cloud.google.com/kubernetes-engine) by using the
[MaxText framework](https://github.com/AI-Hypercomputer/maxtext).

## Orchestration and deployment tools

For this recipe, the following setup is used:

- Orchestration - [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)
- Pretraining job configuration and deployment - A Helm chart is used to configure and deploy the
  [Kubernetes Jobset]((https://kubernetes.io/blog/2025/03/23/introducing-jobset) resource which manages the
  execution  of the [MaxText pretraining workload](https://github.com/AI-Hypercomputer/maxtext/blob/main/MaxText/train.py).

## Test environment

This recipe has been optimized for and tested with the following configuration:

- GKE cluster
    - [A regional standard cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview) version: 1.31.7-gke.1265000 or later.
    - A GPU node pool with 32 [a4-highgpu-8g](https://cloud.google.com/compute/docs/accelerator-optimized-machines#a4-vms) machines provisioned using the DENSE deployment type.
    - [Workload Identity Federation for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity) enabled.
    - [Cloud Storage FUSE CSI driver for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/cloud-storage-fuse-csi-driver) enabled.
    - [DCGM metrics](https://cloud.google.com/kubernetes-engine/docs/how-to/dcgm-metrics) enabled.
    - [Kueue](https://kueue.sigs.k8s.io/docs/reference/kueue.v1beta1/) and [JobSet](https://jobset.sigs.k8s.io/docs/overview/) APIs installed.
    - Kueue configured to support [Topology Aware Scheduling](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/).
- A regional Google Cloud Storage (GCS) bucket to store logs generated by the recipe runs.

To prepare the required environment, see
[GKE environment setup guide](../../../../docs/configuring-environment-gke-a4.md).

## Training dataset

This recipe uses a mock pre-training dataset provided by the MaxText framework.

## Docker container image

This recipe uses the following [Deep Learning Software Layer](https://cloud.google.com/ai-hypercomputer/docs/software-stack#cluster_images) container image:

`us-central1-docker.pkg.dev/deeplearning-images/reproducibility/jax-maxtext-gpu:jax0.5.1-cuda_dl25.02-rev1-maxtext-20150317`.

## Run the recipe

From your client workstation, complete the following steps:

### Configure environment settings


Set the environment variables to match your environment:

 ```bash
 export PROJECT_ID=<PROJECT_ID>
 export CLUSTER_REGION=<CLUSTER_REGION>
 export CLUSTER_NAME=<CLUSTER_NAME>
 export GCS_BUCKET=<GCS_BUCKET>
 export KUEUE_NAME=<KUEUE_NAME>
 ```

 Replace the following values:

 - `<PROJECT_ID>`: your Google Cloud project ID.
 - `<CLUSTER_REGION>`: the region where your cluster is located.
 - `<CLUSTER_NAME>`: the name of your GKE cluster.
 - `<GCS_BUCKET>`: the name of your Cloud Storage bucket. Don't include the `gs://` prefix.
 - `<KUEUE_NAME>`: the name of the Kueue queue configured for TAS. The default queue created by the cluster toolkit is `a4`. The default queue created by the cluster toolkit is `a4`. Make sure to verify the name of the local queue in your cluster.

Set the default project:

 ```bash
 gcloud config set project $PROJECT_ID
 ```

### Get the recipe

Clone the `gpu-recipes` repository and set a reference to the recipe folder.

```
cd
git clone https://github.com/ai-hypercomputer/gpu-recipes.git
cd gpu-recipes
export REPO_ROOT=`git rev-parse --show-toplevel`
export RECIPE_ROOT=$REPO_ROOT/training/a4/llama3-1-70b/maxtext-pretraining-gke
```

### Get cluster credentials

From your client, get the credentials for your cluster.

```
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION
```

### Configure and submit a pretraining job

#### Using 32 nodes (256 GPUs) fp8 precision

The default job setting is 15 training steps and fp8 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/maxtext-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/maxtext-configs/llama3-1-70b-256gpus-a4-fp8.yaml \
    --set workload.gpus=256 \
    --set queue=$KUEUE_NAME \
    --set workload.arguments[0]="base_output_directory=gs://${GCS_BUCKET}/maxtext-experiments" \
    $USER-llama-3-1-70b-maxtext-fp8 \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 32 nodes (256 GPUs) bf16 precision

The default job setting is 15 training steps and bf16 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/maxtext-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/maxtext-configs/llama3-1-70b-256gpus-a4-bf16.yaml \
    --set workload.gpus=256 \
    --set queue=$KUEUE_NAME \
    --set workload.arguments[0]="base_output_directory=gs://${GCS_BUCKET}/maxtext-experiments" \
    $USER-llama-3-1-70b-maxtext-bf16 \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 128 nodes (1024 GPUs) fp8 precision

The default job setting is 15 training steps and fp8 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/maxtext-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/maxtext-configs/llama3-1-70b-1024gpus-a4-fp8.yaml \
    --set workload.gpus=1024 \
    --set queue=$KUEUE_NAME \
    --set workload.arguments[0]="base_output_directory=gs://${GCS_BUCKET}/maxtext-experiments" \
    $USER-llama-3-1-70b-maxtext-fp8 \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 128 nodes (1024 GPUs) bf16 precision

The default job setting is 15 training steps and bf16 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/maxtext-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/maxtext-configs/llama3-1-70b-1024gpus-a4-bf16.yaml \
    --set workload.gpus=1024 \
    --set queue=$KUEUE_NAME \
    --set workload.arguments[0]="base_output_directory=gs://${GCS_BUCKET}/maxtext-experiments" \
    $USER-llama-3-1-70b-maxtext-bf16 \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Configure job settings

**Examples**

-   To set the number of training steps to 100, run the following command from
    your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/maxtext-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/maxtext-configs/llama3-1-70b-256gpus-a4-fp8.yaml \
    --set workload.gpus=256 \
    --set queue=$KUEUE_NAME \
    --set workload.arguments[0]="base_output_directory=gs://${GCS_BUCKET}/maxtext-experiments steps=100" \
    $USER-llama-3-1-70b-maxtext-fp8 \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

### Monitor the job

To check the status of pods in your job, run the following command:

```
kubectl get pods | grep JOB_NAME_PREFIX
```

Replace the following:
- JOB_NAME_PREFIX - your job name prefix. For example $USER-llama-3-1-70b-maxtext.

To get the logs for one of the pods, run the following command:

```
kubectl logs POD_NAME
```

Information about the training job's progress, including crucial details such as loss,
step count, and step time, is generated by the rank 0 process.
This process runs on the pod whose name begins with
`JOB_NAME_PREFIX-workload-0-0`. For example: `user-llama-3-1-70b-maxtext-workload-0-0-s9zrv`.

### Analyze results

When completed, the job creates tensorboard logs in the following location:

```
gs://${GCS_BUCKET}/maxtext-experiments/$JOB_ID/tensorboard/$JOB_ID/
├── events.out.tfevents....
...
```

To inspect the text logs generated by MaxText, retrieve them from any Pod in the
job using the following command: `kubectl logs "<pod_name>"`

Here is an example of an entry in :

```
completed step: 11, seconds: 20.935, TFLOP/s/device: 1507.142, Tokens/s/device: 3130.521, total_weights: 16777216, loss: 12.329
```

The logs will show you the step time in seconds and the TFLOP/s/device.

### Calculate training performance metrics (eMFU)

This section explains how to calculate the effective Model FLOPS Utilization
(eMFU), using the logs from the pods. Using the example logs from the previous
step, and considering the number of TFLOP/s/device of 903.017, you can compute
the eMFU using the following formula:

```
           TFLOP/s/device        1507.142
eMFU =   ------------------- =  --------- = 0.6737 = 67.37%
             MAX TFLOP B200        2237

```

MAX TFLOP B200 BF16: 2237

### Troubleshooting

This section provides guidance on troubleshooting issues with the training job.

To check the status of the job's pods, use the following command:

```bash
kubectl get pods | grep JOB_NAME_PREFIX
```

Replace `JOB_NAME_PREFIX` with the prefix of your job name. For example, `$USER-llama-3-1-70b-maxtext`. This command will list all pods associated with the specified job, along with their current status.


To get the logs from a specific pod, use the following command:

```bash
kubectl logs POD_NAME
```

Replace `POD_NAME` with the name of the pod you want to inspect.

In this recipe, the training job is orchestrated by the [Kubernetes JobSet](https://jobset.sigs.k8s.io/docs/overview/). If the JobSet encounters a fatal failure, it removes all pods, making it impossible to inspect their logs directly. To analyze logs from a failed job, retrieve them from Cloud Logging using the following filter:

```
resource.type="k8s_container"
resource.labels.project_id="PROJECT_ID"
resource.labels.location="CLUSTER_REGION"
resource.labels.cluster_name="CLUSTER_NAME"
resource.labels.namespace_name="default"
resource.labels.pod_name=~"^JOB_NAME_PREFIX.*"
severity>=DEFAULT
```

Replace the following:
- `PROJECT_ID`: your Google Cloud project ID.
- `CLUSTER_REGION`: the region where your cluster is located.
- `CLUSTER_NAME`: the name of your GKE cluster.
- `JOB_NAME_PREFIX`: the prefix of your job name (e.g., `$USER-llama-3-1-70b-maxtext`).

This filter will retrieve logs from all containers within pods that match the job with the specified name prefix.

### Uninstall the Helm release

You can delete the job and other resources created by the Helm chart. To
uninstall Helm, run the following command from your client:

```bash
helm uninstall $USER-llama-3-1-70b-maxtext-fp8
helm uninstall $USER-llama-3-1-70b-maxtext-bf16
```

