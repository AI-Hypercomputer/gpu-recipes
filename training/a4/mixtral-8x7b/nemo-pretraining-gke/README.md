# Pretrain Mixtral-8x7B workloads on A4 GKE Node pools with Nvidia NeMo Framework

This recipe outlines the steps for running a Mixtral 8x7B pretraining workload on
[A4 GKE Node pools](https://cloud.google.com/kubernetes-engine) by using the
[NVIDIA NeMo framework](https://github.com/NVIDIA/nemo).

## Orchestration and deployment tools

For this recipe, the following setup is used:

- Orchestration - [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)
- Pretraining job configuration and deployment - A Helm chart is used to configure and deploy
  the [Kubernetes Jobset](https://kubernetes.io/blog/2025/03/23/introducing-jobset)
  resource which manages the execution  of the
  [NeMo pretraining workload](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_pretraining.py).

## Test environment

This recipe has been optimized for and tested with the following configuration:

- GKE cluster
    - [A regional standard cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview) version: 1.32.4-gke.1236000 or later.
    - A GPU node pool with 2, 4, 32, 64 or 128
    [a4-highgpu-8g](https://cloud.google.com/compute/docs/accelerator-optimized-machines#a4-high-vms) provisioned using the DENSE deployment type.
    - [Workload Identity Federation for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity) enabled.
    - [Cloud Storage FUSE CSI driver for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/cloud-storage-fuse-csi-driver) enabled.
    - [DCGM metrics](https://cloud.google.com/kubernetes-engine/docs/how-to/dcgm-metrics) enabled.
    - [Kueue](https://kueue.sigs.k8s.io/docs/reference/kueue.v1beta1/) and [JobSet](https://jobset.sigs.k8s.io/docs/overview/) APIs installed.
    - Kueue configured to support [Topology Aware Scheduling](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/).
- A regional Google Cloud Storage (GCS) bucket to store logs generated by the recipe runs.

To prepare the required environment, see
[GKE environment setup guide](../../../../docs/configuring-environment-gke-a4.md).

## Training dataset

This recipe uses a mock pretraining dataset provided by the NeMo framework

## Docker container image

This recipe uses the following [Deep Learning Software Layer](https://cloud.google.com/ai-hypercomputer/docs/software-stack#cluster_images) container image:

`us-central1-docker.pkg.dev/deeplearning-images/reproducibility/pytorch-gpu-nemo-nccl:nemo25.02-gib1.0.5-A4`.

This image is based on NVIDIA NeMo 25.02 and contains the NCCL gIB plugin v1.0.5, bundling all NCCL binaries validated for use with A4 GPUs.


## Run the recipe

From your client workstation, complete the following steps:

### Configure environment settings

Set the environment variables to match your environment:

 ```bash
 export PROJECT_ID=<PROJECT_ID>
 export CLUSTER_REGION=<CLUSTER_REGION>
 export CLUSTER_NAME=<CLUSTER_NAME>
 export GCS_BUCKET=<GCS_BUCKET>
 export KUEUE_NAME=<KUEUE_NAME>
 ```

Replace the following values:

 - `<PROJECT_ID>`: your Google Cloud project ID.
 - `<CLUSTER_REGION>`: the region where your cluster is located.
 - `<CLUSTER_NAME>`: the name of your GKE cluster.
 - `<GCS_BUCKET>`: the name of your Cloud Storage bucket. Don't include the `gs://` prefix.
 - `<KUEUE_NAME>`: the name of the Kueue local queue. The default queue created by the cluster toolkit is `a4`. Make sure to verify the name of the local queue in your cluster.

Set the default project:

 ```bash
 gcloud config set project $PROJECT_ID
 ```

### Get the recipe

Clone the `gpu-recipes` repository and set a reference to the recipe folder.

```
git clone https://github.com/ai-hypercomputer/gpu-recipes.git
cd gpu-recipes
export REPO_ROOT=`git rev-parse --show-toplevel`
export RECIPE_ROOT=$REPO_ROOT/training/a4/mixtral-8x7b/nemo-pretraining-gke
cd $RECIPE_ROOT
```

### Get cluster credentials

```
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION
```

### Configure and submit a pretraining job

#### Using 32 nodes (256 GPUs) FP8

The default job setting is 50 training steps and fp8 precision. To execute the job with the
default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-256gpus-a4-fp8.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 32 nodes (256 GPUs) BF16

The default job setting is 50 training steps and bf16 precision. To execute the job with the
default settings, run the following command:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-256gpus-a4-bf16.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 4 nodes (32 GPUs) FP8

The default job setting is 50 training steps and fp8 precision. To execute the job with the
default settings, run the following command:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-16-32-gpus-a4-fp8.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.gpus=32 \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 4 nodes (32 GPUs) BF16

The default job setting is 50 training steps and bf16 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-16-32-gpus-a4-bf16.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.gpus=32 \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 2 nodes (16 GPUs) FP8

The default job setting is 50 training steps and fp8 precision. To execute the job with the
default settings, run the following command:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-16-32-gpus-a4-fp8.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.gpus=16 \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 2 nodes (16 GPUs) BF16

The default job setting is 50 training steps and bf16 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-16-32-gpus-a4-bf16.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.gpus=16 \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 64 nodes (512 GPUs) FP8

The default job setting is 30 training steps and fp8 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-512gpus-a4-fp8.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.gpus=512 \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Using 128 nodes (1024 GPUs) FP8

The default job setting is 30 training steps and fp8 precision. To execute the
job with the default settings, run the following command from your client:

```bash
helm  install -f $RECIPE_ROOT/values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
    --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-1024gpus-a4-fp8.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.gpus=1024 \
    $USER-mixtral-8x7b-nemo \
    $REPO_ROOT/src/helm-charts/a4/jobset
```

#### Configure job settings

You can overwrite any of the default
[NeMo configurations](../../../../src/frameworks/a4/nemo-configs/mixtral-8x7b-256gpus-a4-fp8.yaml)
for this job. To do this, we can set the new arguments using `--set workload.arguments`.

**Examples**

-   To set the number of training steps to 100, run the following command from
    your client:

    ```bash
    helm  install -f $RECIPE_ROOT/values.yaml \
        --set-file workload_launcher=$REPO_ROOT/src/launchers/nemo-10-launcher.sh \
        --set-file workload_config=$REPO_ROOT/src/frameworks/a4/nemo-configs/mixtral-8x7b-256gpus-a4-fp8.yaml \
        --set queue=${KUEUE_NAME} \
        --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
        --set workload.arguments[0]="trainer.max_steps=100" \
        $USER-mixtral-8x7b-nemo \
        $REPO_ROOT/src/helm-charts/a4/jobset
    ```

### Monitor the job

To check the status of pods in your job, run the following command:

```
kubectl get pods | grep JOB_NAME_PREFIX
```

Replace the following:
- JOB_NAME_PREFIX - your job name prefix. For example $USER-mixtral-8x7b-nemo.

To get the logs for one of the pods, run the following command:

```
kubectl logs POD_NAME
```

Information about the training job's progress, including crucial details such as loss,
step count, and step time, is generated by the rank 0 process.
This process runs on the pod whose name begins with `JOB_NAME_PREFIX-workload-0-0`.
For example: `user-mixtral-8x7b-nemo-workload-0-0-s9zrv`.

### Analyze results

When completed, the job creates several artifacts, including logs and traces, and places them
in the configured Google Cloud Storage bucket as follows:

```
gs://${GCS_BUCKET}/nemo-experiments/<JOB_ID>
├── hparams.yaml
├── lightning_logs.txt
├── nemo_error_logs.txt
├── nemo_log_globalrank-[RANK]_localrank-[LOCAL].txt
├── dllogger
│   ├── rank-0
│   │   ├── dllogger.json
...
```

- `hparams.yaml`: the NeMo configuration used by the pretraining script. This includes
   the combined [configuration file](../../../../src/frameworks/a4/nemo-configs/mixtral-8x7b-256gpus-a4-bf16.yaml)
   and the command line overrides
- `lightning_logs.txt`: the log files generated by PyTorch Lightning, which is used by NeMo
- `nemo_error_logs.txt`: the warning and error logs generated by NeMo
- `nemo_log_globalrank-[RANK]_localrank-[LOCAL].txt`: the NeMo logs for each rank
- `dllogger/: The log captured by [NVIDIA DLLogger](https://github.com/NVIDIA/dllogger)`:
   DLLogger is configured to store logs on the rank 0 node. The log is in JSON format
   and includes loss, step_time, and other key metrics for each training step

The JOB_ID has the following format:

$USER-mixtral-8x7b-nemo-[YYYY]-[MM]-[DD]-[hh]-[mm]-[ss], where the suffix of the ID is a day and time when the job was started.

Here is an example of an entry in the DLLogger log:

```json
DLLL {
  "timestamp": "1742329596.862594",
  "datetime": "2025-03-18 20:26:36.862594",
  "elapsedtime": "347.470315",
  "type": "LOG",
  "step": 38,
  "data":
  {
    "reduced_train_loss": 2.0715765953063965,
    "lr": 5.974842679279391e-06,
    "global_step": 38.0,
    "consumed_samples": 39936.0,
    "train_backward_timing in s": 4.15802001953125e-05,
    "train_step_timing in s": 1.0826486110687257,
    "epoch": 0
  }
}
```

The DLLogger log can be used to calculate the Model FLOPS Utilization (MFU) metric,
as described in the next section.

### Calculate training performance metrics (MFU, TFLOPS, Average Step Time)

This section explains how to calculate key training performance metrics,
such as Model FLOPS Utilization (MFU), using the `dllogger.json` file generated during training.

We provide a tool called [training_metrics](../../../../src/utils/training_metrics/) to help
you easily compute these metrics. This tool can calculate the following metrics:

- *MFU*: Model FLOPS Utilization
- *Average training step time*: the average time taken for each training step
- *TFLOPS per GPU*: the number of Tera Floating Point Operations per second achieved by each GPU

To calculate training performance metrics using the `training_metrics` tool, complete the
following steps command from your client:

1. Download the `dllogger.json` file. The `dllogger.json` file is generated during the
   training session.

    To download the file, run the following command. Replace `<JOB_ID>` with the ID of your
    training session.

    ```bash
    gcloud storage cp gs://${GCS_BUCKET}/nemo-experiments/megatron_gpt/<JOB_ID>/dllogger/rank-0/dllogger.json \
        /path/to/your/local/dllogger.json
    ```

2. Run the
   [`process_training_results.py`](../../../../src/utils/training_metrics/process_training_results.py)
   script

   ```bash
   cd $REPO_ROOT/src/utils/training_metrics
   python3 process_training_results.py --file $RECIPE_ROOT/dllogger.json \
   --batch_size 1024 \
   --num_accelerators 256 \
   --precision fp8 \
   --model_type mixtral-7b \
   --accelerator_type b200
   ```

**Note:** The `batch_size`, `num_accelerators`, `precision`, `model_type` and `accelerator_type` are the
specific values for this recipe running the default configuration with 32 nodes. Average step time
is computed by default using the steps 10 to 30.

For more detailed information and advanced usage instructions of this tool,
see the [full documentation](../../../../src/utils/training_metrics/README.md)

### Troubleshooting

This section provides guidance on troubleshooting issues with the training job.

To check the status of the job's pods, use the following command:

```bash
kubectl get pods | grep JOB_NAME_PREFIX
```

Replace `JOB_NAME_PREFIX` with the prefix of your job name. For example, `$USER-mixtral-8x7b-nemo`. This command will list all pods associated with the specified job, along with their current status.


To get the logs from a specific pod, use the following command:

```bash
kubectl logs POD_NAME
```

Replace `POD_NAME` with the name of the pod you want to inspect.

In this recipe, the training job is orchestrated by the [Kubernetes JobSet](https://jobset.sigs.k8s.io/docs/overview/). If the JobSet encounters a fatal failure, it removes all pods, making it impossible to inspect their logs directly. To analyze logs from a failed job, retrieve them from Cloud Logging using the following filter:

```
resource.type="k8s_container"
resource.labels.project_id="PROJECT_ID"
resource.labels.location="CLUSTER_REGION"
resource.labels.cluster_name="CLUSTER_NAME"
resource.labels.namespace_name="default"
resource.labels.pod_name=~"^JOB_NAME_PREFIX.*"
severity>=DEFAULT
```

Replace the following:
- `PROJECT_ID`: your Google Cloud project ID.
- `CLUSTER_REGION`: the region where your cluster is located.
- `CLUSTER_NAME`: the name of your GKE cluster.
- `JOB_NAME_PREFIX`: the prefix of your job name (e.g., `$USER-mixtral-8x7b-nemo`).

This filter will retrieve logs from all containers within pods that match the job with the specified name prefix.


### Uninstall the Helm release

You can delete the job and other resources created by the Helm chart.
To uninstall Helm, run the following command from your client:

```bash
helm uninstall $USER-mixtral-8x7b-nemo
```

