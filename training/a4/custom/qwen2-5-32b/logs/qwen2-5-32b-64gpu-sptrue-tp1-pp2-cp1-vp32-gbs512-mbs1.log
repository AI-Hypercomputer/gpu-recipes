Pod on gke-gke-a4-sbrg1-a4-highgpu-8g-a4-hig-03548386-jrkr.us-east4-b.c.supercomputer-testing.internal is running
Pod is assigned job index of 0
Job ID is qwen25-32b-64gpu-1749768238-ocg0
The following GPUs are visible via nvidia-smi:
GPU 0: NVIDIA B200 (UUID: GPU-4c4633c6-e9be-e850-7247-11a4ca05188d)
GPU 1: NVIDIA B200 (UUID: GPU-fa794847-43dd-a97b-06b1-7f61da98b927)
GPU 2: NVIDIA B200 (UUID: GPU-2dfe09a3-0ed2-8ead-c8af-6880f75e3bee)
GPU 3: NVIDIA B200 (UUID: GPU-0ec3b569-3753-e3e8-a720-5599a6245d4a)
GPU 4: NVIDIA B200 (UUID: GPU-27a2fcc9-ea91-f308-530a-fb3882b033a3)
GPU 5: NVIDIA B200 (UUID: GPU-f2239141-35ac-46e2-3730-8df448fe61b8)
GPU 6: NVIDIA B200 (UUID: GPU-53e32a69-3f47-2fc3-9456-f88007700449)
GPU 7: NVIDIA B200 (UUID: GPU-e0b9662f-ec59-8429-1930-35c4c9d24abf)
nvidia-smi:
Thu Jun 12 22:44:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA B200                    Off |   00000000:8F:00.0 Off |                    0 |
| N/A   36C    P0            143W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA B200                    Off |   00000000:90:00.0 Off |                    0 |
| N/A   43C    P0            148W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA B200                    Off |   00000000:96:00.0 Off |                    0 |
| N/A   35C    P0            143W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA B200                    Off |   00000000:97:00.0 Off |                    0 |
| N/A   43C    P0            150W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA B200                    Off |   00000000:C4:00.0 Off |                    0 |
| N/A   36C    P0            144W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA B200                    Off |   00000000:C5:00.0 Off |                    0 |
| N/A   43C    P0            149W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA B200                    Off |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            141W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA B200                    Off |   00000000:CC:00.0 Off |                    0 |
| N/A   43C    P0            147W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Local SSD contents (path /ssd):
  containerd
  hello-from-gke-gke-a4-sbrg1-a4-highgpu-8g-a4-hig-03548386-jrkr.txt
  kubelet
  log_pods
  lost+found
Contents (mounted at /usr/local/nccl-plugin/):
Warning: Set LD_LIBRARY_PATH=/usr/local/gib/lib64:/usr/local/nvidia/lib64:/usr/local/mpi/lib:/usr/local/lib/python3.12/dist-packages/torch/lib:/usr/local/lib/python3.12/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/tensorrt/lib to override the NCCL library
/usr/local/gib/scripts/set_nccl_env.sh: line 16: lspci: command not found
/usr/local/gib/scripts/set_nccl_env.sh: line 17: lspci: command not found
/sbin/ldconfig.real: /usr/local/nvidia/lib64/libnccl-net.so is not a symbolic link

Added /usr/local/nvidia/lib64/ to ldconfig:
  	libcudart.so.12 (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12
  	libcudart.so (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so
  	libcudadebugger.so.1 (libc6,x86-64) => /usr/local/nvidia/lib64/libcudadebugger.so.1
  	libcuda_wrapper.so.0 (libc6,x86-64) => /opt/hpcx/clusterkit/lib/libcuda_wrapper.so.0
  	libcuda_wrapper.so (libc6,x86-64) => /opt/hpcx/clusterkit/lib/libcuda_wrapper.so
  	libcuda.so.1 (libc6,x86-64) => /usr/local/nvidia/lib64/libcuda.so.1
  	libcuda.so (libc6,x86-64) => /usr/local/nvidia/lib64/libcuda.so
Got request to JIT mount GCS bucket benchmark-artifacts via 'gcsfuse' to /gcs:
{"timestamp":{"seconds":1749768243,"nanos":918911899},"severity":"INFO","message":"Start gcsfuse/2.12.2 (Go version go1.24.0) for app \"\" using mount point: /gcs\n"}
{"timestamp":{"seconds":1749768243,"nanos":918969112},"severity":"INFO","message":"GCSFuse config","config":{"AppName":"","CacheDir":"","Debug":{"ExitOnInvariantViolation":false,"Fuse":false,"Gcs":false,"LogMutex":false},"DisableAutoconfig":true,"EnableAtomicRenameObject":false,"EnableHns":true,"EnableNewReader":false,"FileCache":{"CacheFileForRangeRead":false,"DownloadChunkSizeMb":200,"EnableCrc":false,"EnableODirect":false,"EnableParallelDownloads":false,"ExperimentalParallelDownloadsDefaultOn":true,"MaxParallelDownloads":448,"MaxSizeMb":-1,"ParallelDownloadsPerFile":16,"WriteBufferSize":4194304},"FileSystem":{"DirMode":"755","DisableParallelDirops":false,"FileMode":"644","FuseOptions":[],"Gid":-1,"IgnoreInterrupts":true,"KernelListCacheTtlSecs":0,"PreconditionErrors":true,"RenameDirLimit":0,"TempDir":"","Uid":-1},"Foreground":false,"GcsAuth":{"AnonymousAccess":false,"KeyFile":"","ReuseTokenFromUrl":true,"TokenUrl":""},"GcsConnection":{"BillingProject":"","ClientProtocol":"http2","CustomEndpoint":"","ExperimentalEnableJsonRead":false,"GrpcConnPoolSize":1,"HttpClientTimeout":0,"LimitBytesPerSec":-1,"LimitOpsPerSec":-1,"MaxConnsPerHost":0,"MaxIdleConnsPerHost":100,"SequentialReadSizeMb":200},"GcsRetries":{"ChunkTransferTimeoutSecs":10,"MaxRetryAttempts":0,"MaxRetrySleep":30000000000,"Multiplier":2,"ReadStall":{"Enable":false,"InitialReqTimeout":20000000000,"MaxReqTimeout":1200000000000,"MinReqTimeout":1500000000,"ReqIncreaseRate":15,"ReqTargetPercentile":0.99}},"ImplicitDirs":false,"List":{"EnableEmptyManagedFolders":false},"Logging":{"FilePath":"","Format":"json","LogRotate":{"BackupFileCount":10,"Compress":true,"MaxFileSizeMb":512},"Severity":"INFO"},"MachineType":"","MetadataCache":{"DeprecatedStatCacheCapacity":20460,"DeprecatedStatCacheTtl":60000000000,"DeprecatedTypeCacheTtl":60000000000,"EnableNonexistentTypeCache":false,"ExperimentalMetadataPrefetchOnMount":"disabled","NegativeTtlSecs":5,"StatCacheMaxSizeMb":32,"TtlSecs":60,"TypeCacheMaxSizeMb":4},"Metrics":{"CloudMetricsExportIntervalSecs":0,"PrometheusPort":0,"StackdriverExportInterval":0,"UseNewNames":false},"Monitoring":{"ExperimentalTracingMode":"","ExperimentalTracingSamplingRatio":0},"OnlyDir":"","Write":{"BlockSizeMb":33554432,"CreateEmptyFile":false,"EnableStreamingWrites":false,"GlobalMaxBlocks":9223372036854775807,"MaxBlocksPerFile":1}}}
{"timestamp":{"seconds":1749768244,"nanos":106488196},"severity":"INFO","message":"File system has been successfully mounted."}
Downloading GPT vocabulary files
--2025-06-12 22:44:04--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.24.93, 3.5.16.15, 16.182.36.32, ...
Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.24.93|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1042301 (1018K) [application/json]
Saving to: ‘gpt2-vocab.json’

     0K .......... .......... .......... .......... ..........  4% 9.71M 0s
    50K .......... .......... .......... .......... ..........  9% 14.9M 0s
   100K .......... .......... .......... .......... .......... 14% 9.98M 0s
   150K .......... .......... .......... .......... .......... 19% 11.9M 0s
   200K .......... .......... .......... .......... .......... 24% 14.8M 0s
   250K .......... .......... .......... .......... .......... 29% 9.09M 0s
   300K .......... .......... .......... .......... .......... 34%  469M 0s
   350K .......... .......... .......... .......... .......... 39% 20.9M 0s
   400K .......... .......... .......... .......... .......... 44%  297M 0s
   450K .......... .......... .......... .......... .......... 49%  293M 0s
   500K .......... .......... .......... .......... .......... 54% 22.6M 0s
   550K .......... .......... .......... .......... .......... 58%  364M 0s
   600K .......... .......... .......... .......... .......... 63%  485M 0s
   650K .......... .......... .......... .......... .......... 68%  465M 0s
   700K .......... .......... .......... .......... .......... 73%  492M 0s
   750K .......... .......... .......... .......... .......... 78%  304M 0s
   800K .......... .......... .......... .......... .......... 83%  577M 0s
   850K .......... .......... .......... .......... .......... 88% 29.1M 0s
   900K .......... .......... .......... .......... .......... 93%  157M 0s
   950K .......... .......... .......... .......... .......... 98%  514M 0s
  1000K .......... .......                                    100%  621M=0.03s

2025-06-12 22:44:04 (29.5 MB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]

--2025-06-12 22:44:04--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.60.128, 52.217.162.48, 54.231.195.160, ...
Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.60.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 456318 (446K) [text/plain]
Saving to: ‘gpt2-merges.txt’

     0K .......... .......... .......... .......... .......... 11% 20.5M 0s
    50K .......... .......... .......... .......... .......... 22% 21.0M 0s
   100K .......... .......... .......... .......... .......... 33% 21.9M 0s
   150K .......... .......... .......... .......... .......... 44%  370M 0s
   200K .......... .......... .......... .......... .......... 56%  307M 0s
   250K .......... .......... .......... .......... .......... 67% 25.3M 0s
   300K .......... .......... .......... .......... .......... 78%  245M 0s
   350K .......... .......... .......... .......... .......... 89%  398M 0s
   400K .......... .......... .......... .......... .....     100%  464M=0.01s

2025-06-12 22:44:04 (45.4 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]

NeMo configuration file:
| 
| run:
|   name: qwen25_32b_64gpu_tp1_pp2_cp1_vp32_gbs512_mbs1
|   results_dir: ${base_results_dir}/${.name}
|   time_limit: "0-02:30:00"
|   dependency: "singleton"
| trainer:
|   devices: 8
|   accelerator: gpu
|   precision: bf16
|   logger: false # logger provided by exp_manager
|   enable_checkpointing: false
|   use_distributed_sampler: false
|   max_epochs: null
|   max_steps: 20 # consumed_samples = global_step * global_batch_size
|   max_time: "05:23:30:00" # days:hours:minutes:seconds
|   log_every_n_steps: 1
|   val_check_interval: null
|   limit_val_batches: 0.0
|   limit_test_batches: 5
|   accumulate_grad_batches: 1
|   gradient_clip_val: 1.0
| exp_manager:
|   exp_dir: null
|   name: megatron_llama
|   create_wandb_logger: false
|   wandb_logger_kwargs:
|     project: nemo_llama_pretrain
|     name: ${run.name}
|   create_dllogger_logger: True
|   dllogger_logger_kwargs:
|     verbose: True
|     stdout: True
|     json_file: "./dllogger.json"     
|   resume_if_exists: false
|   resume_ignore_no_checkpoint: true
|   create_checkpoint_callback: false
|   checkpoint_callback_params:
|     monitor: val_loss
|     save_top_k: 10
|     mode: min
|     always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
|     save_nemo_on_train_end: False # not recommended when training large models on clusters with short time limits
|     filename: 'megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}'
|     model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
|   log_step_timing: true
|   step_timing_kwargs:
|     sync_cuda: true
|     buffer_size: 5
|   seconds_to_sleep: 60
| model:
|   override_vocab_size: 152064
|   tp_only_amax_red: true
|   mcore_gpt: true
|   micro_batch_size: 1
|   global_batch_size: 512
|   rampup_batch_size: null
|   tensor_model_parallel_size: 1
|   pipeline_model_parallel_size: 2
|   # virtual_pipeline_model_parallel_size: 18
|   # virtual_pipeline_model_parallel_size: 14
|   # virtual_pipeline_model_parallel_size: 7
|   virtual_pipeline_model_parallel_size: 32
|   context_parallel_size: 1
|   # context_parallel_size: 2
|   encoder_seq_length: 4096
|   max_position_embeddings: 4096
|   num_layers: 64
|   hidden_size: 5120
|   ffn_hidden_size: 27648
|   num_attention_heads: 40
|   num_query_groups: 8
|   init_method_std: 0.02
|   use_scaled_init_method: true
|   hidden_dropout: 0.0
|   attention_dropout: 0.0
|   ffn_dropout: 0.0
|   kv_channels: null
|   apply_query_key_layer_scaling: true
|   normalization: rmsnorm
|   layernorm_epsilon: 1.0e-05
|   do_layer_norm_weight_decay: false
|   make_vocab_size_divisible_by: 128
|   pre_process: true
|   post_process: true
|   persist_layer_norm: true
|   bias: false
|   activation: fast-swiglu
|   headscale: false
|   transformer_block_type: pre_ln
|   openai_gelu: false
|   normalize_attention_scores: true
|   position_embedding_type: rope
|   rotary_percentage: 1.0
|   apply_rope_fusion: true
|   attention_type: multihead
|   share_embeddings_and_output_weights: false
|   scale_positional_embedding: true
|   tokenizer:
|     library: 'megatron'
|     type: 'GPT2BPETokenizer'
|     model: null
|     delimiter: null  # only used for tabular tokenizer
|     vocab_file: gpt2-vocab.json
|     merge_file: gpt2-merges.txt     
| #  tokenizer:
| #    library: huggingface
| #    type: meta-llama/Meta-Llama-3.1-8B
| #    use_fast: true
|   native_amp_init_scale: 4294967296
|   native_amp_growth_interval: 1000
|   hysteresis: 2
|   fp32_residual_connection: false
|   fp16_lm_cross_entropy: false
|   megatron_amp_O2: true
|   grad_allreduce_chunk_size_mb: 125
|   grad_div_ar_fusion: true
|   gradient_accumulation_fusion: true
|   cross_entropy_loss_fusion: true
|   bias_activation_fusion: true
|   bias_dropout_add_fusion: true
|   masked_softmax_fusion: true
|   seed: 1234
|   resume_from_checkpoint: null
|   use_cpu_initialization: false
|   onnx_safe: false
|   apex_transformer_log_level: 30
|   gradient_as_bucket_view: true
|   sync_batch_comm: false
|   activations_checkpoint_granularity: null
|   activations_checkpoint_method: null
|   activations_checkpoint_num_layers: null
|   num_micro_batches_with_partial_activation_checkpoints: null
|   activations_checkpoint_layers_per_pipeline: null
|   sequence_parallel: false
|   defer_embedding_wgrad_compute: false
|   wgrad_deferral_limit: 50
|   deterministic_mode: false
| 
|   ## Transformer Engine
|   transformer_engine: true
|   fp8: false # enables fp8 in TransformerLayer forward
|   # enable_cuda_graph: true
|   # use_te_rng_tracker: true    
|   # fp8: false # enables fp8 in TransformerLayer forward
|   fp8_e4m3: False # sets fp8_format = recipe.Format.E4M3
|   fp8_hybrid: true # sets fp8_format = recipe.Format.HYBRID
|   fp8_margin: 0 # scaling margin
|   fp8_interval: 1 # scaling update interval
|   fp8_amax_history_len: 1024 # Number of steps for which amax history is recorded per tensor
|   fp8_amax_compute_algo: max # 'most_recent' or 'max'. Algorithm for computing amax from history
|   ub_tp_comm_overlap: true
|   ub_tp_comm_overlap_cfg:
| #    # UB communicator configurations
| #    # Model configs: H100/405B/TP8/CP2/MBS1/SeqLen8K/FP8
| #    
| #    # Bulk overlap with AllGather / ReduceScatter
| #    qkv_dgrad:
| #      method: bulk
| #      num_sm: 2
| #      cga_size: 2
| #      set_sm_margin: 0
| #    
| #    qkv_wgrad:
| #      method: bulk
| #      num_sm: 24
| #      cga_size: 2
| #      set_sm_margin: 0
| #    
| #    fc1_dgrad:
| #      method: bulk
| #      num_sm: 2
| #      cga_size: 2
| #      set_sm_margin: 0
| #    
| #    fc1_wgrad:
| #      method: bulk
| #      num_sm: 2
| #      cga_size: 2
| #      set_sm_margin: 0
| #    
| #    ## Ring
| #    qkv_fprop:
| #      method: ring_exchange
| #      aggregate: 1
| #    
| #    proj_dgrad:
| #      method: ring_exchange
| #      aggregate: 1
| #    
| #    fc1_fprop:
| #      method: ring_exchange
| #      aggregate: 1
| #    
| #    fc2_dgrad:
| #      method: ring_exchange
| #      aggregate: 0
| #    
| #    # Chunked
| #    proj_fprop:
| #      method: pipeline
| #      num_sm: 24
| #      cga_size: 2
| #      num_splits: 4
| #      set_sm_margin: 1
| #      fp8_buf: 1
| #      atomic_gemm: 0
| #    
| #    fc2_fprop:
| #      method: pipeline
| #      num_sm: 16
| #      cga_size: 2
| #      num_splits: 4
| #      set_sm_margin: 1
| #      fp8_buf: 1
| #      atomic_gemm: 0
|     fc1_dgrad:
|       cga_size: 2
|       method: bulk
|       num_sm: 2
|       set_sm_margin: 0
|     fc1_fprop:
|       aggregate: 0
|       method: ring_exchange
|       num_sm: 1
|       set_sm_margin: 0
|     fc1_wgrad:
|       cga_size: 2
|       method: bulk
|       num_sm: 8
|       set_sm_margin: 0
|     fc2_dgrad:
|       aggregate: 0
|       method: ring_exchange
|       num_sm: 1
|       set_sm_margin: 0
|     fc2_fprop:
|       aggregate: 1
|       method: ring_exchange
|       num_sm: 1
|       set_sm_margin: 1
|     proj_dgrad:
|       aggregate: 0
|       method: ring_exchange
|       num_sm: 1
|       set_sm_margin: 0
|     proj_fprop:
|       aggregate: 1
|       method: ring_exchange
|       num_sm: 1
|       set_sm_margin: 1
|     qkv_dgrad:
|       cga_size: 2
|       method: bulk
|       num_sm: 8
|       set_sm_margin: 0
|     qkv_fprop:
|       aggregate: 0
|       method: ring_exchange
|       num_sm: 1
|       set_sm_margin: 0
|     qkv_wgrad:
|       cga_size: 2
|       method: bulk
|       num_sm: 32
|       set_sm_margin: 0
| 
|   use_flash_attention: true
|   overlap_p2p_comm: true
|   batch_p2p_comm: false
|   gc_interval: 100
|   nsys_profile:
|     enabled: false
|     trace: [nvtx,cuda]
|     start_step: 25  # Global batch to start profiling
|     end_step: 30 # Global batch to end profiling
|     #    ranks: [0, 16, 32, 48, 64, 80, 96, 112]
|     ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575] 
|     gen_shape: False # Generate model and kernel details including input shapes
|   optim:
|     name: distributed_fused_adam
|     # name: mcore_distributed_optim
|     lr: 0.00015
|     weight_decay: 0.1
|     betas:
|     - 0.9
|     - 0.95
|     bucket_cap_mb: 125
|     overlap_grad_sync: true
|     overlap_param_sync: true
|     contiguous_grad_buffer: true
|     contiguous_param_buffer: true
|     grad_sync_dtype: bf16
|     sched:
|       name: CosineAnnealing
|       warmup_steps: 2000
|       constant_steps: 11873
|       min_lr: 1.0e-05
|   fp8_params: true
|   data:
|     # data_impl: mmap
|     data_impl: mock
|     splits_string: 99990,8,2
|     seq_length: 4096
|     skip_warmup: true
|     num_workers: 2
|     dataloader_type: single
|     reset_position_ids: false
|     reset_attention_mask: false
|     eod_mask_loss: false
|     index_mapping_dir: null
Detected the following additional workload arguments:
  +exp_manager.explicit_log_dir=/nemo-experiments/results
  +model.data.index_mapping_dir=/gcs/index_mapping_dir
  +model.data.data_prefix=[1.0,/ssd/.cache/wikipedia-tokenized-for-gpt2]
  +exp_manager.exp_dir=/nemo-experiments/
Checking for presence of nsys:
/usr/local/cuda/bin/nsys
NeMo job artifacts will go to /gcs/nemo-experiments/qwen25-32b-64gpu-1749768238-ocg0/

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]
Get:2 http://archive.ubuntu.com/ubuntu noble InRelease [256 kB]
Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1765 kB]
Get:4 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]
Get:5 https://packages.cloud.google.com/apt gcsfuse-buster InRelease [1231 B]
Get:6 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]
Get:7 https://packages.cloud.google.com/apt gcsfuse-buster/main all Packages [751 B]
Get:8 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Packages [1442 kB]
Get:9 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]
Get:10 https://packages.cloud.google.com/apt gcsfuse-buster/main amd64 Packages [40.8 kB]
Get:11 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]
Get:12 http://archive.ubuntu.com/ubuntu noble/multiverse amd64 Packages [331 kB]
Get:13 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1743 kB]
Get:14 http://archive.ubuntu.com/ubuntu noble/universe amd64 Packages [19.3 MB]
Get:15 http://security.ubuntu.com/ubuntu noble-security/main amd64 Packages [1093 kB]
Get:16 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Packages [22.1 kB]
Get:17 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages [1097 kB]
Get:18 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3899 kB]
Get:19 http://archive.ubuntu.com/ubuntu noble/restricted amd64 Packages [117 kB]
Get:20 http://archive.ubuntu.com/ubuntu noble/main amd64 Packages [1808 kB]
Get:21 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Packages [1495 kB]
Get:22 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [1418 kB]
Get:23 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1400 kB]
Get:24 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Packages [26.7 kB]
Get:25 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Packages [31.8 kB]
Get:26 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Packages [48.0 kB]
Fetched 37.7 MB in 2s (17.7 MB/s)
Reading package lists...
Building dependency tree...
Reading state information...
75 packages can be upgraded. Run 'apt list --upgradable' to see them.
W: https://packages.cloud.google.com/apt/dists/gcsfuse-buster/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.
W: https://packages.cloud.google.com/apt/dists/cloud-sdk/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
Building dependency tree...
Reading state information...
The following additional packages will be installed:
  libbabeltrace1 libc6-dbg libdebuginfod-common libdebuginfod1t64 libdw1t64
  libelf1t64 libipt2 libpython3-dbg libpython3.12t64-dbg
  libsource-highlight-common libsource-highlight4t64 python3.12-dbg
Suggested packages:
  gdb-doc gdbserver python3-gdbm-dbg python3-tk-dbg
The following NEW packages will be installed:
  gdb libbabeltrace1 libc6-dbg libdebuginfod-common libdebuginfod1t64
  libdw1t64 libelf1t64 libipt2 libpython3-dbg libpython3.12t64-dbg
  libsource-highlight-common libsource-highlight4t64 python3-dbg
  python3.12-dbg
0 upgraded, 14 newly installed, 0 to remove and 75 not upgraded.
Need to get 85.3 MB of archives.
After this operation, 217 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libdebuginfod-common all 0.190-1.1ubuntu0.1 [14.6 kB]
Get:2 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libelf1t64 amd64 0.190-1.1ubuntu0.1 [57.8 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libdw1t64 amd64 0.190-1.1ubuntu0.1 [261 kB]
Get:4 http://archive.ubuntu.com/ubuntu noble/main amd64 libbabeltrace1 amd64 1.5.11-3build3 [164 kB]
Get:5 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libdebuginfod1t64 amd64 0.190-1.1ubuntu0.1 [17.1 kB]
Get:6 http://archive.ubuntu.com/ubuntu noble/main amd64 libipt2 amd64 2.0.6-1build1 [45.7 kB]
Get:7 http://archive.ubuntu.com/ubuntu noble/main amd64 libsource-highlight-common all 3.1.9-4.3build1 [64.2 kB]
Get:8 http://archive.ubuntu.com/ubuntu noble/main amd64 libsource-highlight4t64 amd64 3.1.9-4.3build1 [258 kB]
Get:9 http://archive.ubuntu.com/ubuntu noble/main amd64 gdb amd64 15.0.50.20240403-0ubuntu1 [4010 kB]
Get:10 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12t64-dbg amd64 3.12.3-1ubuntu0.5 [24.1 MB]
Get:11 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-dbg amd64 3.12.3-1ubuntu0.5 [48.8 MB]
Get:12 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libc6-dbg amd64 2.39-0ubuntu8.4 [7460 kB]
Get:13 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3-dbg amd64 3.12.3-0ubuntu2 [10.3 kB]
Get:14 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3-dbg amd64 3.12.3-0ubuntu2 [1060 B]
Preconfiguring packages ...
Fetched 85.3 MB in 2s (44.9 MB/s)
Selecting previously unselected package libdebuginfod-common.
(Reading database ... 
(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%
(Reading database ... 75%
(Reading database ... 80%
(Reading database ... 85%
(Reading database ... 90%
(Reading database ... 95%
(Reading database ... 100%
(Reading database ... 78111 files and directories currently installed.)
Preparing to unpack .../00-libdebuginfod-common_0.190-1.1ubuntu0.1_all.deb ...
Unpacking libdebuginfod-common (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libelf1t64:amd64.
Preparing to unpack .../01-libelf1t64_0.190-1.1ubuntu0.1_amd64.deb ...
Unpacking libelf1t64:amd64 (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libdw1t64:amd64.
Preparing to unpack .../02-libdw1t64_0.190-1.1ubuntu0.1_amd64.deb ...
Unpacking libdw1t64:amd64 (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libbabeltrace1:amd64.
Preparing to unpack .../03-libbabeltrace1_1.5.11-3build3_amd64.deb ...
Unpacking libbabeltrace1:amd64 (1.5.11-3build3) ...
Selecting previously unselected package libdebuginfod1t64:amd64.
Preparing to unpack .../04-libdebuginfod1t64_0.190-1.1ubuntu0.1_amd64.deb ...
Unpacking libdebuginfod1t64:amd64 (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libipt2.
Preparing to unpack .../05-libipt2_2.0.6-1build1_amd64.deb ...
Unpacking libipt2 (2.0.6-1build1) ...
Selecting previously unselected package libsource-highlight-common.
Preparing to unpack .../06-libsource-highlight-common_3.1.9-4.3build1_all.deb ...
Unpacking libsource-highlight-common (3.1.9-4.3build1) ...
Selecting previously unselected package libsource-highlight4t64:amd64.
Preparing to unpack .../07-libsource-highlight4t64_3.1.9-4.3build1_amd64.deb ...
Unpacking libsource-highlight4t64:amd64 (3.1.9-4.3build1) ...
Selecting previously unselected package gdb.
Preparing to unpack .../08-gdb_15.0.50.20240403-0ubuntu1_amd64.deb ...
Unpacking gdb (15.0.50.20240403-0ubuntu1) ...
Selecting previously unselected package libpython3.12t64-dbg:amd64.
Preparing to unpack .../09-libpython3.12t64-dbg_3.12.3-1ubuntu0.5_amd64.deb ...
Unpacking libpython3.12t64-dbg:amd64 (3.12.3-1ubuntu0.5) ...
Selecting previously unselected package python3.12-dbg.
Preparing to unpack .../10-python3.12-dbg_3.12.3-1ubuntu0.5_amd64.deb ...
Unpacking python3.12-dbg (3.12.3-1ubuntu0.5) ...
Selecting previously unselected package libc6-dbg:amd64.
Preparing to unpack .../11-libc6-dbg_2.39-0ubuntu8.4_amd64.deb ...
Unpacking libc6-dbg:amd64 (2.39-0ubuntu8.4) ...
Selecting previously unselected package libpython3-dbg:amd64.
Preparing to unpack .../12-libpython3-dbg_3.12.3-0ubuntu2_amd64.deb ...
Unpacking libpython3-dbg:amd64 (3.12.3-0ubuntu2) ...
Selecting previously unselected package python3-dbg.
Preparing to unpack .../13-python3-dbg_3.12.3-0ubuntu2_amd64.deb ...
Unpacking python3-dbg (3.12.3-0ubuntu2) ...
Setting up libpython3.12t64-dbg:amd64 (3.12.3-1ubuntu0.5) ...
Setting up libdebuginfod-common (0.190-1.1ubuntu0.1) ...
Setting up libpython3-dbg:amd64 (3.12.3-0ubuntu2) ...
Setting up python3.12-dbg (3.12.3-1ubuntu0.5) ...
python3.12-dbg: can't get files for byte-compilation
Setting up libsource-highlight-common (3.1.9-4.3build1) ...
Setting up libelf1t64:amd64 (0.190-1.1ubuntu0.1) ...
Setting up libc6-dbg:amd64 (2.39-0ubuntu8.4) ...
Setting up libdw1t64:amd64 (0.190-1.1ubuntu0.1) ...
Setting up libipt2 (2.0.6-1build1) ...
Setting up libbabeltrace1:amd64 (1.5.11-3build3) ...
Setting up python3-dbg (3.12.3-0ubuntu2) ...
Setting up libdebuginfod1t64:amd64 (0.190-1.1ubuntu0.1) ...
Setting up libsource-highlight4t64:amd64 (3.1.9-4.3build1) ...
Setting up gdb (15.0.50.20240403-0ubuntu1) ...
Processing triggers for libc-bin (2.39-0ubuntu8.4) ...
/sbin/ldconfig.real: /usr/local/nvidia/lib64/libnccl-net.so is not a symbolic link

Launching Torch distributed as node rank 0 out of 8 nodes
Launched rank 0 with PID 643
Launched rank 1 with PID 644
Launched rank 2 with PID 645
Launched rank 3 with PID 646
Launched rank 4 with PID 647
Launched rank 5 with PID 648
Launched rank 6 with PID 649
Launched rank 7 with PID 650
Waiting on Torch PID 643
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.

0it [00:00, ?it/s]
0it [00:00, ?it/s]
[NeMo W 2025-06-12 22:44:47 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
    
[NeMo W 2025-06-12 22:44:47 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
    
[NeMo W 2025-06-12 22:44:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/google/protobuf/internal/well_known_types.py:93: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).
      _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0)
    
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1049) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1044) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1080) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1092) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1087) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1144) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1139) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1134) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] /opt/NeMo/nemo/lightning/pytorch/strategies/utils.py:31: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/jieba/_compat.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
      import pkg_resources
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pyannote')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:45:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:45:14 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:14: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
      import audioop
    
[NeMo W 2025-06-12 22:45:14 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo W 2025-06-12 22:45:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
      cm = get_cmap("Set1")
    
[NeMo W 2025-06-12 22:45:18 nemo_logging:405] <frozen importlib._bootstrap_external>:1297: DeprecationWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.
    
[NeMo I 2025-06-12 22:45:19 nemo_logging:393] 
    
    ************** Experiment configuration ***********
[NeMo I 2025-06-12 22:45:19 nemo_logging:393] 
    run:
      name: qwen25_32b_64gpu_tp1_pp2_cp1_vp32_gbs512_mbs1
      results_dir: ${base_results_dir}/${.name}
      time_limit: 0-02:30:00
      dependency: singleton
    trainer:
      devices: 8
      accelerator: gpu
      precision: bf16
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_epochs: null
      max_steps: 20
      max_time: 05:23:30:00
      log_every_n_steps: 1
      val_check_interval: null
      limit_val_batches: 0.0
      limit_test_batches: 5
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
      num_nodes: 8
    exp_manager:
      exp_dir: /nemo-experiments/
      name: megatron_llama
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: nemo_llama_pretrain
        name: ${run.name}
      create_dllogger_logger: true
      dllogger_logger_kwargs:
        verbose: true
        stdout: true
        json_file: ./dllogger.json
      resume_if_exists: false
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: false
      checkpoint_callback_params:
        monitor: val_loss
        save_top_k: 10
        mode: min
        always_save_nemo: false
        save_nemo_on_train_end: false
        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
      log_step_timing: true
      step_timing_kwargs:
        sync_cuda: true
        buffer_size: 5
      seconds_to_sleep: 60
      version: qwen25-32b-64gpu-1749768238-ocg0
      explicit_log_dir: /nemo-experiments/results
    model:
      override_vocab_size: 152064
      tp_only_amax_red: true
      mcore_gpt: true
      micro_batch_size: 1
      global_batch_size: 512
      rampup_batch_size: null
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 2
      virtual_pipeline_model_parallel_size: 32
      context_parallel_size: 1
      encoder_seq_length: 4096
      max_position_embeddings: 4096
      num_layers: 64
      hidden_size: 5120
      ffn_hidden_size: 27648
      num_attention_heads: 40
      num_query_groups: 8
      init_method_std: 0.02
      use_scaled_init_method: true
      hidden_dropout: 0.0
      attention_dropout: 0.0
      ffn_dropout: 0.0
      kv_channels: null
      apply_query_key_layer_scaling: true
      normalization: rmsnorm
      layernorm_epsilon: 1.0e-05
      do_layer_norm_weight_decay: false
      make_vocab_size_divisible_by: 128
      pre_process: true
      post_process: true
      persist_layer_norm: true
      bias: false
      activation: fast-swiglu
      headscale: false
      transformer_block_type: pre_ln
      openai_gelu: false
      normalize_attention_scores: true
      position_embedding_type: rope
      rotary_percentage: 1.0
      apply_rope_fusion: true
      attention_type: multihead
      share_embeddings_and_output_weights: false
      scale_positional_embedding: true
      tokenizer:
        library: megatron
        type: GPT2BPETokenizer
        model: null
        delimiter: null
        vocab_file: gpt2-vocab.json
        merge_file: gpt2-merges.txt
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: true
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: true
      gradient_accumulation_fusion: true
      cross_entropy_loss_fusion: true
      bias_activation_fusion: true
      bias_dropout_add_fusion: true
      masked_softmax_fusion: true
      seed: 1234
      resume_from_checkpoint: null
      use_cpu_initialization: false
      onnx_safe: false
      apex_transformer_log_level: 30
      gradient_as_bucket_view: true
      sync_batch_comm: false
      activations_checkpoint_granularity: null
      activations_checkpoint_method: null
      activations_checkpoint_num_layers: null
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: false
      defer_embedding_wgrad_compute: false
      wgrad_deferral_limit: 50
      deterministic_mode: false
      transformer_engine: true
      fp8: false
      fp8_e4m3: false
      fp8_hybrid: true
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1024
      fp8_amax_compute_algo: max
      ub_tp_comm_overlap: true
      ub_tp_comm_overlap_cfg:
        fc1_dgrad:
          cga_size: 2
          method: bulk
          num_sm: 2
          set_sm_margin: 0
        fc1_fprop:
          aggregate: 0
          method: ring_exchange
          num_sm: 1
          set_sm_margin: 0
        fc1_wgrad:
          cga_size: 2
          method: bulk
          num_sm: 8
          set_sm_margin: 0
        fc2_dgrad:
          aggregate: 0
          method: ring_exchange
          num_sm: 1
          set_sm_margin: 0
        fc2_fprop:
          aggregate: 1
          method: ring_exchange
          num_sm: 1
          set_sm_margin: 1
        proj_dgrad:
          aggregate: 0
          method: ring_exchange
          num_sm: 1
          set_sm_margin: 0
        proj_fprop:
          aggregate: 1
          method: ring_exchange
          num_sm: 1
          set_sm_margin: 1
        qkv_dgrad:
          cga_size: 2
          method: bulk
          num_sm: 8
          set_sm_margin: 0
        qkv_fprop:
          aggregate: 0
          method: ring_exchange
          num_sm: 1
          set_sm_margin: 0
        qkv_wgrad:
          cga_size: 2
          method: bulk
          num_sm: 32
          set_sm_margin: 0
      use_flash_attention: true
      overlap_p2p_comm: true
      batch_p2p_comm: false
      gc_interval: 100
      nsys_profile:
        enabled: false
        trace:
        - nvtx
        - cuda
        start_step: 25
        end_step: 30
        ranks:
        - 0
        - 1
        - 2
        - 3
        - 4
        - 5
        - 6
        - 7
        - 8
        - 9
        - 10
        - 11
        - 12
        - 13
        - 14
        - 15
        - 16
        - 17
        - 18
        - 19
        - 20
        - 21
        - 22
        - 23
        - 24
        - 25
        - 26
        - 27
        - 28
        - 29
        - 30
        - 31
        - 32
        - 33
        - 34
        - 35
        - 36
        - 37
        - 38
        - 39
        - 40
        - 41
        - 42
        - 43
        - 44
        - 45
        - 46
        - 47
        - 48
        - 49
        - 50
        - 51
        - 52
        - 53
        - 54
        - 55
        - 56
        - 57
        - 58
        - 59
        - 60
        - 61
        - 62
        - 63
        - 64
        - 65
        - 66
        - 67
        - 68
        - 69
        - 70
        - 71
        - 72
        - 73
        - 74
        - 75
        - 76
        - 77
        - 78
        - 79
        - 80
        - 81
        - 82
        - 83
        - 84
        - 85
        - 86
        - 87
        - 88
        - 89
        - 90
        - 91
        - 92
        - 93
        - 94
        - 95
        - 96
        - 97
        - 98
        - 99
        - 100
        - 101
        - 102
        - 103
        - 104
        - 105
        - 106
        - 107
        - 108
        - 109
        - 110
        - 111
        - 112
        - 113
        - 114
        - 115
        - 116
        - 117
        - 118
        - 119
        - 120
        - 121
        - 122
        - 123
        - 124
        - 125
        - 126
        - 127
        - 128
        - 129
        - 130
        - 131
        - 132
        - 133
        - 134
        - 135
        - 136
        - 137
        - 138
        - 139
        - 140
        - 141
        - 142
        - 143
        - 144
        - 145
        - 146
        - 147
        - 148
        - 149
        - 150
        - 151
        - 152
        - 153
        - 154
        - 155
        - 156
        - 157
        - 158
        - 159
        - 160
        - 161
        - 162
        - 163
        - 164
        - 165
        - 166
        - 167
        - 168
        - 169
        - 170
        - 171
        - 172
        - 173
        - 174
        - 175
        - 176
        - 177
        - 178
        - 179
        - 180
        - 181
        - 182
        - 183
        - 184
        - 185
        - 186
        - 187
        - 188
        - 189
        - 190
        - 191
        - 192
        - 193
        - 194
        - 195
        - 196
        - 197
        - 198
        - 199
        - 200
        - 201
        - 202
        - 203
        - 204
        - 205
        - 206
        - 207
        - 208
        - 209
        - 210
        - 211
        - 212
        - 213
        - 214
        - 215
        - 216
        - 217
        - 218
        - 219
        - 220
        - 221
        - 222
        - 223
        - 224
        - 225
        - 226
        - 227
        - 228
        - 229
        - 230
        - 231
        - 232
        - 233
        - 234
        - 235
        - 236
        - 237
        - 238
        - 239
        - 240
        - 241
        - 242
        - 243
        - 244
        - 245
        - 246
        - 247
        - 248
        - 249
        - 250
        - 251
        - 252
        - 253
        - 254
        - 255
        - 256
        - 257
        - 258
        - 259
        - 260
        - 261
        - 262
        - 263
        - 264
        - 265
        - 266
        - 267
        - 268
        - 269
        - 270
        - 271
        - 272
        - 273
        - 274
        - 275
        - 276
        - 277
        - 278
        - 279
        - 280
        - 281
        - 282
        - 283
        - 284
        - 285
        - 286
        - 287
        - 288
        - 289
        - 290
        - 291
        - 292
        - 293
        - 294
        - 295
        - 296
        - 297
        - 298
        - 299
        - 300
        - 301
        - 302
        - 303
        - 304
        - 305
        - 306
        - 307
        - 308
        - 309
        - 310
        - 311
        - 312
        - 313
        - 314
        - 315
        - 316
        - 317
        - 318
        - 319
        - 320
        - 321
        - 322
        - 323
        - 324
        - 325
        - 326
        - 327
        - 328
        - 329
        - 330
        - 331
        - 332
        - 333
        - 334
        - 335
        - 336
        - 337
        - 338
        - 339
        - 340
        - 341
        - 342
        - 343
        - 344
        - 345
        - 346
        - 347
        - 348
        - 349
        - 350
        - 351
        - 352
        - 353
        - 354
        - 355
        - 356
        - 357
        - 358
        - 359
        - 360
        - 361
        - 362
        - 363
        - 364
        - 365
        - 366
        - 367
        - 368
        - 369
        - 370
        - 371
        - 372
        - 373
        - 374
        - 375
        - 376
        - 377
        - 378
        - 379
        - 380
        - 381
        - 382
        - 383
        - 384
        - 385
        - 386
        - 387
        - 388
        - 389
        - 390
        - 391
        - 392
        - 393
        - 394
        - 395
        - 396
        - 397
        - 398
        - 399
        - 400
        - 401
        - 402
        - 403
        - 404
        - 405
        - 406
        - 407
        - 408
        - 409
        - 410
        - 411
        - 412
        - 413
        - 414
        - 415
        - 416
        - 417
        - 418
        - 419
        - 420
        - 421
        - 422
        - 423
        - 424
        - 425
        - 426
        - 427
        - 428
        - 429
        - 430
        - 431
        - 432
        - 433
        - 434
        - 435
        - 436
        - 437
        - 438
        - 439
        - 440
        - 441
        - 442
        - 443
        - 444
        - 445
        - 446
        - 447
        - 448
        - 449
        - 450
        - 451
        - 452
        - 453
        - 454
        - 455
        - 456
        - 457
        - 458
        - 459
        - 460
        - 461
        - 462
        - 463
        - 464
        - 465
        - 466
        - 467
        - 468
        - 469
        - 470
        - 471
        - 472
        - 473
        - 474
        - 475
        - 476
        - 477
        - 478
        - 479
        - 480
        - 481
        - 482
        - 483
        - 484
        - 485
        - 486
        - 487
        - 488
        - 489
        - 490
        - 491
        - 492
        - 493
        - 494
        - 495
        - 496
        - 497
        - 498
        - 499
        - 500
        - 501
        - 502
        - 503
        - 504
        - 505
        - 506
        - 507
        - 508
        - 509
        - 510
        - 511
        - 512
        - 513
        - 514
        - 515
        - 516
        - 517
        - 518
        - 519
        - 520
        - 521
        - 522
        - 523
        - 524
        - 525
        - 526
        - 527
        - 528
        - 529
        - 530
        - 531
        - 532
        - 533
        - 534
        - 535
        - 536
        - 537
        - 538
        - 539
        - 540
        - 541
        - 542
        - 543
        - 544
        - 545
        - 546
        - 547
        - 548
        - 549
        - 550
        - 551
        - 552
        - 553
        - 554
        - 555
        - 556
        - 557
        - 558
        - 559
        - 560
        - 561
        - 562
        - 563
        - 564
        - 565
        - 566
        - 567
        - 568
        - 569
        - 570
        - 571
        - 572
        - 573
        - 574
        - 575
        gen_shape: false
      optim:
        name: distributed_fused_adam
        lr: 0.00015
        weight_decay: 0.1
        betas:
        - 0.9
        - 0.95
        bucket_cap_mb: 125
        overlap_grad_sync: true
        overlap_param_sync: true
        contiguous_grad_buffer: true
        contiguous_param_buffer: true
        grad_sync_dtype: bf16
        sched:
          name: CosineAnnealing
          warmup_steps: 2000
          constant_steps: 11873
          min_lr: 1.0e-05
      fp8_params: true
      data:
        data_impl: mock
        splits_string: 99990,8,2
        seq_length: 4096
        skip_warmup: true
        num_workers: 2
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        index_mapping_dir: /gcs/index_mapping_dir
        data_prefix:
        - 1.0
        - /ssd/.cache/wikipedia-tokenized-for-gpt2
    
[NeMo W 2025-06-12 22:45:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `lightning.pytorch.plugins.precision.MixedPrecision` instead.
    
[NeMo W 2025-06-12 22:45:19 nemo_logging:405] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1466: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:750.)
      torch.set_autocast_gpu_dtype(dtype)
    
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] ExpManager schema
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False, 'save_last_n_optim_states': -1}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'log_delta_step_timing': False, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}
[NeMo E 2025-06-12 22:45:21 nemo_logging:417] You are running multi-node training without SLURM handling the processes. Please note that this is not tested in NeMo and could result in errors.
[NeMo E 2025-06-12 22:45:21 nemo_logging:417] exp_manager received explicit_log_dir: /nemo-experiments/results and at least one of exp_dir: /nemo-experiments/, or version: qwen25-32b-64gpu-1749768238-ocg0. Please note that exp_dir, name, and version will be ignored.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] Exp_manager is logging to /nemo-experiments/results, but it already exists.
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Experiments will be logged at /nemo-experiments/results
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] TensorboardLogger has been set up
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] DLLogger has been set up
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_comm_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: hierarchical_context_parallel_sizes in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_tensor_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cross_entropy_fusion_impl in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm_warmup_flush in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: microbatch_group_size_per_vp_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Ranks 0 has data parallel rank: 0
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has context parallel group: [0]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Ranks 0 has context parallel rank: 0
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has model parallel group: [0, 32]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] All model parallel group ranks: [[0, 32], [1, 33], [2, 34], [3, 35], [4, 36], [5, 37], [6, 38], [7, 39], [8, 40], [9, 41], [10, 42], [11, 43], [12, 44], [13, 45], [14, 46], [15, 47], [16, 48], [17, 49], [18, 50], [19, 51], [20, 52], [21, 53], [22, 54], [23, 55], [24, 56], [25, 57], [26, 58], [27, 59], [28, 60], [29, 61], [30, 62], [31, 63]]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has pipeline model parallel group: [0, 32]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has embedding group: [0, 32]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] All pipeline model parallel group ranks: [[0, 32], [1, 33], [2, 34], [3, 35], [4, 36], [5, 37], [6, 38], [7, 39], [8, 40], [9, 41], [10, 42], [11, 43], [12, 44], [13, 45], [14, 46], [15, 47], [16, 48], [17, 49], [18, 50], [19, 51], [20, 52], [21, 53], [22, 54], [23, 55], [24, 56], [25, 57], [26, 58], [27, 59], [28, 60], [29, 61], [30, 62], [31, 63]]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] All embedding group ranks: [[0, 32], [1, 33], [2, 34], [3, 35], [4, 36], [5, 37], [6, 38], [7, 39], [8, 40], [9, 41], [10, 42], [11, 43], [12, 44], [13, 45], [14, 46], [15, 47], [16, 48], [17, 49], [18, 50], [19, 51], [20, 52], [21, 53], [22, 54], [23, 55], [24, 56], [25, 57], [26, 58], [27, 59], [28, 60], [29, 61], [30, 62], [31, 63]]
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Rank 0 has embedding rank: 0
[NeMo I 2025-06-12 22:45:21 num_microbatches_calculator:228] setting number of microbatches to constant 16
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Pipelined tensor-parallel communication overlap is available with sequence-parallelism.Setting `ub_tp_comm_overlap` to False.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_comm_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: hierarchical_context_parallel_sizes in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_tensor_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cross_entropy_fusion_impl in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm_warmup_flush in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: microbatch_group_size_per_vp_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:21 nemo_logging:405] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /app/training/gpt2-vocab.json, and merges file: /app/training/gpt2-merges.txt
[NeMo I 2025-06-12 22:45:21 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /app/training/gpt2-vocab.json, merges_files: /app/training/gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-06-12 22:45:22 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_comm_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: hierarchical_context_parallel_sizes in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_tensor_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cross_entropy_fusion_impl in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm_warmup_flush in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: microbatch_group_size_per_vp_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mtp_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mtp_loss_scaling_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_in_first_pipeline_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_in_last_pipeline_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: softmax_scale in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: init_model_with_meta_device in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: disable_bf16_reduced_precision_matmul in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: recompute_modules in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_recipe in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_param in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: first_last_layers_bf16 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_at_start_in_bf16 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_at_end_in_bf16 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_layer_freq in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_ffn_hidden_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_topk_limited_devices in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_num_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_group_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_topk_scaling_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_score_function in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_dtype in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_enable_expert_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_bias_update_rate in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_use_legacy_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_enable_deepep in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_permute_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_apply_probs_on_input in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_use_single_mempool in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_retain_backward_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_warmup_steps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_scope in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: flash_decode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: inference_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mrope_section in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: is_hybrid_model in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mamba_state_dim in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mamba_head_dim in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mamba_num_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-12 22:45:22 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: heterogeneous_block_specs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
[NeMo W 2025-06-12 22:45:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:161: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
    
[NeMo W 2025-06-12 22:45:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.
    
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

[NeMo I 2025-06-12 22:46:35 nemo_logging:393] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 1.64e+10. Number of precise model parameters on device: 32763417600.
[NeMo I 2025-06-12 22:46:35 nemo_logging:393] Building GPT datasets.
[NeMo I 2025-06-12 22:46:35 utils:532] Let mock = True, as both blend and blend_per_split are None
[NeMo I 2025-06-12 22:46:35 utils:532] Let split = 1,1,1, an arbitrarily even split, as mock is True
[NeMo I 2025-06-12 22:46:35 utils:532] Let split_matrix = [(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)]
[NeMo I 2025-06-12 22:46:35 utils:532] Building MockGPTDataset splits with sizes=[10240, None, 2560] and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=None, blend_per_split=None, split='1,1,1', split_matrix=[(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)], num_dataset_builder_threads=1, path_to_cache='/gcs/index_mapping_dir', mmap_bin_files=True, mock=True, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x782d99e67990>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)
[NeMo I 2025-06-12 22:46:36 utils:532] Load the MockGPTDataset train indices
[NeMo I 2025-06-12 22:46:36 utils:532] 	Load the document index from b0169179e72fdea5237bf0f8d44cc585-MockGPTDataset-train-document_index.npy
[NeMo I 2025-06-12 22:46:36 utils:532] 	Load the sample index from b0169179e72fdea5237bf0f8d44cc585-MockGPTDataset-train-sample_index.npy
[NeMo I 2025-06-12 22:46:36 utils:532] 	Load the shuffle index from b0169179e72fdea5237bf0f8d44cc585-MockGPTDataset-train-shuffle_index.npy
[NeMo I 2025-06-12 22:46:36 utils:532] > total number of samples: 16648
[NeMo I 2025-06-12 22:46:36 utils:532] Load the MockGPTDataset valid indices
[NeMo I 2025-06-12 22:46:36 utils:532] 	Load the document index from e0fcd0b41a52fdff26a08c0b8bddb0cf-MockGPTDataset-valid-document_index.npy
[NeMo I 2025-06-12 22:46:36 utils:532] 	Load the sample index from e0fcd0b41a52fdff26a08c0b8bddb0cf-MockGPTDataset-valid-sample_index.npy
[NeMo I 2025-06-12 22:46:36 utils:532] 	Load the shuffle index from e0fcd0b41a52fdff26a08c0b8bddb0cf-MockGPTDataset-valid-shuffle_index.npy
[NeMo I 2025-06-12 22:46:36 utils:532] > total number of samples: 16640
[NeMo I 2025-06-12 22:46:37 utils:532] Load the MockGPTDataset test indices
[NeMo I 2025-06-12 22:46:37 utils:532] 	Load the document index from c7cc10fef601a68517295327a1dcedfe-MockGPTDataset-test-document_index.npy
[NeMo I 2025-06-12 22:46:37 utils:532] 	Load the sample index from c7cc10fef601a68517295327a1dcedfe-MockGPTDataset-test-sample_index.npy
[NeMo I 2025-06-12 22:46:37 utils:532] 	Load the shuffle index from c7cc10fef601a68517295327a1dcedfe-MockGPTDataset-test-shuffle_index.npy
[NeMo I 2025-06-12 22:46:37 utils:532] > total number of samples: 16671
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Length of train dataset: 16648
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Length of val dataset: 16640
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Length of test dataset: 16671
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Finished building GPT datasets.
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Setting up train dataloader with len(len(self._train_ds)): 16648 and consumed samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Building dataloader with consumed samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Instantiating MegatronPretrainingSampler with total_samples: 16648 and consumed_samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Setting up validation dataloader with len(len(self._validation_ds)): 16640 and consumed samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Building dataloader with consumed samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Instantiating MegatronPretrainingSampler with total_samples: 16640 and consumed_samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Setting up test dataloader with len(len(self._test_ds)): 16671 and consumed samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Building dataloader with consumed samples: 0
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Instantiating MegatronPretrainingSampler with total_samples: 16671 and consumed_samples: 0
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Optimizer config = MegatronDistributedFusedAdam (
    Parameter Group 0
        betas: [0.9, 0.95]
        bias_correction: True
        eps: 1e-08
        is_expert: False
        lr: 0.00015
        weight_decay: 0.1
    adam_w_mode: True
    amsgrad: False
    dtype: torch.float32
    grad_sync_dtype: torch.bfloat16
    param_sync_dtype: torch.bfloat16
    device: cuda:0
    process_group: 0x782d999d27a0, world size 32
    distributed_process_group: 0x782d999d27a0, world size 32
    redundant_process_group: None
    average_grad_sync: True
    overlap_grad_sync: True
    overlap_param_sync: True
    bucket_cap_mb: 125
    pipeline_size: 2
    contiguous_param_buffer: True
    contiguous_grad_buffer: True
    store_params: True
    store_param_remainders: False
    with_scaled_states: False
    nccl_ub: False
    capturable: False
    )
[NeMo I 2025-06-12 22:46:37 nemo_logging:393] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x782d99d33a30>" 
    will be used during training (effective maximum steps = 20) - 
    Parameters : 
    (warmup_steps: 2000
    constant_steps: 11873
    min_lr: 1.0e-05
    max_steps: 20
    )

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 16.4 B | n/a 
---------------------------------------------
16.4 B    Trainable params
0         Non-trainable params
16.4 B    Total params
65,526.825Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
DLL 2025-06-12 22:46:38.569166 - PARAMETER cfg/override_vocab_size : 152064  cfg/tp_only_amax_red : True  cfg/mcore_gpt : True  cfg/micro_batch_size : 1  cfg/global_batch_size : 512  cfg/rampup_batch_size : None  cfg/tensor_model_parallel_size : 1  cfg/pipeline_model_parallel_size : 2  cfg/virtual_pipeline_model_parallel_size : 32  cfg/context_parallel_size : 1  cfg/encoder_seq_length : 4096  cfg/max_position_embeddings : 4096  cfg/num_layers : 64  cfg/hidden_size : 5120  cfg/ffn_hidden_size : 27648  cfg/num_attention_heads : 40  cfg/num_query_groups : 8  cfg/init_method_std : 0.02  cfg/use_scaled_init_method : True  cfg/hidden_dropout : 0.0  cfg/attention_dropout : 0.0  cfg/ffn_dropout : 0.0  cfg/kv_channels : None  cfg/apply_query_key_layer_scaling : True  cfg/normalization : rmsnorm  cfg/layernorm_epsilon : 1e-05  cfg/do_layer_norm_weight_decay : False  cfg/make_vocab_size_divisible_by : 128  cfg/pre_process : True  cfg/post_process : True  cfg/persist_layer_norm : True  cfg/bias : False  cfg/activation : fast-swiglu  cfg/headscale : False  cfg/transformer_block_type : pre_ln  cfg/openai_gelu : False  cfg/normalize_attention_scores : True  cfg/position_embedding_type : rope  cfg/rotary_percentage : 1.0  cfg/apply_rope_fusion : True  cfg/attention_type : multihead  cfg/share_embeddings_and_output_weights : False  cfg/scale_positional_embedding : True  cfg/tokenizer/library : megatron  cfg/tokenizer/type : GPT2BPETokenizer  cfg/tokenizer/model : None  cfg/tokenizer/delimiter : None  cfg/tokenizer/vocab_file : gpt2-vocab.json  cfg/tokenizer/merge_file : gpt2-merges.txt  cfg/native_amp_init_scale : 4294967296  cfg/native_amp_growth_interval : 1000  cfg/hysteresis : 2  cfg/fp32_residual_connection : False  cfg/fp16_lm_cross_entropy : False  cfg/megatron_amp_O2 : True  cfg/grad_allreduce_chunk_size_mb : 125  cfg/grad_div_ar_fusion : True  cfg/gradient_accumulation_fusion : True  cfg/cross_entropy_loss_fusion : True  cfg/bias_activation_fusion : True  cfg/bias_dropout_add_fusion : True  cfg/masked_softmax_fusion : True  cfg/seed : 1234  cfg/resume_from_checkpoint : None  cfg/use_cpu_initialization : False  cfg/onnx_safe : False  cfg/apex_transformer_log_level : 30  cfg/gradient_as_bucket_view : True  cfg/sync_batch_comm : False  cfg/activations_checkpoint_granularity : None  cfg/activations_checkpoint_method : None  cfg/activations_checkpoint_num_layers : None  cfg/num_micro_batches_with_partial_activation_checkpoints : None  cfg/activations_checkpoint_layers_per_pipeline : None  cfg/sequence_parallel : False  cfg/defer_embedding_wgrad_compute : False  cfg/wgrad_deferral_limit : 50  cfg/deterministic_mode : False  cfg/transformer_engine : True  cfg/fp8 : False  cfg/fp8_e4m3 : False  cfg/fp8_hybrid : True  cfg/fp8_margin : 0  cfg/fp8_interval : 1  cfg/fp8_amax_history_len : 1024  cfg/fp8_amax_compute_algo : max  cfg/ub_tp_comm_overlap : True  cfg/ub_tp_comm_overlap_cfg/fc1_dgrad/cga_size : 2  cfg/ub_tp_comm_overlap_cfg/fc1_dgrad/method : bulk  cfg/ub_tp_comm_overlap_cfg/fc1_dgrad/num_sm : 2  cfg/ub_tp_comm_overlap_cfg/fc1_dgrad/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/fc1_fprop/aggregate : 0  cfg/ub_tp_comm_overlap_cfg/fc1_fprop/method : ring_exchange  cfg/ub_tp_comm_overlap_cfg/fc1_fprop/num_sm : 1  cfg/ub_tp_comm_overlap_cfg/fc1_fprop/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/fc1_wgrad/cga_size : 2  cfg/ub_tp_comm_overlap_cfg/fc1_wgrad/method : bulk  cfg/ub_tp_comm_overlap_cfg/fc1_wgrad/num_sm : 8  cfg/ub_tp_comm_overlap_cfg/fc1_wgrad/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/fc2_dgrad/aggregate : 0  cfg/ub_tp_comm_overlap_cfg/fc2_dgrad/method : ring_exchange  cfg/ub_tp_comm_overlap_cfg/fc2_dgrad/num_sm : 1  cfg/ub_tp_comm_overlap_cfg/fc2_dgrad/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/fc2_fprop/aggregate : 1  cfg/ub_tp_comm_overlap_cfg/fc2_fprop/method : ring_exchange  cfg/ub_tp_comm_overlap_cfg/fc2_fprop/num_sm : 1  cfg/ub_tp_comm_overlap_cfg/fc2_fprop/set_sm_margin : 1  cfg/ub_tp_comm_overlap_cfg/proj_dgrad/aggregate : 0  cfg/ub_tp_comm_overlap_cfg/proj_dgrad/method : ring_exchange  cfg/ub_tp_comm_overlap_cfg/proj_dgrad/num_sm : 1  cfg/ub_tp_comm_overlap_cfg/proj_dgrad/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/proj_fprop/aggregate : 1  cfg/ub_tp_comm_overlap_cfg/proj_fprop/method : ring_exchange  cfg/ub_tp_comm_overlap_cfg/proj_fprop/num_sm : 1  cfg/ub_tp_comm_overlap_cfg/proj_fprop/set_sm_margin : 1  cfg/ub_tp_comm_overlap_cfg/qkv_dgrad/cga_size : 2  cfg/ub_tp_comm_overlap_cfg/qkv_dgrad/method : bulk  cfg/ub_tp_comm_overlap_cfg/qkv_dgrad/num_sm : 8  cfg/ub_tp_comm_overlap_cfg/qkv_dgrad/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/qkv_fprop/aggregate : 0  cfg/ub_tp_comm_overlap_cfg/qkv_fprop/method : ring_exchange  cfg/ub_tp_comm_overlap_cfg/qkv_fprop/num_sm : 1  cfg/ub_tp_comm_overlap_cfg/qkv_fprop/set_sm_margin : 0  cfg/ub_tp_comm_overlap_cfg/qkv_wgrad/cga_size : 2  cfg/ub_tp_comm_overlap_cfg/qkv_wgrad/method : bulk  cfg/ub_tp_comm_overlap_cfg/qkv_wgrad/num_sm : 32  cfg/ub_tp_comm_overlap_cfg/qkv_wgrad/set_sm_margin : 0  cfg/use_flash_attention : True  cfg/overlap_p2p_comm : True  cfg/batch_p2p_comm : False  cfg/gc_interval : 100  cfg/nsys_profile/enabled : False  cfg/nsys_profile/trace : ['nvtx', 'cuda']  cfg/nsys_profile/start_step : 25  cfg/nsys_profile/end_step : 30  cfg/nsys_profile/ranks : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575]  cfg/nsys_profile/gen_shape : False  cfg/optim/name : distributed_fused_adam  cfg/optim/lr : 0.00015  cfg/optim/weight_decay : 0.1  cfg/optim/betas : [0.9, 0.95]  cfg/optim/bucket_cap_mb : 125  cfg/optim/overlap_grad_sync : True  cfg/optim/overlap_param_sync : True  cfg/optim/contiguous_grad_buffer : True  cfg/optim/contiguous_param_buffer : True  cfg/optim/grad_sync_dtype : bf16  cfg/optim/sched/name : CosineAnnealing  cfg/optim/sched/warmup_steps : 2000  cfg/optim/sched/constant_steps : 11873  cfg/optim/sched/min_lr : 1e-05  cfg/fp8_params : True  cfg/data/data_impl : mock  cfg/data/splits_string : 99990,8,2  cfg/data/seq_length : 4096  cfg/data/skip_warmup : True  cfg/data/num_workers : 2  cfg/data/dataloader_type : single  cfg/data/reset_position_ids : False  cfg/data/reset_attention_mask : False  cfg/data/eod_mask_loss : False  cfg/data/index_mapping_dir : /gcs/index_mapping_dir  cfg/data/data_prefix : [1.0, '/ssd/.cache/wikipedia-tokenized-for-gpt2']  cfg/precision : bf16-mixed 
[NeMo W 2025-06-12 22:46:48 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
    
[NeMo W 2025-06-12 22:46:48 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
    
[NeMo W 2025-06-12 22:46:48 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/google/protobuf/internal/well_known_types.py:93: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).
      _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0)
    
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2897) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2902) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2908) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2912) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2910) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2931) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2942) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2929) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
[NeMo W 2025-06-12 22:47:12 nemo_logging:405] /opt/NeMo/nemo/lightning/pytorch/strategies/utils.py:31: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/jieba/_compat.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
      import pkg_resources
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pyannote')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:47:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:47:16 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:14: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
      import audioop
    
[NeMo W 2025-06-12 22:47:16 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo W 2025-06-12 22:47:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
      cm = get_cmap("Set1")
    
[NeMo W 2025-06-12 22:47:21 nemo_logging:405] <frozen importlib._bootstrap_external>:1297: DeprecationWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.
    
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=3997) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4060) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
[NeMo W 2025-06-12 22:47:32 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
    
[NeMo W 2025-06-12 22:47:32 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.
    
[NeMo W 2025-06-12 22:47:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/google/protobuf/internal/well_known_types.py:93: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).
      _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0)
    
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4221) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4340) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4346) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4369) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4390) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4398) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
[NeMo W 2025-06-12 22:47:51 nemo_logging:405] /opt/NeMo/nemo/lightning/pytorch/strategies/utils.py:31: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
    
[NeMo W 2025-06-12 22:47:51 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
    
[NeMo W 2025-06-12 22:47:51 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
    
[NeMo W 2025-06-12 22:47:51 nemo_logging:405] <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
    
[NeMo W 2025-06-12 22:47:51 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/jieba/_compat.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
      import pkg_resources
    
[NeMo W 2025-06-12 22:47:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pyannote')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:47:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:47:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.
    Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
      declare_namespace(pkg)
    
[NeMo W 2025-06-12 22:47:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:14: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
      import audioop
    
[NeMo W 2025-06-12 22:47:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo W 2025-06-12 22:47:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
      cm = get_cmap("Set1")
    
[NeMo W 2025-06-12 22:47:58 nemo_logging:405] <frozen importlib._bootstrap_external>:1297: DeprecationWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.
    

Training: |          | 0/? [00:00<?, ?it/s]
Training:   0%|          | 0/20 [00:00<?]  
Epoch 0: :   0%|          | 0/20 [00:00<?]
Epoch 0: :   5%|▌         | 1/20 [00:25<08:04]
Epoch 0: :   5%|▌         | 1/20 [00:25<08:04, v_num=None, reduced_train_loss=13.00, global_step=0.000, consumed_samples=512.0, train_step_timing in s=25.50]DLL 2025-06-12 22:48:27.779638 - 0 reduced_train_loss : 12.953968048095703  lr : 0.0  global_step : 0.0  consumed_samples : 512.0  train_backward_timing in s : 8.869171142578125e-05  grad_norm : 14.948970794677734  train_step_timing in s : 25.4822199344635  epoch : 0 

Epoch 0: :  10%|█         | 2/20 [00:31<04:46, v_num=None, reduced_train_loss=13.00, global_step=0.000, consumed_samples=512.0, train_step_timing in s=25.50]
Epoch 0: :  10%|█         | 2/20 [00:31<04:46, v_num=None, reduced_train_loss=13.00, global_step=1.000, consumed_samples=1024.0, train_step_timing in s=15.90]DLL 2025-06-12 22:48:34.177388 - 1 reduced_train_loss : 12.955612182617188  lr : 7.500000265281415e-08  global_step : 1.0  consumed_samples : 1024.0  train_backward_timing in s : 0.00010132789611816406  grad_norm : 15.177976608276367  train_step_timing in s : 15.937272071838379  epoch : 0 

Epoch 0: :  15%|█▌        | 3/20 [00:38<03:37, v_num=None, reduced_train_loss=13.00, global_step=1.000, consumed_samples=1024.0, train_step_timing in s=15.90]
Epoch 0: :  15%|█▌        | 3/20 [00:38<03:37, v_num=None, reduced_train_loss=13.00, global_step=2.000, consumed_samples=1536.0, train_step_timing in s=12.80]DLL 2025-06-12 22:48:40.605959 - 2 reduced_train_loss : 12.955720901489258  lr : 1.500000053056283e-07  global_step : 2.0  consumed_samples : 1536.0  train_backward_timing in s : 0.00010347366333007812  grad_norm : 15.40392780303955  train_step_timing in s : 12.766103506088257  epoch : 0 

Epoch 0: :  20%|██        | 4/20 [00:44<02:58, v_num=None, reduced_train_loss=13.00, global_step=2.000, consumed_samples=1536.0, train_step_timing in s=12.80]
Epoch 0: :  20%|██        | 4/20 [00:44<02:58, v_num=None, reduced_train_loss=13.00, global_step=3.000, consumed_samples=2048.0, train_step_timing in s=11.20]DLL 2025-06-12 22:48:47.032751 - 3 reduced_train_loss : 12.953078269958496  lr : 2.2499999374758772e-07  global_step : 3.0  consumed_samples : 2048.0  train_backward_timing in s : 0.00010198354721069336  grad_norm : 14.39413833618164  train_step_timing in s : 11.179940044879913  epoch : 0 

Epoch 0: :  25%|██▌       | 5/20 [00:51<02:33, v_num=None, reduced_train_loss=13.00, global_step=3.000, consumed_samples=2048.0, train_step_timing in s=11.20]
Epoch 0: :  25%|██▌       | 5/20 [00:51<02:33, v_num=None, reduced_train_loss=12.90, global_step=4.000, consumed_samples=2560.0, train_step_timing in s=10.20]DLL 2025-06-12 22:48:53.457673 - 4 reduced_train_loss : 12.945974349975586  lr : 3.000000106112566e-07  global_step : 4.0  consumed_samples : 2560.0  train_backward_timing in s : 0.00010619163513183593  grad_norm : 14.726126670837402  train_step_timing in s : 10.22776255607605  epoch : 0 

Epoch 0: :  30%|███       | 6/20 [00:57<02:14, v_num=None, reduced_train_loss=12.90, global_step=4.000, consumed_samples=2560.0, train_step_timing in s=10.20]
Epoch 0: :  30%|███       | 6/20 [00:57<02:14, v_num=None, reduced_train_loss=12.90, global_step=5.000, consumed_samples=3072.0, train_step_timing in s=6.420]DLL 2025-06-12 22:48:59.893110 - 5 reduced_train_loss : 12.916672706604004  lr : 3.7500001326407073e-07  global_step : 5.0  consumed_samples : 3072.0  train_backward_timing in s : 0.00010957717895507813  grad_norm : 14.466233253479004  train_step_timing in s : 6.417265558242798  epoch : 0 

Epoch 0: :  35%|███▌      | 7/20 [01:04<01:58, v_num=None, reduced_train_loss=12.90, global_step=5.000, consumed_samples=3072.0, train_step_timing in s=6.420]
Epoch 0: :  35%|███▌      | 7/20 [01:04<01:58, v_num=None, reduced_train_loss=12.80, global_step=6.000, consumed_samples=3584.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:06.340798 - 6 reduced_train_loss : 12.83534049987793  lr : 4.4999998749517545e-07  global_step : 6.0  consumed_samples : 3584.0  train_backward_timing in s : 0.00010843276977539063  grad_norm : 14.945436477661133  train_step_timing in s : 6.427131319046021  epoch : 0 

Epoch 0: :  40%|████      | 8/20 [01:10<01:45, v_num=None, reduced_train_loss=12.80, global_step=6.000, consumed_samples=3584.0, train_step_timing in s=6.430]
Epoch 0: :  40%|████      | 8/20 [01:10<01:45, v_num=None, reduced_train_loss=12.80, global_step=7.000, consumed_samples=4096.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:12.776892 - 7 reduced_train_loss : 12.795648574829102  lr : 5.25000018569699e-07  global_step : 7.0  consumed_samples : 4096.0  train_backward_timing in s : 0.00010585784912109375  grad_norm : 14.91381549835205  train_step_timing in s : 6.4286011219024655  epoch : 0 

Epoch 0: :  45%|████▌     | 9/20 [01:16<01:33, v_num=None, reduced_train_loss=12.80, global_step=7.000, consumed_samples=4096.0, train_step_timing in s=6.430]
Epoch 0: :  45%|████▌     | 9/20 [01:16<01:33, v_num=None, reduced_train_loss=12.60, global_step=8.000, consumed_samples=4608.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:19.195937 - 8 reduced_train_loss : 12.570353507995605  lr : 6.000000212225132e-07  global_step : 8.0  consumed_samples : 4608.0  train_backward_timing in s : 0.00010280609130859375  grad_norm : 14.45373249053955  train_step_timing in s : 6.427197408676148  epoch : 0 

Epoch 0: :  50%|█████     | 10/20 [01:23<01:23, v_num=None, reduced_train_loss=12.60, global_step=8.000, consumed_samples=4608.0, train_step_timing in s=6.430]
Epoch 0: :  50%|█████     | 10/20 [01:23<01:23, v_num=None, reduced_train_loss=12.40, global_step=9.000, consumed_samples=5120.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:25.637113 - 9 reduced_train_loss : 12.436516761779785  lr : 6.750000238753273e-07  global_step : 9.0  consumed_samples : 5120.0  train_backward_timing in s : 9.431838989257813e-05  grad_norm : 14.699613571166992  train_step_timing in s : 6.430376148223877  epoch : 0 

Epoch 0: :  55%|█████▌    | 11/20 [01:29<01:13, v_num=None, reduced_train_loss=12.40, global_step=9.000, consumed_samples=5120.0, train_step_timing in s=6.430]
Epoch 0: :  55%|█████▌    | 11/20 [01:29<01:13, v_num=None, reduced_train_loss=12.30, global_step=10.00, consumed_samples=5632.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:32.065742 - 10 reduced_train_loss : 12.3068208694458  lr : 7.500000265281415e-07  global_step : 10.0  consumed_samples : 5632.0  train_backward_timing in s : 8.955001831054688e-05  grad_norm : 15.133544921875  train_step_timing in s : 6.428895378112793  epoch : 0 

Epoch 0: :  60%|██████    | 12/20 [01:36<01:04, v_num=None, reduced_train_loss=12.30, global_step=10.00, consumed_samples=5632.0, train_step_timing in s=6.430]
Epoch 0: :  60%|██████    | 12/20 [01:36<01:04, v_num=None, reduced_train_loss=11.80, global_step=11.00, consumed_samples=6144.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:38.507803 - 11 reduced_train_loss : 11.77142333984375  lr : 8.249999723375367e-07  global_step : 11.0  consumed_samples : 6144.0  train_backward_timing in s : 9.255409240722656e-05  grad_norm : 13.858604431152344  train_step_timing in s : 6.427949047088623  epoch : 0 

Epoch 0: :  65%|██████▌   | 13/20 [01:42<00:55, v_num=None, reduced_train_loss=11.80, global_step=11.00, consumed_samples=6144.0, train_step_timing in s=6.430]
Epoch 0: :  65%|██████▌   | 13/20 [01:42<00:55, v_num=None, reduced_train_loss=11.30, global_step=12.00, consumed_samples=6656.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:44.978301 - 12 reduced_train_loss : 11.296424865722656  lr : 8.999999749903509e-07  global_step : 12.0  consumed_samples : 6656.0  train_backward_timing in s : 9.222030639648438e-05  grad_norm : 14.213419914245605  train_step_timing in s : 6.434770345687866  epoch : 0 

Epoch 0: :  70%|███████   | 14/20 [01:49<00:46, v_num=None, reduced_train_loss=11.30, global_step=12.00, consumed_samples=6656.0, train_step_timing in s=6.430]
Epoch 0: :  70%|███████   | 14/20 [01:49<00:46, v_num=None, reduced_train_loss=11.10, global_step=13.00, consumed_samples=7168.0, train_step_timing in s=6.440]DLL 2025-06-12 22:49:51.402748 - 13 reduced_train_loss : 11.084091186523438  lr : 9.75000034486584e-07  global_step : 13.0  consumed_samples : 7168.0  train_backward_timing in s : 9.479522705078126e-05  grad_norm : 14.125746726989746  train_step_timing in s : 6.43574628829956  epoch : 0 

Epoch 0: :  75%|███████▌  | 15/20 [01:55<00:38, v_num=None, reduced_train_loss=11.10, global_step=13.00, consumed_samples=7168.0, train_step_timing in s=6.440]
Epoch 0: :  75%|███████▌  | 15/20 [01:55<00:38, v_num=None, reduced_train_loss=10.90, global_step=14.00, consumed_samples=7680.0, train_step_timing in s=6.430]DLL 2025-06-12 22:49:57.824736 - 14 reduced_train_loss : 10.919382095336914  lr : 1.050000037139398e-06  global_step : 14.0  consumed_samples : 7680.0  train_backward_timing in s : 0.00011391639709472657  grad_norm : 13.319463729858398  train_step_timing in s : 6.43210883140564  epoch : 0 

Epoch 0: :  80%|████████  | 16/20 [02:01<00:30, v_num=None, reduced_train_loss=10.90, global_step=14.00, consumed_samples=7680.0, train_step_timing in s=6.430]
Epoch 0: :  80%|████████  | 16/20 [02:01<00:30, v_num=None, reduced_train_loss=10.30, global_step=15.00, consumed_samples=8192.0, train_step_timing in s=6.430]DLL 2025-06-12 22:50:04.249328 - 15 reduced_train_loss : 10.295988082885742  lr : 1.1250000397922122e-06  global_step : 15.0  consumed_samples : 8192.0  train_backward_timing in s : 0.00011920928955078125  grad_norm : 12.99934196472168  train_step_timing in s : 6.431418991088867  epoch : 0 

Epoch 0: :  85%|████████▌ | 17/20 [02:08<00:22, v_num=None, reduced_train_loss=10.30, global_step=15.00, consumed_samples=8192.0, train_step_timing in s=6.430]
Epoch 0: :  85%|████████▌ | 17/20 [02:08<00:22, v_num=None, reduced_train_loss=9.530, global_step=16.00, consumed_samples=8704.0, train_step_timing in s=6.420]DLL 2025-06-12 22:50:10.652514 - 16 reduced_train_loss : 9.52897834777832  lr : 1.2000000424450263e-06  global_step : 16.0  consumed_samples : 8704.0  train_backward_timing in s : 0.00011734962463378906  grad_norm : 11.559502601623535  train_step_timing in s : 6.423512649536133  epoch : 0 

Epoch 0: :  90%|█████████ | 18/20 [02:14<00:14, v_num=None, reduced_train_loss=9.530, global_step=16.00, consumed_samples=8704.0, train_step_timing in s=6.420]
Epoch 0: :  90%|█████████ | 18/20 [02:14<00:14, v_num=None, reduced_train_loss=9.270, global_step=17.00, consumed_samples=9216.0, train_step_timing in s=6.420]DLL 2025-06-12 22:50:17.108084 - 17 reduced_train_loss : 9.265058517456055  lr : 1.2750000450978405e-06  global_step : 17.0  consumed_samples : 9216.0  train_backward_timing in s : 0.00012302398681640625  grad_norm : 11.013927459716797  train_step_timing in s : 6.420359182357788  epoch : 0 

Epoch 0: :  95%|█████████▌| 19/20 [02:21<00:07, v_num=None, reduced_train_loss=9.270, global_step=17.00, consumed_samples=9216.0, train_step_timing in s=6.420]
Epoch 0: :  95%|█████████▌| 19/20 [02:21<00:07, v_num=None, reduced_train_loss=9.050, global_step=18.00, consumed_samples=9728.0, train_step_timing in s=6.420]DLL 2025-06-12 22:50:23.529930 - 18 reduced_train_loss : 9.052398681640625  lr : 1.3500000477506546e-06  global_step : 18.0  consumed_samples : 9728.0  train_backward_timing in s : 0.00012764930725097656  grad_norm : 10.560256004333496  train_step_timing in s : 6.419741010665893  epoch : 0 

Epoch 0: : 100%|██████████| 20/20 [02:27<00:00, v_num=None, reduced_train_loss=9.050, global_step=18.00, consumed_samples=9728.0, train_step_timing in s=6.420]
Epoch 0: : 100%|██████████| 20/20 [02:27<00:00, v_num=None, reduced_train_loss=8.540, global_step=19.00, consumed_samples=10240.0, train_step_timing in s=6.430]DLL 2025-06-12 22:50:29.981785 - 19 reduced_train_loss : 8.535394668579102  lr : 1.4250000504034688e-06  global_step : 19.0  consumed_samples : 10240.0  train_backward_timing in s : 0.0001110076904296875  grad_norm : 10.102982521057129  train_step_timing in s : 6.425825500488282  epoch : 0 
`Trainer.fit` stopped: `max_steps=20` reached.

Epoch 0: : 100%|██████████| 20/20 [02:27<00:00, v_num=None, reduced_train_loss=8.540, global_step=19.00, consumed_samples=10240.0, train_step_timing in s=6.430]
Epoch 0: : 100%|██████████| 20/20 [02:27<00:00, v_num=None, reduced_train_loss=8.540, global_step=19.00, consumed_samples=10240.0, train_step_timing in s=6.430]
[NeMo I 2025-06-12 22:50:30 nemo_logging:393] train_step_timing in s: [25.48, 15.94, 12.77, 11.18, 10.23, 6.42, 6.43, 6.43, 6.43, 6.43, 6.43, 6.43, 6.43, 6.44, 6.43, 6.43, 6.42, 6.42, 6.42, 6.43]
[NeMo I 2025-06-12 22:50:30 nemo_logging:393] FLOPs measurement supported for ['gpt3', 'llama2', 'llama3', 'nemotron', 'mixtral', 'bert']
[NeMo E 2025-06-12 22:50:30 nemo_logging:417] Failed to calculate TFLOPs per sec per GPU.
    'Failed to extract valid model name from or missing FLOPs calculations for qwen25_32b_64gpu_tp1_pp2_cp1_vp32_gbs512_mbs1'
[NeMo I 2025-06-12 22:50:30 nemo_logging:393] TFLOPs per sec per GPU=-1.00
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmplo6v0s0q'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpwfbv51fy'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpvpdf9gjy'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmplkjvyzk6'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmprh3wcbc3'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp4a93510i'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpchp9se3o'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp58bxdfxd'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpfo5adw2h'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp65jmwiia'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp5ryvxjkg'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpsi1snbk0'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpjbdoith7'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp933ai5op'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp0qb5xqeg'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpjyupz_fo'>
  _warnings.warn(warn_message, ResourceWarning)
[NeMo W 2025-06-12 22:50:31 nemo_logging:405] /usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpfypdbczj'>
      _warnings.warn(warn_message, ResourceWarning)
    
[NeMo W 2025-06-12 22:50:31 nemo_logging:405] /usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmplv66p967'>
      _warnings.warn(warn_message, ResourceWarning)
    
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpkplk83u9'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpmhunp8me'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp5mlvhvu1'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpaez3lwgp'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpmx9_sipu'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpn_fnbxkj'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpbpps7qpz'>
  _warnings.warn(warn_message, ResourceWarning)
/usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmptvirhjb1'>
  _warnings.warn(warn_message, ResourceWarning)
[NeMo W 2025-06-12 22:50:39 nemo_logging:405] /usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp1ur77ge3'>
      _warnings.warn(warn_message, ResourceWarning)
    
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name='/dev/null'>
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CPUDispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type CUDADispatcher is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
gc:0: ResourceWarning: Object of type google._upb._message.DescriptorPool is not untracked before destruction
NCCL version 2.25.1+cuda12.8
NCCL version 2.25.1+cuda12.8
NCCL version 2.25.1+cuda12.8
NCCL version 2.25.1+cuda12.8
Processing events...
Generated:
	No reports were generated
NCCL version 2.25.1+cuda12.8
NCCL version 2.25.1+cuda12.8
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
NCCL version 2.25.1+cuda12.8
NCCL version 2.25.1+cuda12.8
Processing events...
Generated:
	No reports were generated
Waiting on Torch PID 644
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Waiting on Torch PID 645
Waiting on Torch PID 646
Waiting on Torch PID 647
Waiting on Torch PID 648
Processing events...
Generated:
	No reports were generated
Waiting on Torch PID 649
Waiting on Torch PID 650
Pod on gke-gke-a4-sbrg1-a4-highgpu-8g-a4-hig-03548386-jrkr.us-east4-b.c.supercomputer-testing.internal is exiting
