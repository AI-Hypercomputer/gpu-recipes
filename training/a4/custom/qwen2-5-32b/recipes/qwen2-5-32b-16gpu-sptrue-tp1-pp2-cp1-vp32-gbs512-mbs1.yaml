run:
  name: qwen25_32b_16gpu_tp1_pp2_cp1_vp32_gbs512_mbs1
  results_dir: ${base_results_dir}/${.name}
  time_limit: "0-02:30:00"
  dependency: "singleton"
trainer:
  devices: 8
  accelerator: gpu
  precision: bf16
  logger: false # logger provided by exp_manager
  enable_checkpointing: false
  use_distributed_sampler: false
  max_epochs: null
  max_steps: 20 # consumed_samples = global_step * global_batch_size
  max_time: "05:23:30:00" # days:hours:minutes:seconds
  log_every_n_steps: 1
  val_check_interval: null
  limit_val_batches: 0.0
  limit_test_batches: 5
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
exp_manager:
  exp_dir: null
  name: megatron_llama
  create_wandb_logger: false
  wandb_logger_kwargs:
    project: nemo_llama_pretrain
    name: ${run.name}
  create_dllogger_logger: True
  dllogger_logger_kwargs:
    verbose: True
    stdout: True
    json_file: "./dllogger.json"     
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  checkpoint_callback_params:
    monitor: val_loss
    save_top_k: 10
    mode: min
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    save_nemo_on_train_end: False # not recommended when training large models on clusters with short time limits
    filename: 'megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
  log_step_timing: true
  step_timing_kwargs:
    sync_cuda: true
    buffer_size: 5
  seconds_to_sleep: 60
model:
  override_vocab_size: 152064
  tp_only_amax_red: true
  mcore_gpt: true
  micro_batch_size: 1
  global_batch_size: 512
  rampup_batch_size: null
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 2
  # virtual_pipeline_model_parallel_size: 18
  # virtual_pipeline_model_parallel_size: 14
  # virtual_pipeline_model_parallel_size: 7
  virtual_pipeline_model_parallel_size: 32
  context_parallel_size: 1
  # context_parallel_size: 2
  encoder_seq_length: 4096
  max_position_embeddings: 4096
  num_layers: 64
  hidden_size: 5120
  ffn_hidden_size: 27648
  num_attention_heads: 40
  num_query_groups: 8
  init_method_std: 0.02
  use_scaled_init_method: true
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  kv_channels: null
  apply_query_key_layer_scaling: true
  normalization: rmsnorm
  layernorm_epsilon: 1.0e-05
  do_layer_norm_weight_decay: false
  make_vocab_size_divisible_by: 128
  pre_process: true
  post_process: true
  persist_layer_norm: true
  bias: false
  activation: fast-swiglu
  headscale: false
  transformer_block_type: pre_ln
  openai_gelu: false
  normalize_attention_scores: true
  position_embedding_type: rope
  rotary_percentage: 1.0
  apply_rope_fusion: true
  attention_type: multihead
  share_embeddings_and_output_weights: false
  scale_positional_embedding: true
  tokenizer:
    library: 'megatron'
    type: 'GPT2BPETokenizer'
    model: null
    delimiter: null  # only used for tabular tokenizer
    vocab_file: gpt2-vocab.json
    merge_file: gpt2-merges.txt     
#  tokenizer:
#    library: huggingface
#    type: meta-llama/Meta-Llama-3.1-8B
#    use_fast: true
  native_amp_init_scale: 4294967296
  native_amp_growth_interval: 1000
  hysteresis: 2
  fp32_residual_connection: false
  fp16_lm_cross_entropy: false
  megatron_amp_O2: true
  grad_allreduce_chunk_size_mb: 125
  grad_div_ar_fusion: true
  gradient_accumulation_fusion: true
  cross_entropy_loss_fusion: true
  bias_activation_fusion: true
  bias_dropout_add_fusion: true
  masked_softmax_fusion: true
  seed: 1234
  resume_from_checkpoint: null
  use_cpu_initialization: false
  onnx_safe: false
  apex_transformer_log_level: 30
  gradient_as_bucket_view: true
  sync_batch_comm: false
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  num_micro_batches_with_partial_activation_checkpoints: null
  activations_checkpoint_layers_per_pipeline: null
  sequence_parallel: false
  defer_embedding_wgrad_compute: false
  wgrad_deferral_limit: 50
  deterministic_mode: false

  ## Transformer Engine
  transformer_engine: true
  fp8: false # enables fp8 in TransformerLayer forward
  # enable_cuda_graph: true
  # use_te_rng_tracker: true    
  # fp8: false # enables fp8 in TransformerLayer forward
  fp8_e4m3: False # sets fp8_format = recipe.Format.E4M3
  fp8_hybrid: true # sets fp8_format = recipe.Format.HYBRID
  fp8_margin: 0 # scaling margin
  fp8_interval: 1 # scaling update interval
  fp8_amax_history_len: 1024 # Number of steps for which amax history is recorded per tensor
  fp8_amax_compute_algo: max # 'most_recent' or 'max'. Algorithm for computing amax from history
  ub_tp_comm_overlap: true
  ub_tp_comm_overlap_cfg:
#    # UB communicator configurations
#    # Model configs: H100/405B/TP8/CP2/MBS1/SeqLen8K/FP8
#    
#    # Bulk overlap with AllGather / ReduceScatter
#    qkv_dgrad:
#      method: bulk
#      num_sm: 2
#      cga_size: 2
#      set_sm_margin: 0
#    
#    qkv_wgrad:
#      method: bulk
#      num_sm: 24
#      cga_size: 2
#      set_sm_margin: 0
#    
#    fc1_dgrad:
#      method: bulk
#      num_sm: 2
#      cga_size: 2
#      set_sm_margin: 0
#    
#    fc1_wgrad:
#      method: bulk
#      num_sm: 2
#      cga_size: 2
#      set_sm_margin: 0
#    
#    ## Ring
#    qkv_fprop:
#      method: ring_exchange
#      aggregate: 1
#    
#    proj_dgrad:
#      method: ring_exchange
#      aggregate: 1
#    
#    fc1_fprop:
#      method: ring_exchange
#      aggregate: 1
#    
#    fc2_dgrad:
#      method: ring_exchange
#      aggregate: 0
#    
#    # Chunked
#    proj_fprop:
#      method: pipeline
#      num_sm: 24
#      cga_size: 2
#      num_splits: 4
#      set_sm_margin: 1
#      fp8_buf: 1
#      atomic_gemm: 0
#    
#    fc2_fprop:
#      method: pipeline
#      num_sm: 16
#      cga_size: 2
#      num_splits: 4
#      set_sm_margin: 1
#      fp8_buf: 1
#      atomic_gemm: 0
    fc1_dgrad:
      cga_size: 2
      method: bulk
      num_sm: 2
      set_sm_margin: 0
    fc1_fprop:
      aggregate: 0
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
    fc1_wgrad:
      cga_size: 2
      method: bulk
      num_sm: 8
      set_sm_margin: 0
    fc2_dgrad:
      aggregate: 0
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
    fc2_fprop:
      aggregate: 1
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 1
    proj_dgrad:
      aggregate: 0
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
    proj_fprop:
      aggregate: 1
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 1
    qkv_dgrad:
      cga_size: 2
      method: bulk
      num_sm: 8
      set_sm_margin: 0
    qkv_fprop:
      aggregate: 0
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
    qkv_wgrad:
      cga_size: 2
      method: bulk
      num_sm: 32
      set_sm_margin: 0

  use_flash_attention: true
  overlap_p2p_comm: true
  batch_p2p_comm: false
  gc_interval: 100
  nsys_profile:
    enabled: false
    trace: [nvtx,cuda]
    start_step: 25  # Global batch to start profiling
    end_step: 30 # Global batch to end profiling
    #    ranks: [0, 16, 32, 48, 64, 80, 96, 112]
    ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575] 
    gen_shape: False # Generate model and kernel details including input shapes
  optim:
    name: distributed_fused_adam
    # name: mcore_distributed_optim
    lr: 0.00015
    weight_decay: 0.1
    betas:
    - 0.9
    - 0.95
    bucket_cap_mb: 125
    overlap_grad_sync: true
    overlap_param_sync: true
    contiguous_grad_buffer: true
    contiguous_param_buffer: true
    grad_sync_dtype: bf16
    sched:
      name: CosineAnnealing
      warmup_steps: 2000
      constant_steps: 11873
      min_lr: 1.0e-05
  fp8_params: true
  data:
    # data_impl: mmap
    data_impl: mock
    splits_string: 99990,8,2
    seq_length: 4096
    skip_warmup: true
    num_workers: 2
    dataloader_type: single
    reset_position_ids: false
    reset_attention_mask: false
    eod_mask_loss: false
    index_mapping_dir: null