diff --git a/nemo/lightning/_strategy_lib.py b/nemo/lightning/_strategy_lib.py
index f4f8b80d4..67635c14f 100644
--- a/nemo/lightning/_strategy_lib.py
+++ b/nemo/lightning/_strategy_lib.py
@@ -132,6 +132,7 @@ def init_model_parallel(model: Optional[nn.Module] = None) -> None:
                 expert_model_parallel_size=app_state.expert_model_parallel_size,
                 expert_tensor_parallel_size=app_state.expert_tensor_parallel_size,
                 order="tp-cp-ep-pp-dp" if app_state.use_tp_pp_dp_mapping else "tp-cp-ep-dp-pp",
+                distributed_timeout_minutes=180,
             )
 
             # assert that fake tp and pp rank match after model parallel init
@@ -146,8 +147,8 @@ def init_model_parallel(model: Optional[nn.Module] = None) -> None:
             app_state.pipeline_model_parallel_group = parallel_state.get_pipeline_model_parallel_group()
 
             # create MPI process group for UCX-based communication APIs
-            if app_state.init_mpi_proc_group:
-                torch.distributed.new_group(backend="mpi")
+            #if app_state.init_mpi_proc_group:
+            #    torch.distributed.new_group(backend="mpi")
 
 
 def set_model_parallel_attributes(model, parallelism):
diff --git a/nemo/lightning/megatron_parallel.py b/nemo/lightning/megatron_parallel.py
index 016c09f81..1bccd867f 100644
--- a/nemo/lightning/megatron_parallel.py
+++ b/nemo/lightning/megatron_parallel.py
@@ -699,8 +699,6 @@ class MegatronParallel(nn.ModuleList, Generic[ModelT]):
                         module.config,
                         self.ddp_config,
                         module,
-                        data_parallel_group=parallel_state.get_data_parallel_group(with_context_parallel=True),
-                        expert_data_parallel_group=parallel_state.get_data_modulo_expert_parallel_group(),
                         disable_bucketing=disable_bucketing,
                     )
                 else:
