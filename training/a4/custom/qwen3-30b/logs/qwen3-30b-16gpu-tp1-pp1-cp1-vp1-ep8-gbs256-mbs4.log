Pod on gke-gke-a4-comp-a4-highgpu-8g-a4-high-89f97933-1rkx.us-central1-b.c.supercomputer-testing.internal is running
Pod is assigned job index of 0
Job ID is achiu-qwen3-30b-16gpu-tp1-pp2-cp1-vp1-ep4-gbs256-mbs4-1750414640-hg8j
The following GPUs are visible via nvidia-smi:
GPU 0: NVIDIA B200 (UUID: GPU-993ceac9-59cd-3c1b-2aa9-762f123d778f)
GPU 1: NVIDIA B200 (UUID: GPU-5741bee7-1f80-a294-5f03-ef06b0be821f)
GPU 2: NVIDIA B200 (UUID: GPU-acb1246a-243a-77b9-8a49-b99bf3d13a30)
GPU 3: NVIDIA B200 (UUID: GPU-f0129490-4d6e-006b-71b8-f0f0f30fa85a)
GPU 4: NVIDIA B200 (UUID: GPU-5bbba25c-ee46-641d-099d-0dd46e159664)
GPU 5: NVIDIA B200 (UUID: GPU-7eb480be-2768-6777-7f65-6c6ec7415bc0)
GPU 6: NVIDIA B200 (UUID: GPU-e45f9a1c-dc0a-212f-38fd-a7bc15c26d0c)
GPU 7: NVIDIA B200 (UUID: GPU-046ed58a-6984-dc6c-704c-50ca1fc6759e)
nvidia-smi:
Fri Jun 20 10:17:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA B200                    Off |   00000000:8F:00.0 Off |                    0 |
| N/A   38C    P0            143W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA B200                    Off |   00000000:90:00.0 Off |                    0 |
| N/A   46C    P0            151W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA B200                    Off |   00000000:96:00.0 Off |                    0 |
| N/A   37C    P0            144W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA B200                    Off |   00000000:97:00.0 Off |                    0 |
| N/A   45C    P0            148W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA B200                    Off |   00000000:C4:00.0 Off |                    0 |
| N/A   36C    P0            143W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA B200                    Off |   00000000:C5:00.0 Off |                    0 |
| N/A   45C    P0            146W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA B200                    Off |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            142W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA B200                    Off |   00000000:CC:00.0 Off |                    0 |
| N/A   45C    P0            150W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Local SSD contents (path /ssd):
  containerd
  hello-from-gke-gke-a4-comp-a4-highgpu-8g-a4-high-89f97933-1rkx.txt
  kubelet
  log_pods
  lost+found
  waymo-open-dataset
Contents (mounted at /usr/local/nccl-plugin/):
Warning: Set LD_LIBRARY_PATH=/usr/local/gib/lib64:/usr/local/nvidia/lib64:/usr/local/mpi/lib:/usr/local/lib/python3.12/dist-packages/torch/lib:/usr/local/lib/python3.12/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/tensorrt/lib to override the NCCL library
/sbin/ldconfig.real: /usr/local/nvidia/lib64/libnccl-net.so is not a symbolic link

Added /usr/local/nvidia/lib64/ to ldconfig:
  	libcudart.so.12 (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12
  	libcudart.so (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so
  	libcudadebugger.so.1 (libc6,x86-64) => /usr/local/nvidia/lib64/libcudadebugger.so.1
  	libcuda_wrapper.so.0 (libc6,x86-64) => /opt/hpcx/clusterkit/lib/libcuda_wrapper.so.0
  	libcuda_wrapper.so (libc6,x86-64) => /opt/hpcx/clusterkit/lib/libcuda_wrapper.so
  	libcuda.so.1 (libc6,x86-64) => /usr/local/nvidia/lib64/libcuda.so.1
  	libcuda.so (libc6,x86-64) => /usr/local/nvidia/lib64/libcuda.so
Got request to JIT mount GCS bucket benchmark-artifacts via 'gcsfuse' to /gcs:
{"timestamp":{"seconds":1750414648,"nanos":27082395},"severity":"INFO","message":"Start gcsfuse/2.12.2 (Go version go1.24.0) for app \"\" using mount point: /gcs\n"}
{"timestamp":{"seconds":1750414648,"nanos":27131535},"severity":"INFO","message":"GCSFuse config","config":{"AppName":"","CacheDir":"","Debug":{"ExitOnInvariantViolation":false,"Fuse":false,"Gcs":false,"LogMutex":false},"DisableAutoconfig":true,"EnableAtomicRenameObject":false,"EnableHns":true,"EnableNewReader":false,"FileCache":{"CacheFileForRangeRead":false,"DownloadChunkSizeMb":200,"EnableCrc":false,"EnableODirect":false,"EnableParallelDownloads":false,"ExperimentalParallelDownloadsDefaultOn":true,"MaxParallelDownloads":448,"MaxSizeMb":-1,"ParallelDownloadsPerFile":16,"WriteBufferSize":4194304},"FileSystem":{"DirMode":"755","DisableParallelDirops":false,"FileMode":"644","FuseOptions":[],"Gid":-1,"IgnoreInterrupts":true,"KernelListCacheTtlSecs":0,"PreconditionErrors":true,"RenameDirLimit":0,"TempDir":"","Uid":-1},"Foreground":false,"GcsAuth":{"AnonymousAccess":false,"KeyFile":"","ReuseTokenFromUrl":true,"TokenUrl":""},"GcsConnection":{"BillingProject":"","ClientProtocol":"http2","CustomEndpoint":"","ExperimentalEnableJsonRead":false,"GrpcConnPoolSize":1,"HttpClientTimeout":0,"LimitBytesPerSec":-1,"LimitOpsPerSec":-1,"MaxConnsPerHost":0,"MaxIdleConnsPerHost":100,"SequentialReadSizeMb":200},"GcsRetries":{"ChunkTransferTimeoutSecs":10,"MaxRetryAttempts":0,"MaxRetrySleep":30000000000,"Multiplier":2,"ReadStall":{"Enable":false,"InitialReqTimeout":20000000000,"MaxReqTimeout":1200000000000,"MinReqTimeout":1500000000,"ReqIncreaseRate":15,"ReqTargetPercentile":0.99}},"ImplicitDirs":false,"List":{"EnableEmptyManagedFolders":false},"Logging":{"FilePath":"","Format":"json","LogRotate":{"BackupFileCount":10,"Compress":true,"MaxFileSizeMb":512},"Severity":"INFO"},"MachineType":"","MetadataCache":{"DeprecatedStatCacheCapacity":20460,"DeprecatedStatCacheTtl":60000000000,"DeprecatedTypeCacheTtl":60000000000,"EnableNonexistentTypeCache":false,"ExperimentalMetadataPrefetchOnMount":"disabled","NegativeTtlSecs":5,"StatCacheMaxSizeMb":32,"TtlSecs":60,"TypeCacheMaxSizeMb":4},"Metrics":{"CloudMetricsExportIntervalSecs":0,"PrometheusPort":0,"StackdriverExportInterval":0,"UseNewNames":false},"Monitoring":{"ExperimentalTracingMode":"","ExperimentalTracingSamplingRatio":0},"OnlyDir":"","Write":{"BlockSizeMb":33554432,"CreateEmptyFile":false,"EnableStreamingWrites":false,"GlobalMaxBlocks":9223372036854775807,"MaxBlocksPerFile":1}}}
{"timestamp":{"seconds":1750414648,"nanos":375824402},"severity":"INFO","message":"File system has been successfully mounted."}
Downloading GPT vocabulary files
--2025-06-20 10:17:28--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.12.149, 52.216.205.149, 52.217.226.152, ...
Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.12.149|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1042301 (1018K) [application/json]
Saving to: ‘gpt2-vocab.json’

     0K .......... .......... .......... .......... ..........  4%  868K 1s
    50K .......... .......... .......... .......... ..........  9% 1.69M 1s
   100K .......... .......... .......... .......... .......... 14%  574M 1s
   150K .......... .......... .......... .......... .......... 19% 1.69M 0s
   200K .......... .......... .......... .......... .......... 24%  487M 0s
   250K .......... .......... .......... .......... .......... 29%  454M 0s
   300K .......... .......... .......... .......... .......... 34%  498M 0s
   350K .......... .......... .......... .......... .......... 39% 1.71M 0s
   400K .......... .......... .......... .......... .......... 44%  448M 0s
   450K .......... .......... .......... .......... .......... 49%  474M 0s
   500K .......... .......... .......... .......... .......... 54%  450M 0s
   550K .......... .......... .......... .......... .......... 58%  414M 0s
   600K .......... .......... .......... .......... .......... 63%  522M 0s
   650K .......... .......... .......... .......... .......... 68%  505M 0s
   700K .......... .......... .......... .......... .......... 73%  568M 0s
   750K .......... .......... .......... .......... .......... 78% 1.72M 0s
   800K .......... .......... .......... .......... .......... 83%  261M 0s
   850K .......... .......... .......... .......... .......... 88%  407M 0s
   900K .......... .......... .......... .......... .......... 93%  573M 0s
   950K .......... .......... .......... .......... .......... 98%  584M 0s
  1000K .......... .......                                    100%  605M=0.2s

2025-06-20 10:17:28 (5.72 MB/s) - ‘gpt2-vocab.json’ saved [1042301/1042301]

--2025-06-20 10:17:28--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.98.126, 54.231.193.152, 52.217.133.152, ...
Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.98.126|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 456318 (446K) [text/plain]
Saving to: ‘gpt2-merges.txt’

     0K .......... .......... .......... .......... .......... 11%  859K 0s
    50K .......... .......... .......... .......... .......... 22% 1.67M 0s
   100K .......... .......... .......... .......... .......... 33%  584M 0s
   150K .......... .......... .......... .......... .......... 44% 1.68M 0s
   200K .......... .......... .......... .......... .......... 56%  403M 0s
   250K .......... .......... .......... .......... .......... 67%  384M 0s
   300K .......... .......... .......... .......... .......... 78%  470M 0s
   350K .......... .......... .......... .......... .......... 89% 1.70M 0s
   400K .......... .......... .......... .......... .....     100%  474M=0.1s

2025-06-20 10:17:29 (2.99 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]

NeMo configuration file:
| 
| run:
|   name: qwen3-30b_16gpu_tp1_pp2_cp1_vp1_ep4_gbs256_mbs4
|   results_dir: ${base_results_dir}/${.name}
|   time_limit: "0-02:30:00"
|   dependency: "singleton"
| trainer:
|   devices: 8
|   accelerator: gpu
|   precision: bf16
|   logger: false # logger provided by exp_manager
|   enable_checkpointing: false
|   use_distributed_sampler: false
|   max_epochs: null
|   max_steps: 20 # consumed_samples = global_step * global_batch_size
|   max_time: "05:23:30:00" # days:hours:minutes:seconds
|   log_every_n_steps: 1
|   val_check_interval: null
|   limit_val_batches: 0.0
|   limit_test_batches: 5
|   accumulate_grad_batches: 1
|   gradient_clip_val: 1.0
| exp_manager:
|   exp_dir: null
|   name: megatron_llama
|   create_wandb_logger: false
|   wandb_logger_kwargs:
|     project: nemo_llama_pretrain
|     name: ${run.name}
|   create_dllogger_logger: True
|   dllogger_logger_kwargs:
|     verbose: True
|     stdout: True
|     json_file: "./dllogger.json"     
|   resume_if_exists: false
|   resume_ignore_no_checkpoint: true
|   create_checkpoint_callback: false
|   checkpoint_callback_params:
|     monitor: val_loss
|     save_top_k: 10
|     mode: min
|     always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
|     save_nemo_on_train_end: False # not recommended when training large models on clusters with short time limits
|     filename: 'megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}'
|     model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
|   log_step_timing: true
|   step_timing_kwargs:
|     sync_cuda: true
|     buffer_size: 5
|   seconds_to_sleep: 60
| model:
|   override_vocab_size: 151936
|   tp_only_amax_red: true
|   mcore_gpt: true
|   moe_grouped_gemm: True
|   moe_token_dispatcher_type: alltoall
|   moe_pad_expert_input_to_capacity: True
|   moe_expert_capacity_factor: 1.0
|   moe_aux_loss_coeff: 1e-3
|   moe_router_load_balancing_type: aux_loss
|   moe_router_pre_softmax: False
|   moe_permute_fusion: True
|   micro_batch_size: 4 #4
|   global_batch_size: 256
|   rampup_batch_size: null
|   tensor_model_parallel_size: 1
|   pipeline_model_parallel_size: 1
|   virtual_pipeline_model_parallel_size: null
|   context_parallel_size: 1
|   expert_model_parallel_size: 8
|   encoder_seq_length: 4096
|   max_position_embeddings: 40960
|   num_layers: 48
|   hidden_size: 2048
|   ffn_hidden_size: 6144
|   moe_ffn_hidden_size: 768
|   num_attention_heads: 32
|   num_query_groups: 4
|   init_method_std: 0.02
|   use_scaled_init_method: true
|   hidden_dropout: 0.0
|   attention_dropout: 0.0
|   ffn_dropout: 0.0
|   kv_channels: 128
|   apply_query_key_layer_scaling: true
|   normalization: rmsnorm
|   layernorm_epsilon: 1.0e-05
|   do_layer_norm_weight_decay: false
|   make_vocab_size_divisible_by: 128
|   pre_process: true
|   post_process: true
|   persist_layer_norm: true
|   bias: false
|   activation: fast-swiglu
|   headscale: false
|   transformer_block_type: pre_ln
|   openai_gelu: false
|   normalize_attention_scores: true
|   position_embedding_type: rope
|   rotary_percentage: 1.0
|   apply_rope_fusion: true
|   moe_router_topk: 8
|   num_moe_experts: 128
|   attention_type: multihead
|   share_embeddings_and_output_weights: false
|   scale_positional_embedding: true
|   tokenizer:
|     library: 'megatron'
|     type: 'GPT2BPETokenizer'
|     model: null
|     delimiter: null  # only used for tabular tokenizer
|     vocab_file: gpt2-vocab.json
|     merge_file: gpt2-merges.txt     
|   native_amp_init_scale: 4294967296
|   native_amp_growth_interval: 1000
|   hysteresis: 2
|   fp32_residual_connection: false
|   fp16_lm_cross_entropy: false
|   megatron_amp_O2: true
|   grad_allreduce_chunk_size_mb: 125
|   grad_div_ar_fusion: true
|   gradient_accumulation_fusion: true
|   cross_entropy_loss_fusion: true
|   bias_activation_fusion: true
|   bias_dropout_add_fusion: true
|   masked_softmax_fusion: true
|   get_attention_mask_from_fusion: true
|   seed: 1234
|   resume_from_checkpoint: null
|   use_cpu_initialization: false
|   onnx_safe: false
|   apex_transformer_log_level: 30
|   gradient_as_bucket_view: true
|   sync_batch_comm: false
|   activations_checkpoint_granularity: null
|   activations_checkpoint_method: null
|   activations_checkpoint_num_layers: null
|   num_micro_batches_with_partial_activation_checkpoints: null
|   activations_checkpoint_layers_per_pipeline: null
|   sequence_parallel: false
|   defer_embedding_wgrad_compute: false
|   wgrad_deferral_limit: 50
|   deterministic_mode: false
| 
|   ## Transformer Engine
|   transformer_engine: true
|   fp8: true # enables fp8 in TransformerLayer forward  
|   fp8_e4m3: False # sets fp8_format = recipe.Format.E4M3
|   fp8_hybrid: true # sets fp8_format = recipe.Format.HYBRID
|   fp8_margin: 0 # scaling margin
|   fp8_interval: 1 # scaling update interval
|   fp8_amax_history_len: 1024 # Number of steps for which amax history is recorded per tensor
|   fp8_amax_compute_algo: max # 'most_recent' or 'max'. Algorithm for computing amax from history
|   fp8_params: true
|   ub_tp_comm_overlap: true
|   ub_tp_comm_overlap_cfg: null
| 
|   use_flash_attention: true
|   overlap_p2p_comm: false
|   batch_p2p_comm: false
|   gc_interval: 100
|   nsys_profile:
|     enabled: false
|     trace: [nvtx,cuda]
|     start_step: 25  # Global batch to start profiling
|     end_step: 30 # Global batch to end profiling
|     ranks: [0] 
|     gen_shape: False # Generate model and kernel details including input shapes
|   optim:
|     # name: distributed_fused_adam
|     name: mcore_distributed_optim
|     lr: 0.00015
|     weight_decay: 0.1
|     betas:
|     - 0.9
|     - 0.95
|     bucket_cap_mb: 125
|     overlap_grad_sync: true
|     overlap_param_sync: true
|     contiguous_grad_buffer: true
|     contiguous_param_buffer: true
|     grad_sync_dtype: bf16
|     sched:
|       name: CosineAnnealing
|       warmup_steps: 2000
|       constant_steps: 11873
|       min_lr: 1.0e-05
|   data:
|     # data_impl: mmap
|     data_impl: mock
|     splits_string: 99990,8,2
|     seq_length: 4096
|     skip_warmup: true
|     num_workers: 2
|     dataloader_type: single
|     reset_position_ids: false
|     reset_attention_mask: false
|     eod_mask_loss: false
|     index_mapping_dir: null
Detected the following additional workload arguments:
  +exp_manager.explicit_log_dir=/nemo-experiments/results
  +model.data.index_mapping_dir=/gcs/index_mapping_dir
  +model.data.data_prefix=[1.0,/ssd/.cache/wikipedia-tokenized-for-gpt2]
  +exp_manager.exp_dir=/nemo-experiments/
Checking for presence of nsys:
/usr/local/cuda/bin/nsys
NeMo job artifacts will go to /gcs/nemo-experiments/achiu-qwen3-30b-16gpu-tp1-pp2-cp1-vp1-ep4-gbs256-mbs4-1750414640-hg8j/

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Get:1 https://packages.cloud.google.com/apt gcsfuse-buster InRelease [1231 B]
Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]
Get:3 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]
Get:4 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]
Get:5 http://archive.ubuntu.com/ubuntu noble InRelease [256 kB]
Get:6 https://packages.cloud.google.com/apt gcsfuse-buster/main all Packages [751 B]
Get:7 https://packages.cloud.google.com/apt gcsfuse-buster/main amd64 Packages [40.8 kB]
Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1798 kB]
Get:9 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1749 kB]
Get:10 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Packages [1496 kB]
Get:11 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3917 kB]
Get:12 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages [1108 kB]
Get:13 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Packages [22.1 kB]
Get:14 http://security.ubuntu.com/ubuntu noble-security/main amd64 Packages [1129 kB]
Get:15 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]
Get:16 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]
Get:17 http://archive.ubuntu.com/ubuntu noble/universe amd64 Packages [19.3 MB]
Get:18 http://archive.ubuntu.com/ubuntu noble/restricted amd64 Packages [117 kB]
Get:19 http://archive.ubuntu.com/ubuntu noble/multiverse amd64 Packages [331 kB]
Get:20 http://archive.ubuntu.com/ubuntu noble/main amd64 Packages [1808 kB]
Get:21 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Packages [1554 kB]
Get:22 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1410 kB]
Get:23 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Packages [26.7 kB]
Get:24 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [1451 kB]
Get:25 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Packages [31.8 kB]
Get:26 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Packages [48.0 kB]
Fetched 38.0 MB in 3s (13.6 MB/s)
Reading package lists...
Building dependency tree...
Reading state information...
87 packages can be upgraded. Run 'apt list --upgradable' to see them.
W: https://packages.cloud.google.com/apt/dists/gcsfuse-buster/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.
W: https://packages.cloud.google.com/apt/dists/cloud-sdk/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
Building dependency tree...
Reading state information...
The following additional packages will be installed:
  libbabeltrace1 libc6-dbg libdebuginfod-common libdebuginfod1t64 libdw1t64
  libelf1t64 libipt2 libpython3-dbg libpython3.12-dev libpython3.12-minimal
  libpython3.12-stdlib libpython3.12t64 libpython3.12t64-dbg
  libsource-highlight-common libsource-highlight4t64 python3.12 python3.12-dbg
  python3.12-dev python3.12-minimal python3.12-venv
Suggested packages:
  gdb-doc gdbserver python3.12-doc python3-gdbm-dbg python3-tk-dbg
  binfmt-support
The following NEW packages will be installed:
  gdb libbabeltrace1 libc6-dbg libdebuginfod-common libdebuginfod1t64
  libdw1t64 libelf1t64 libipt2 libpython3-dbg libpython3.12t64-dbg
  libsource-highlight-common libsource-highlight4t64 python3-dbg
  python3.12-dbg
The following packages will be upgraded:
  libpython3.12-dev libpython3.12-minimal libpython3.12-stdlib
  libpython3.12t64 python3.12 python3.12-dev python3.12-minimal
  python3.12-venv
8 upgraded, 14 newly installed, 0 to remove and 79 not upgraded.
Need to get 99.7 MB of archives.
After this operation, 217 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-dev amd64 3.12.3-1ubuntu0.7 [498 kB]
Get:2 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12-dev amd64 3.12.3-1ubuntu0.7 [5680 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12t64 amd64 3.12.3-1ubuntu0.7 [2355 kB]
Get:4 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3.12-venv amd64 3.12.3-1ubuntu0.7 [5680 B]
Get:5 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12 amd64 3.12.3-1ubuntu0.7 [651 kB]
Get:6 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12-stdlib amd64 3.12.3-1ubuntu0.7 [2070 kB]
Get:7 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-minimal amd64 3.12.3-1ubuntu0.7 [2342 kB]
Get:8 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12-minimal amd64 3.12.3-1ubuntu0.7 [836 kB]
Get:9 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libdebuginfod-common all 0.190-1.1ubuntu0.1 [14.6 kB]
Get:10 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libelf1t64 amd64 0.190-1.1ubuntu0.1 [57.8 kB]
Get:11 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libdw1t64 amd64 0.190-1.1ubuntu0.1 [261 kB]
Get:12 http://archive.ubuntu.com/ubuntu noble/main amd64 libbabeltrace1 amd64 1.5.11-3build3 [164 kB]
Get:13 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libdebuginfod1t64 amd64 0.190-1.1ubuntu0.1 [17.1 kB]
Get:14 http://archive.ubuntu.com/ubuntu noble/main amd64 libipt2 amd64 2.0.6-1build1 [45.7 kB]
Get:15 http://archive.ubuntu.com/ubuntu noble/main amd64 libsource-highlight-common all 3.1.9-4.3build1 [64.2 kB]
Get:16 http://archive.ubuntu.com/ubuntu noble/main amd64 libsource-highlight4t64 amd64 3.1.9-4.3build1 [258 kB]
Get:17 http://archive.ubuntu.com/ubuntu noble/main amd64 gdb amd64 15.0.50.20240403-0ubuntu1 [4010 kB]
Get:18 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3.12t64-dbg amd64 3.12.3-1ubuntu0.7 [24.1 MB]
Get:19 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-dbg amd64 3.12.3-1ubuntu0.7 [48.8 MB]
Get:20 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libc6-dbg amd64 2.39-0ubuntu8.4 [7460 kB]
Get:21 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpython3-dbg amd64 3.12.3-0ubuntu2 [10.3 kB]
Get:22 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3-dbg amd64 3.12.3-0ubuntu2 [1060 B]
Preconfiguring packages ...
Fetched 99.7 MB in 3s (32.7 MB/s)
(Reading database ... 
(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%
(Reading database ... 75%
(Reading database ... 80%
(Reading database ... 85%
(Reading database ... 90%
(Reading database ... 95%
(Reading database ... 100%
(Reading database ... 78111 files and directories currently installed.)
Preparing to unpack .../00-python3.12-dev_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12-dev (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../01-libpython3.12-dev_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking libpython3.12-dev:amd64 (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../02-libpython3.12t64_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking libpython3.12t64:amd64 (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../03-python3.12-venv_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12-venv (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../04-python3.12_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12 (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../05-libpython3.12-stdlib_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking libpython3.12-stdlib:amd64 (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../06-python3.12-minimal_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12-minimal (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Preparing to unpack .../07-libpython3.12-minimal_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking libpython3.12-minimal:amd64 (3.12.3-1ubuntu0.7) over (3.12.3-1ubuntu0.5) ...
Selecting previously unselected package libdebuginfod-common.
Preparing to unpack .../08-libdebuginfod-common_0.190-1.1ubuntu0.1_all.deb ...
Unpacking libdebuginfod-common (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libelf1t64:amd64.
Preparing to unpack .../09-libelf1t64_0.190-1.1ubuntu0.1_amd64.deb ...
Unpacking libelf1t64:amd64 (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libdw1t64:amd64.
Preparing to unpack .../10-libdw1t64_0.190-1.1ubuntu0.1_amd64.deb ...
Unpacking libdw1t64:amd64 (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libbabeltrace1:amd64.
Preparing to unpack .../11-libbabeltrace1_1.5.11-3build3_amd64.deb ...
Unpacking libbabeltrace1:amd64 (1.5.11-3build3) ...
Selecting previously unselected package libdebuginfod1t64:amd64.
Preparing to unpack .../12-libdebuginfod1t64_0.190-1.1ubuntu0.1_amd64.deb ...
Unpacking libdebuginfod1t64:amd64 (0.190-1.1ubuntu0.1) ...
Selecting previously unselected package libipt2.
Preparing to unpack .../13-libipt2_2.0.6-1build1_amd64.deb ...
Unpacking libipt2 (2.0.6-1build1) ...
Selecting previously unselected package libsource-highlight-common.
Preparing to unpack .../14-libsource-highlight-common_3.1.9-4.3build1_all.deb ...
Unpacking libsource-highlight-common (3.1.9-4.3build1) ...
Selecting previously unselected package libsource-highlight4t64:amd64.
Preparing to unpack .../15-libsource-highlight4t64_3.1.9-4.3build1_amd64.deb ...
Unpacking libsource-highlight4t64:amd64 (3.1.9-4.3build1) ...
Selecting previously unselected package gdb.
Preparing to unpack .../16-gdb_15.0.50.20240403-0ubuntu1_amd64.deb ...
Unpacking gdb (15.0.50.20240403-0ubuntu1) ...
Selecting previously unselected package libpython3.12t64-dbg:amd64.
Preparing to unpack .../17-libpython3.12t64-dbg_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking libpython3.12t64-dbg:amd64 (3.12.3-1ubuntu0.7) ...
Selecting previously unselected package python3.12-dbg.
Preparing to unpack .../18-python3.12-dbg_3.12.3-1ubuntu0.7_amd64.deb ...
Unpacking python3.12-dbg (3.12.3-1ubuntu0.7) ...
Selecting previously unselected package libc6-dbg:amd64.
Preparing to unpack .../19-libc6-dbg_2.39-0ubuntu8.4_amd64.deb ...
Unpacking libc6-dbg:amd64 (2.39-0ubuntu8.4) ...
Selecting previously unselected package libpython3-dbg:amd64.
Preparing to unpack .../20-libpython3-dbg_3.12.3-0ubuntu2_amd64.deb ...
Unpacking libpython3-dbg:amd64 (3.12.3-0ubuntu2) ...
Selecting previously unselected package python3-dbg.
Preparing to unpack .../21-python3-dbg_3.12.3-0ubuntu2_amd64.deb ...
Unpacking python3-dbg (3.12.3-0ubuntu2) ...
Setting up libdebuginfod-common (0.190-1.1ubuntu0.1) ...
Setting up libpython3.12-minimal:amd64 (3.12.3-1ubuntu0.7) ...
Setting up libsource-highlight-common (3.1.9-4.3build1) ...
Setting up libelf1t64:amd64 (0.190-1.1ubuntu0.1) ...
Setting up libc6-dbg:amd64 (2.39-0ubuntu8.4) ...
Setting up libdw1t64:amd64 (0.190-1.1ubuntu0.1) ...
Setting up libipt2 (2.0.6-1build1) ...
Setting up libbabeltrace1:amd64 (1.5.11-3build3) ...
Setting up python3.12-minimal (3.12.3-1ubuntu0.7) ...
Setting up libdebuginfod1t64:amd64 (0.190-1.1ubuntu0.1) ...
Setting up libpython3.12-stdlib:amd64 (3.12.3-1ubuntu0.7) ...
Setting up libpython3.12t64-dbg:amd64 (3.12.3-1ubuntu0.7) ...
Setting up python3.12 (3.12.3-1ubuntu0.7) ...
Setting up libpython3-dbg:amd64 (3.12.3-0ubuntu2) ...
Setting up python3.12-dbg (3.12.3-1ubuntu0.7) ...
python3.12-dbg: can't get files for byte-compilation
Setting up libsource-highlight4t64:amd64 (3.1.9-4.3build1) ...
Setting up libpython3.12t64:amd64 (3.12.3-1ubuntu0.7) ...
Setting up gdb (15.0.50.20240403-0ubuntu1) ...
Setting up python3-dbg (3.12.3-0ubuntu2) ...
Setting up python3.12-venv (3.12.3-1ubuntu0.7) ...
Setting up libpython3.12-dev:amd64 (3.12.3-1ubuntu0.7) ...
Setting up python3.12-dev (3.12.3-1ubuntu0.7) ...
Processing triggers for libc-bin (2.39-0ubuntu8.4) ...
/sbin/ldconfig.real: /usr/local/nvidia/lib64/libnccl-net.so is not a symbolic link

DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/deep_ep-1.0.0+a84a248-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting triton==3.3.1
  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)
Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton==3.3.1) (78.1.1)
Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.7/155.7 MB 274.8 MB/s eta 0:00:00
Installing collected packages: triton
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
Successfully installed triton-3.3.1
Launching Torch distributed as node rank 0 out of 2 nodes
Launched rank 0 with PID 1453
Launched rank 1 with PID 1454
Launched rank 2 with PID 1455
Launched rank 3 with PID 1456
Launched rank 4 with PID 1457
Launched rank 5 with PID 1458
Launched rank 6 with PID 1459
Launched rank 7 with PID 1460
Waiting on Torch PID 1453
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.

0it [00:00, ?it/s]
0it [00:00, ?it/s]
[NeMo W 2025-06-20 10:18:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-06-20 10:18:26 nemo_logging:393] 
    
    ************** Experiment configuration ***********
[NeMo I 2025-06-20 10:18:26 nemo_logging:393] 
    run:
      name: qwen3-30b_16gpu_tp1_pp2_cp1_vp1_ep4_gbs256_mbs4
      results_dir: ${base_results_dir}/${.name}
      time_limit: 0-02:30:00
      dependency: singleton
    trainer:
      devices: 8
      accelerator: gpu
      precision: bf16
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_epochs: null
      max_steps: 20
      max_time: 05:23:30:00
      log_every_n_steps: 1
      val_check_interval: null
      limit_val_batches: 0.0
      limit_test_batches: 5
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
      num_nodes: 2
    exp_manager:
      exp_dir: /nemo-experiments/
      name: megatron_llama
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: nemo_llama_pretrain
        name: ${run.name}
      create_dllogger_logger: true
      dllogger_logger_kwargs:
        verbose: true
        stdout: true
        json_file: ./dllogger.json
      resume_if_exists: false
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: false
      checkpoint_callback_params:
        monitor: val_loss
        save_top_k: 10
        mode: min
        always_save_nemo: false
        save_nemo_on_train_end: false
        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
      log_step_timing: true
      step_timing_kwargs:
        sync_cuda: true
        buffer_size: 5
      seconds_to_sleep: 60
      version: achiu-qwen3-30b-16gpu-tp1-pp2-cp1-vp1-ep4-gbs256-mbs4-1750414640-hg8j
      explicit_log_dir: /nemo-experiments/results
    model:
      override_vocab_size: 151936
      tp_only_amax_red: true
      mcore_gpt: true
      moe_grouped_gemm: true
      moe_token_dispatcher_type: alltoall
      moe_pad_expert_input_to_capacity: true
      moe_expert_capacity_factor: 1.0
      moe_aux_loss_coeff: 0.001
      moe_router_load_balancing_type: aux_loss
      moe_router_pre_softmax: false
      moe_permute_fusion: true
      micro_batch_size: 4
      global_batch_size: 256
      rampup_batch_size: null
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      context_parallel_size: 1
      expert_model_parallel_size: 8
      encoder_seq_length: 4096
      max_position_embeddings: 40960
      num_layers: 48
      hidden_size: 2048
      ffn_hidden_size: 6144
      moe_ffn_hidden_size: 768
      num_attention_heads: 32
      num_query_groups: 4
      init_method_std: 0.02
      use_scaled_init_method: true
      hidden_dropout: 0.0
      attention_dropout: 0.0
      ffn_dropout: 0.0
      kv_channels: 128
      apply_query_key_layer_scaling: true
      normalization: rmsnorm
      layernorm_epsilon: 1.0e-05
      do_layer_norm_weight_decay: false
      make_vocab_size_divisible_by: 128
      pre_process: true
      post_process: true
      persist_layer_norm: true
      bias: false
      activation: fast-swiglu
      headscale: false
      transformer_block_type: pre_ln
      openai_gelu: false
      normalize_attention_scores: true
      position_embedding_type: rope
      rotary_percentage: 1.0
      apply_rope_fusion: true
      moe_router_topk: 8
      num_moe_experts: 128
      attention_type: multihead
      share_embeddings_and_output_weights: false
      scale_positional_embedding: true
      tokenizer:
        library: megatron
        type: GPT2BPETokenizer
        model: null
        delimiter: null
        vocab_file: gpt2-vocab.json
        merge_file: gpt2-merges.txt
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: true
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: true
      gradient_accumulation_fusion: true
      cross_entropy_loss_fusion: true
      bias_activation_fusion: true
      bias_dropout_add_fusion: true
      masked_softmax_fusion: true
      get_attention_mask_from_fusion: true
      seed: 1234
      resume_from_checkpoint: null
      use_cpu_initialization: false
      onnx_safe: false
      apex_transformer_log_level: 30
      gradient_as_bucket_view: true
      sync_batch_comm: false
      activations_checkpoint_granularity: null
      activations_checkpoint_method: null
      activations_checkpoint_num_layers: null
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: false
      defer_embedding_wgrad_compute: false
      wgrad_deferral_limit: 50
      deterministic_mode: false
      transformer_engine: true
      fp8: true
      fp8_e4m3: false
      fp8_hybrid: true
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1024
      fp8_amax_compute_algo: max
      fp8_params: true
      ub_tp_comm_overlap: true
      ub_tp_comm_overlap_cfg: null
      use_flash_attention: true
      overlap_p2p_comm: false
      batch_p2p_comm: false
      gc_interval: 100
      nsys_profile:
        enabled: false
        trace:
        - nvtx
        - cuda
        start_step: 25
        end_step: 30
        ranks:
        - 0
        gen_shape: false
      optim:
        name: mcore_distributed_optim
        lr: 0.00015
        weight_decay: 0.1
        betas:
        - 0.9
        - 0.95
        bucket_cap_mb: 125
        overlap_grad_sync: true
        overlap_param_sync: true
        contiguous_grad_buffer: true
        contiguous_param_buffer: true
        grad_sync_dtype: bf16
        sched:
          name: CosineAnnealing
          warmup_steps: 2000
          constant_steps: 11873
          min_lr: 1.0e-05
      data:
        data_impl: mock
        splits_string: 99990,8,2
        seq_length: 4096
        skip_warmup: true
        num_workers: 2
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        index_mapping_dir: /gcs/index_mapping_dir
        data_prefix:
        - 1.0
        - /ssd/.cache/wikipedia-tokenized-for-gpt2
    
[NeMo W 2025-06-20 10:18:26 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `lightning.pytorch.plugins.precision.MixedPrecision` instead.
    
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] ExpManager schema
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False, 'save_last_n_optim_states': -1}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'log_delta_step_timing': False, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}
[NeMo E 2025-06-20 10:18:28 nemo_logging:417] You are running multi-node training without SLURM handling the processes. Please note that this is not tested in NeMo and could result in errors.
[NeMo E 2025-06-20 10:18:28 nemo_logging:417] exp_manager received explicit_log_dir: /nemo-experiments/results and at least one of exp_dir: /nemo-experiments/, or version: achiu-qwen3-30b-16gpu-tp1-pp2-cp1-vp1-ep4-gbs256-mbs4-1750414640-hg8j. Please note that exp_dir, name, and version will be ignored.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] Exp_manager is logging to /nemo-experiments/results, but it already exists.
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Experiments will be logged at /nemo-experiments/results
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] TensorboardLogger has been set up
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] DLLogger has been set up
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_comm_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: hierarchical_context_parallel_sizes in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_tensor_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cross_entropy_fusion_impl in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm_warmup_flush in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: microbatch_group_size_per_vp_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Ranks 0 has data parallel rank: 0
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has context parallel group: [0]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Ranks 0 has context parallel rank: 0
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has model parallel group: [0]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has embedding group: [0]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Rank 0 has embedding rank: 0
[NeMo I 2025-06-20 10:18:28 num_microbatches_calculator:228] setting number of microbatches to constant 4
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Pipelined tensor-parallel communication overlap is available with sequence-parallelism.Setting `ub_tp_comm_overlap` to False.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_comm_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: hierarchical_context_parallel_sizes in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_tensor_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cross_entropy_fusion_impl in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm_warmup_flush in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: microbatch_group_size_per_vp_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /app/training/gpt2-vocab.json, and merges file: /app/training/gpt2-merges.txt
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /app/training/gpt2-vocab.json, merges_files: /app/training/gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-06-20 10:18:28 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_comm_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: hierarchical_context_parallel_sizes in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: expert_tensor_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cross_entropy_fusion_impl in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm_warmup_flush in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: microbatch_group_size_per_vp_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mtp_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mtp_loss_scaling_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_in_first_pipeline_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_in_last_pipeline_stage in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: softmax_scale in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: init_model_with_meta_device in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: disable_bf16_reduced_precision_matmul in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: recompute_modules in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_recipe in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_param in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: first_last_layers_bf16 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_at_start_in_bf16 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: num_layers_at_end_in_bf16 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_layer_freq in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_topk_limited_devices in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_num_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_group_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_topk_scaling_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_score_function in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_dtype in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_enable_expert_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_router_bias_update_rate in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_use_legacy_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_enable_deepep in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: moe_apply_probs_on_input in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_use_single_mempool in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_retain_backward_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_warmup_steps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: cuda_graph_scope in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: flash_decode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: inference_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mrope_section in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: is_hybrid_model in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mamba_state_dim in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mamba_head_dim in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: mamba_num_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2025-06-20 10:18:28 nemo_logging:405] The model: MegatronGPTModel() does not have field.name: heterogeneous_block_specs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
apply rope scaling ...
[NeMo W 2025-06-20 10:18:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:161: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
    
[NeMo W 2025-06-20 10:18:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.
    
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/16
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/16
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/16
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/16
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/16
apply rope scaling ...
apply rope scaling ...
apply rope scaling ...
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/16
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/16
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/16
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 16 processes
----------------------------------------------------------------------------------------------------

[NeMo I 2025-06-20 10:19:38 nemo_logging:393] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 5.16e+09. Number of precise model parameters on device: 5164959744.
[NeMo I 2025-06-20 10:19:38 nemo_logging:393] Building GPT datasets.
[NeMo I 2025-06-20 10:19:38 utils:532] Let mock = True, as both blend and blend_per_split are None
[NeMo I 2025-06-20 10:19:38 utils:532] Let split = 1,1,1, an arbitrarily even split, as mock is True
[NeMo I 2025-06-20 10:19:38 utils:532] Let split_matrix = [(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)]
[NeMo I 2025-06-20 10:19:38 utils:532] Building MockGPTDataset splits with sizes=[5120, None, 1280] and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=None, blend_per_split=None, split='1,1,1', split_matrix=[(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)], num_dataset_builder_threads=1, path_to_cache='/gcs/index_mapping_dir', mmap_bin_files=True, mock=True, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x78e84836e5d0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)
[NeMo I 2025-06-20 10:19:38 utils:532] Load the MockGPTDataset train indices
[NeMo I 2025-06-20 10:19:38 utils:532] 	Load the document index from dda32ae74f9d8cdad88dcb5454743969-MockGPTDataset-train-document_index.npy
[NeMo I 2025-06-20 10:19:38 utils:532] 	Load the sample index from dda32ae74f9d8cdad88dcb5454743969-MockGPTDataset-train-sample_index.npy
[NeMo I 2025-06-20 10:19:38 utils:532] 	Load the shuffle index from dda32ae74f9d8cdad88dcb5454743969-MockGPTDataset-train-shuffle_index.npy
[NeMo I 2025-06-20 10:19:38 utils:532] > total number of samples: 16648
[NeMo I 2025-06-20 10:19:38 utils:532] Load the MockGPTDataset valid indices
[NeMo I 2025-06-20 10:19:38 utils:532] 	Load the document index from e0fcd0b41a52fdff26a08c0b8bddb0cf-MockGPTDataset-valid-document_index.npy
[NeMo I 2025-06-20 10:19:38 utils:532] 	Load the sample index from e0fcd0b41a52fdff26a08c0b8bddb0cf-MockGPTDataset-valid-sample_index.npy
[NeMo I 2025-06-20 10:19:38 utils:532] 	Load the shuffle index from e0fcd0b41a52fdff26a08c0b8bddb0cf-MockGPTDataset-valid-shuffle_index.npy
[NeMo I 2025-06-20 10:19:39 utils:532] > total number of samples: 16640
[NeMo I 2025-06-20 10:19:39 utils:532] Load the MockGPTDataset test indices
[NeMo I 2025-06-20 10:19:39 utils:532] 	Load the document index from b7f9e7b59a68bd49b9182fbc85c5f04f-MockGPTDataset-test-document_index.npy
[NeMo I 2025-06-20 10:19:39 utils:532] 	Load the sample index from b7f9e7b59a68bd49b9182fbc85c5f04f-MockGPTDataset-test-sample_index.npy
[NeMo I 2025-06-20 10:19:39 utils:532] 	Load the shuffle index from b7f9e7b59a68bd49b9182fbc85c5f04f-MockGPTDataset-test-shuffle_index.npy
[NeMo I 2025-06-20 10:19:39 utils:532] > total number of samples: 16671
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Length of train dataset: 16648
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Length of val dataset: 16640
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Length of test dataset: 16671
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Finished building GPT datasets.
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Setting up train dataloader with len(len(self._train_ds)): 16648 and consumed samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Building dataloader with consumed samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Instantiating MegatronPretrainingSampler with total_samples: 16648 and consumed_samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Setting up validation dataloader with len(len(self._validation_ds)): 16640 and consumed samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Building dataloader with consumed samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Instantiating MegatronPretrainingSampler with total_samples: 16640 and consumed_samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Setting up test dataloader with len(len(self._test_ds)): 16671 and consumed samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Building dataloader with consumed samples: 0
[NeMo I 2025-06-20 10:19:39 nemo_logging:393] Instantiating MegatronPretrainingSampler with total_samples: 16671 and consumed_samples: 0
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[NeMo I 2025-06-20 10:19:39 utils:532] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=40000000, pad_buckets_for_high_nccl_busbw=False, average_in_collective=True, fp8_param_gather=True, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)
[NeMo I 2025-06-20 10:19:39 utils:553] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (311164928 elements, 311164928 padded size):
    	module.output_layer.weight
    Params for bucket 2 (323946496 elements, 323946496 padded size):
    	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.41.mlp.router.weight
    	module.decoder.layers.35.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.19.mlp.router.weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.router.weight
    	module.decoder.layers.0.mlp.router.weight
    	module.decoder.layers.46.mlp.router.weight
    	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.19.pre_mlp_layernorm.weight
    	module.decoder.layers.3.pre_mlp_layernorm.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.41.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.31.mlp.router.weight
    	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.46.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.36.mlp.router.weight
    	module.decoder.layers.25.mlp.router.weight
    	module.decoder.layers.24.mlp.router.weight
    	module.decoder.layers.15.mlp.router.weight
    	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.23.pre_mlp_layernorm.weight
    	module.decoder.layers.18.pre_mlp_layernorm.weight
    	module.decoder.layers.36.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.pre_mlp_layernorm.weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.router.weight
    	module.decoder.layers.47.mlp.router.weight
    	module.decoder.layers.47.pre_mlp_layernorm.weight
    	module.decoder.layers.25.pre_mlp_layernorm.weight
    	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.mlp.router.weight
    	module.decoder.layers.32.mlp.router.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.47.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.37.mlp.router.weight
    	module.decoder.layers.23.mlp.router.weight
    	module.decoder.layers.16.mlp.router.weight
    	module.decoder.layers.42.mlp.router.weight
    	module.decoder.layers.40.pre_mlp_layernorm.weight
    	module.decoder.layers.21.mlp.router.weight
    	module.decoder.layers.20.mlp.router.weight
    	module.decoder.layers.13.pre_mlp_layernorm.weight
    	module.decoder.layers.1.mlp.router.weight
    	module.decoder.layers.41.pre_mlp_layernorm.weight
    	module.decoder.layers.37.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.35.pre_mlp_layernorm.weight
    	module.decoder.layers.27.mlp.router.weight
    	module.decoder.layers.13.mlp.router.weight
    	module.decoder.layers.11.pre_mlp_layernorm.weight
    	module.decoder.layers.6.mlp.router.weight
    	module.decoder.layers.2.pre_mlp_layernorm.weight
    	module.decoder.layers.36.pre_mlp_layernorm.weight
    	module.decoder.layers.46.pre_mlp_layernorm.weight
    	module.decoder.layers.42.pre_mlp_layernorm.weight
    	module.decoder.layers.42.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.27.pre_mlp_layernorm.weight
    	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.20.pre_mlp_layernorm.weight
    	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.pre_mlp_layernorm.weight
    	module.decoder.layers.45.pre_mlp_layernorm.weight
    	module.decoder.layers.31.pre_mlp_layernorm.weight
    	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.22.pre_mlp_layernorm.weight
    	module.decoder.layers.10.pre_mlp_layernorm.weight
    	module.decoder.layers.8.pre_mlp_layernorm.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.pre_mlp_layernorm.weight
    	module.decoder.layers.38.mlp.router.weight
    	module.decoder.layers.32.pre_mlp_layernorm.weight
    	module.decoder.layers.32.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.28.pre_mlp_layernorm.weight
    	module.decoder.layers.7.pre_mlp_layernorm.weight
    	module.decoder.layers.43.mlp.router.weight
    	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.28.mlp.router.weight
    	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.43.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.33.mlp.router.weight
    	module.decoder.layers.12.mlp.router.weight
    	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.22.mlp.router.weight
    	module.decoder.layers.17.mlp.router.weight
    	module.decoder.layers.9.pre_mlp_layernorm.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.33.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.21.pre_mlp_layernorm.weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.34.pre_mlp_layernorm.weight
    	module.decoder.layers.44.mlp.router.weight
    	module.decoder.layers.44.pre_mlp_layernorm.weight
    	module.decoder.layers.38.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.router.weight
    	module.decoder.layers.11.mlp.router.weight
    	module.decoder.layers.44.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.34.mlp.router.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.39.mlp.router.weight
    	module.decoder.layers.39.pre_mlp_layernorm.weight
    	module.decoder.layers.26.pre_mlp_layernorm.weight
    	module.decoder.layers.18.mlp.router.weight
    	module.decoder.layers.34.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.router.weight
    	module.decoder.layers.1.pre_mlp_layernorm.weight
    	module.embedding.word_embeddings.weight
    	module.decoder.layers.39.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.29.mlp.router.weight
    	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.8.mlp.router.weight
    	module.decoder.layers.24.pre_mlp_layernorm.weight
    	module.decoder.layers.17.pre_mlp_layernorm.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.35.mlp.router.weight
    	module.decoder.layers.33.pre_mlp_layernorm.weight
    	module.decoder.layers.29.pre_mlp_layernorm.weight
    	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.26.mlp.router.weight
    	module.decoder.layers.14.pre_mlp_layernorm.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.30.pre_mlp_layernorm.weight
    	module.decoder.layers.40.mlp.router.weight
    	module.decoder.layers.38.pre_mlp_layernorm.weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.pre_mlp_layernorm.weight
    	module.decoder.layers.0.pre_mlp_layernorm.weight
    	module.decoder.layers.45.mlp.router.weight
    	module.decoder.layers.37.pre_mlp_layernorm.weight
    	module.decoder.layers.16.pre_mlp_layernorm.weight
    	module.decoder.layers.6.pre_mlp_layernorm.weight
    	module.decoder.layers.40.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.30.mlp.router.weight
    	module.decoder.layers.10.mlp.router.weight
    	module.decoder.layers.9.mlp.router.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.45.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.43.pre_mlp_layernorm.weight
    	module.decoder.layers.14.mlp.router.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[NeMo I 2025-06-20 10:19:39 utils:553] Number of buckets for gradient all-reduce / reduce-scatter: 20
    Params for bucket 1 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.47.self_attention.linear_proj.weight
    	module.decoder.layers.46.self_attention.linear_qkv.weight
    	module.decoder.layers.47.self_attention.linear_qkv.weight
    	module.decoder.layers.46.self_attention.linear_proj.weight
    	module.decoder.layers.45.self_attention.linear_qkv.weight
    Params for bucket 2 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.45.self_attention.linear_proj.weight
    	module.decoder.layers.43.self_attention.linear_proj.weight
    	module.decoder.layers.44.self_attention.linear_qkv.weight
    	module.decoder.layers.43.self_attention.linear_qkv.weight
    	module.decoder.layers.44.self_attention.linear_proj.weight
    Params for bucket 3 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.42.self_attention.linear_qkv.weight
    	module.decoder.layers.41.self_attention.linear_proj.weight
    	module.decoder.layers.40.self_attention.linear_qkv.weight
    	module.decoder.layers.42.self_attention.linear_proj.weight
    	module.decoder.layers.41.self_attention.linear_qkv.weight
    Params for bucket 4 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.39.self_attention.linear_proj.weight
    	module.decoder.layers.38.self_attention.linear_qkv.weight
    	module.decoder.layers.40.self_attention.linear_proj.weight
    	module.decoder.layers.38.self_attention.linear_proj.weight
    	module.decoder.layers.39.self_attention.linear_qkv.weight
    Params for bucket 5 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.37.self_attention.linear_proj.weight
    	module.decoder.layers.36.self_attention.linear_qkv.weight
    	module.decoder.layers.37.self_attention.linear_qkv.weight
    	module.decoder.layers.35.self_attention.linear_qkv.weight
    	module.decoder.layers.36.self_attention.linear_proj.weight
    Params for bucket 6 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.35.self_attention.linear_proj.weight
    	module.decoder.layers.34.self_attention.linear_qkv.weight
    	module.decoder.layers.33.self_attention.linear_proj.weight
    	module.decoder.layers.33.self_attention.linear_qkv.weight
    	module.decoder.layers.34.self_attention.linear_proj.weight
    Params for bucket 7 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.32.self_attention.linear_qkv.weight
    	module.decoder.layers.30.self_attention.linear_qkv.weight
    	module.decoder.layers.31.self_attention.linear_proj.weight
    	module.decoder.layers.31.self_attention.linear_qkv.weight
    	module.decoder.layers.32.self_attention.linear_proj.weight
    Params for bucket 8 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.28.self_attention.linear_qkv.weight
    	module.decoder.layers.29.self_attention.linear_proj.weight
    	module.decoder.layers.30.self_attention.linear_proj.weight
    	module.decoder.layers.29.self_attention.linear_qkv.weight
    	module.decoder.layers.28.self_attention.linear_proj.weight
    Params for bucket 9 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.25.self_attention.linear_qkv.weight
    	module.decoder.layers.26.self_attention.linear_proj.weight
    	module.decoder.layers.27.self_attention.linear_proj.weight
    	module.decoder.layers.27.self_attention.linear_qkv.weight
    	module.decoder.layers.26.self_attention.linear_qkv.weight
    Params for bucket 10 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.23.self_attention.linear_proj.weight
    	module.decoder.layers.24.self_attention.linear_qkv.weight
    	module.decoder.layers.25.self_attention.linear_proj.weight
    	module.decoder.layers.23.self_attention.linear_qkv.weight
    	module.decoder.layers.24.self_attention.linear_proj.weight
    Params for bucket 11 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.22.self_attention.linear_qkv.weight
    	module.decoder.layers.20.self_attention.linear_qkv.weight
    	module.decoder.layers.21.self_attention.linear_qkv.weight
    	module.decoder.layers.21.self_attention.linear_proj.weight
    	module.decoder.layers.22.self_attention.linear_proj.weight
    Params for bucket 12 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.18.self_attention.linear_proj.weight
    	module.decoder.layers.19.self_attention.linear_proj.weight
    	module.decoder.layers.20.self_attention.linear_proj.weight
    	module.decoder.layers.19.self_attention.linear_qkv.weight
    	module.decoder.layers.18.self_attention.linear_qkv.weight
    Params for bucket 13 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.17.self_attention.linear_qkv.weight
    	module.decoder.layers.16.self_attention.linear_proj.weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.17.self_attention.linear_proj.weight
    	module.decoder.layers.16.self_attention.linear_qkv.weight
    Params for bucket 14 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    Params for bucket 15 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    Params for bucket 16 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    Params for bucket 17 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    Params for bucket 18 (46137344 elements, 46137344 padded size):
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    Params for bucket 19 (48234496 elements, 48234496 padded size):
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    Params for bucket 20 (8388608 elements, 8388608 padded size):
    	module.decoder.layers.0.self_attention.linear_proj.weight
[NeMo I 2025-06-20 10:19:39 utils:553] Number of buckets for gradient all-reduce / reduce-scatter: 89
    Params for bucket 1 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.47.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight15
    Params for bucket 2 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.47.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight13
    Params for bucket 3 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.46.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight10
    Params for bucket 4 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.46.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight8
    Params for bucket 5 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.45.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight7
    Params for bucket 6 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.45.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight15
    Params for bucket 7 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.44.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight6
    Params for bucket 8 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.44.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.43.mlp.experts.linear_fc2.weight6
    Params for bucket 9 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight3
    Params for bucket 10 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.43.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.42.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight14
    Params for bucket 11 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight2
    Params for bucket 12 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.42.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.41.mlp.experts.linear_fc2.weight0
    Params for bucket 13 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.41.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight15
    Params for bucket 14 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.40.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight12
    Params for bucket 15 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.40.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight10
    Params for bucket 16 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.39.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight9
    Params for bucket 17 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.39.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight7
    Params for bucket 18 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.38.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight6
    Params for bucket 19 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.38.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight8
    Params for bucket 20 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.37.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight5
    Params for bucket 21 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.37.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.36.mlp.experts.linear_fc2.weight3
    Params for bucket 22 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight2
    Params for bucket 23 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.36.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.35.mlp.experts.linear_fc2.weight6
    Params for bucket 24 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.35.mlp.experts.linear_fc1.weight1
    Params for bucket 25 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.34.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight14
    Params for bucket 26 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.34.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight12
    Params for bucket 27 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.33.mlp.experts.linear_fc2.weight0
    Params for bucket 28 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.33.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight9
    Params for bucket 29 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.32.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight8
    Params for bucket 30 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.32.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight10
    Params for bucket 31 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.31.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight5
    Params for bucket 32 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.31.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.30.mlp.experts.linear_fc2.weight5
    Params for bucket 33 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight4
    Params for bucket 34 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.30.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.29.mlp.experts.linear_fc2.weight0
    Params for bucket 35 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight1
    Params for bucket 36 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.29.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.28.mlp.experts.linear_fc2.weight7
    Params for bucket 37 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.28.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight14
    Params for bucket 38 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.27.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight11
    Params for bucket 39 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.27.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight11
    Params for bucket 40 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.26.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight8
    Params for bucket 41 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.26.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight6
    Params for bucket 42 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.25.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight6
    Params for bucket 43 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.25.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight9
    Params for bucket 44 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.24.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight5
    Params for bucket 45 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.24.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.23.mlp.experts.linear_fc2.weight4
    Params for bucket 46 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight3
    Params for bucket 47 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.23.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.22.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight15
    Params for bucket 48 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.22.mlp.experts.linear_fc1.weight1
    Params for bucket 49 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.21.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight15
    Params for bucket 50 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.21.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight13
    Params for bucket 51 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.20.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight14
    Params for bucket 52 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.20.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight8
    Params for bucket 53 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.19.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight7
    Params for bucket 54 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.19.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight6
    Params for bucket 55 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.18.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight5
    Params for bucket 56 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.18.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.17.mlp.experts.linear_fc2.weight1
    Params for bucket 57 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight4
    Params for bucket 58 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.17.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.16.mlp.experts.linear_fc2.weight0
    Params for bucket 59 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight1
    Params for bucket 60 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.16.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.15.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight12
    Params for bucket 61 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.15.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight14
    Params for bucket 62 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.14.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight11
    Params for bucket 63 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.14.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight11
    Params for bucket 64 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.13.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight8
    Params for bucket 65 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.13.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight7
    Params for bucket 66 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.12.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight6
    Params for bucket 67 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.12.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight3
    Params for bucket 68 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.11.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight5
    Params for bucket 69 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.11.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.10.mlp.experts.linear_fc2.weight3
    Params for bucket 70 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight3
    Params for bucket 71 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.10.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.9.mlp.experts.linear_fc2.weight2
    Params for bucket 72 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.9.mlp.experts.linear_fc1.weight1
    Params for bucket 73 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.8.mlp.experts.linear_fc2.weight0
    Params for bucket 74 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.8.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight13
    Params for bucket 75 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.7.mlp.experts.linear_fc2.weight3
    Params for bucket 76 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.7.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight8
    Params for bucket 77 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.6.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight7
    Params for bucket 78 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.6.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight5
    Params for bucket 79 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.5.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight6
    Params for bucket 80 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.5.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.4.mlp.experts.linear_fc2.weight0
    Params for bucket 81 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight3
    Params for bucket 82 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.4.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.3.mlp.experts.linear_fc2.weight7
    Params for bucket 83 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight2
    Params for bucket 84 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.3.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.2.mlp.experts.linear_fc2.weight2
    Params for bucket 85 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.2.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight15
    Params for bucket 86 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight10
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.1.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight10
    Params for bucket 87 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight15
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight13
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight11
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight8
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight2
    	module.decoder.layers.1.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight14
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight12
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight10
    Params for bucket 88 (40894464 elements, 40894464 padded size):
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight9
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight7
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight5
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight3
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight1
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight15
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight13
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight11
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight9
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight6
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight8
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight4
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight2
    	module.decoder.layers.0.mlp.experts.linear_fc2.weight0
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight14
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight12
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight10
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight8
    Params for bucket 89 (25165824 elements, 25165824 padded size):
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight7
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight5
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight3
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight1
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight0
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight6
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight4
    	module.decoder.layers.0.mlp.experts.linear_fc1.weight2
[NeMo I 2025-06-20 10:19:39 utils:532] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
[NeMo I 2025-06-20 10:19:40 nemo_logging:393] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x78e7941ecfe0>" 
    will be used during training (effective maximum steps = 20) - 
    Parameters : 
    (warmup_steps: 2000
    constant_steps: 11873
    min_lr: 1.0e-05
    max_steps: 20
    )

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 5.2 B  | n/a 
---------------------------------------------
5.2 B     Trainable params
0         Non-trainable params
5.2 B     Total params
20,659.839Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
DLL 2025-06-20 10:19:40.181649 - PARAMETER cfg/override_vocab_size : 151936  cfg/tp_only_amax_red : True  cfg/mcore_gpt : True  cfg/moe_grouped_gemm : True  cfg/moe_token_dispatcher_type : alltoall  cfg/moe_pad_expert_input_to_capacity : True  cfg/moe_expert_capacity_factor : 1.0  cfg/moe_aux_loss_coeff : 0.001  cfg/moe_router_load_balancing_type : aux_loss  cfg/moe_router_pre_softmax : False  cfg/moe_permute_fusion : True  cfg/micro_batch_size : 4  cfg/global_batch_size : 256  cfg/rampup_batch_size : None  cfg/tensor_model_parallel_size : 1  cfg/pipeline_model_parallel_size : 1  cfg/virtual_pipeline_model_parallel_size : None  cfg/context_parallel_size : 1  cfg/expert_model_parallel_size : 8  cfg/encoder_seq_length : 4096  cfg/max_position_embeddings : 40960  cfg/num_layers : 48  cfg/hidden_size : 2048  cfg/ffn_hidden_size : 6144  cfg/moe_ffn_hidden_size : 768  cfg/num_attention_heads : 32  cfg/num_query_groups : 4  cfg/init_method_std : 0.02  cfg/use_scaled_init_method : True  cfg/hidden_dropout : 0.0  cfg/attention_dropout : 0.0  cfg/ffn_dropout : 0.0  cfg/kv_channels : 128  cfg/apply_query_key_layer_scaling : True  cfg/normalization : rmsnorm  cfg/layernorm_epsilon : 1e-05  cfg/do_layer_norm_weight_decay : False  cfg/make_vocab_size_divisible_by : 128  cfg/pre_process : True  cfg/post_process : True  cfg/persist_layer_norm : True  cfg/bias : False  cfg/activation : fast-swiglu  cfg/headscale : False  cfg/transformer_block_type : pre_ln  cfg/openai_gelu : False  cfg/normalize_attention_scores : True  cfg/position_embedding_type : rope  cfg/rotary_percentage : 1.0  cfg/apply_rope_fusion : True  cfg/moe_router_topk : 8  cfg/num_moe_experts : 128  cfg/attention_type : multihead  cfg/share_embeddings_and_output_weights : False  cfg/scale_positional_embedding : True  cfg/tokenizer/library : megatron  cfg/tokenizer/type : GPT2BPETokenizer  cfg/tokenizer/model : None  cfg/tokenizer/delimiter : None  cfg/tokenizer/vocab_file : gpt2-vocab.json  cfg/tokenizer/merge_file : gpt2-merges.txt  cfg/native_amp_init_scale : 4294967296  cfg/native_amp_growth_interval : 1000  cfg/hysteresis : 2  cfg/fp32_residual_connection : False  cfg/fp16_lm_cross_entropy : False  cfg/megatron_amp_O2 : True  cfg/grad_allreduce_chunk_size_mb : 125  cfg/grad_div_ar_fusion : True  cfg/gradient_accumulation_fusion : True  cfg/cross_entropy_loss_fusion : True  cfg/bias_activation_fusion : True  cfg/bias_dropout_add_fusion : True  cfg/masked_softmax_fusion : True  cfg/get_attention_mask_from_fusion : True  cfg/seed : 1234  cfg/resume_from_checkpoint : None  cfg/use_cpu_initialization : False  cfg/onnx_safe : False  cfg/apex_transformer_log_level : 30  cfg/gradient_as_bucket_view : True  cfg/sync_batch_comm : False  cfg/activations_checkpoint_granularity : None  cfg/activations_checkpoint_method : None  cfg/activations_checkpoint_num_layers : None  cfg/num_micro_batches_with_partial_activation_checkpoints : None  cfg/activations_checkpoint_layers_per_pipeline : None  cfg/sequence_parallel : False  cfg/defer_embedding_wgrad_compute : False  cfg/wgrad_deferral_limit : 50  cfg/deterministic_mode : False  cfg/transformer_engine : True  cfg/fp8 : True  cfg/fp8_e4m3 : False  cfg/fp8_hybrid : True  cfg/fp8_margin : 0  cfg/fp8_interval : 1  cfg/fp8_amax_history_len : 1024  cfg/fp8_amax_compute_algo : max  cfg/fp8_params : True  cfg/ub_tp_comm_overlap : True  cfg/ub_tp_comm_overlap_cfg : None  cfg/use_flash_attention : True  cfg/overlap_p2p_comm : False  cfg/batch_p2p_comm : False  cfg/gc_interval : 100  cfg/nsys_profile/enabled : False  cfg/nsys_profile/trace : ['nvtx', 'cuda']  cfg/nsys_profile/start_step : 25  cfg/nsys_profile/end_step : 30  cfg/nsys_profile/ranks : [0]  cfg/nsys_profile/gen_shape : False  cfg/optim/name : mcore_distributed_optim  cfg/optim/lr : 0.00015  cfg/optim/weight_decay : 0.1  cfg/optim/betas : [0.9, 0.95]  cfg/optim/bucket_cap_mb : 125  cfg/optim/overlap_grad_sync : True  cfg/optim/overlap_param_sync : True  cfg/optim/contiguous_grad_buffer : True  cfg/optim/contiguous_param_buffer : True  cfg/optim/grad_sync_dtype : bf16  cfg/optim/sched/name : CosineAnnealing  cfg/optim/sched/warmup_steps : 2000  cfg/optim/sched/constant_steps : 11873  cfg/optim/sched/min_lr : 1e-05  cfg/data/data_impl : mock  cfg/data/splits_string : 99990,8,2  cfg/data/seq_length : 4096  cfg/data/skip_warmup : True  cfg/data/num_workers : 2  cfg/data/dataloader_type : single  cfg/data/reset_position_ids : False  cfg/data/reset_attention_mask : False  cfg/data/eod_mask_loss : False  cfg/data/index_mapping_dir : /gcs/index_mapping_dir  cfg/data/data_prefix : [1.0, '/ssd/.cache/wikipedia-tokenized-for-gpt2']  cfg/precision : bf16-mixed 
[NeMo W 2025-06-20 10:19:53 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo W 2025-06-20 10:20:04 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    

Training: |          | 0/? [00:00<?, ?it/s]
Training:   0%|          | 0/20 [00:00<?]  
Epoch 0: :   0%|          | 0/20 [00:00<?]
Epoch 0: :   5%|▌         | 1/20 [00:22<07:11]
Epoch 0: :   5%|▌         | 1/20 [00:22<07:11, v_num=None, reduced_train_loss=12.30, global_step=0.000, consumed_samples=256.0, train_step_timing in s=22.70]DLL 2025-06-20 10:20:32.799697 - 0 reduced_train_loss : 12.339681625366211  lr : 0.0  global_step : 0.0  consumed_samples : 256.0  train_backward_timing in s : 4.57763671875e-05  train_step_timing in s : 22.710954189300537  epoch : 0 

Epoch 0: :  10%|█         | 2/20 [00:26<04:00, v_num=None, reduced_train_loss=12.30, global_step=0.000, consumed_samples=256.0, train_step_timing in s=22.70]
Epoch 0: :  10%|█         | 2/20 [00:26<04:00, v_num=None, reduced_train_loss=12.30, global_step=1.000, consumed_samples=512.0, train_step_timing in s=13.30]DLL 2025-06-20 10:20:36.785939 - 1 reduced_train_loss : 12.336347579956055  lr : 7.500000265281415e-08  global_step : 1.0  consumed_samples : 512.0  train_backward_timing in s : 4.279613494873047e-05  train_step_timing in s : 13.346818089485168  epoch : 0 

Epoch 0: :  15%|█▌        | 3/20 [00:30<02:53, v_num=None, reduced_train_loss=12.30, global_step=1.000, consumed_samples=512.0, train_step_timing in s=13.30]
Epoch 0: :  15%|█▌        | 3/20 [00:30<02:53, v_num=None, reduced_train_loss=12.30, global_step=2.000, consumed_samples=768.0, train_step_timing in s=10.20]DLL 2025-06-20 10:20:40.702506 - 2 reduced_train_loss : 12.33846664428711  lr : 1.500000053056283e-07  global_step : 2.0  consumed_samples : 768.0  train_backward_timing in s : 4.291534423828125e-05  train_step_timing in s : 10.20219612121582  epoch : 0 

Epoch 0: :  20%|██        | 4/20 [00:34<02:18, v_num=None, reduced_train_loss=12.30, global_step=2.000, consumed_samples=768.0, train_step_timing in s=10.20]
Epoch 0: :  20%|██        | 4/20 [00:34<02:18, v_num=None, reduced_train_loss=12.30, global_step=3.000, consumed_samples=1024.0, train_step_timing in s=8.630]DLL 2025-06-20 10:20:44.630598 - 3 reduced_train_loss : 12.33735466003418  lr : 2.2499999374758772e-07  global_step : 3.0  consumed_samples : 1024.0  train_backward_timing in s : 4.3511390686035156e-05  train_step_timing in s : 8.63277804851532  epoch : 0 

Epoch 0: :  25%|██▌       | 5/20 [00:38<01:55, v_num=None, reduced_train_loss=12.30, global_step=3.000, consumed_samples=1024.0, train_step_timing in s=8.630]
Epoch 0: :  25%|██▌       | 5/20 [00:38<01:55, v_num=None, reduced_train_loss=12.30, global_step=4.000, consumed_samples=1280.0, train_step_timing in s=7.690]DLL 2025-06-20 10:20:48.554730 - 4 reduced_train_loss : 12.335771560668945  lr : 3.000000106112566e-07  global_step : 4.0  consumed_samples : 1280.0  train_backward_timing in s : 4.2390823364257815e-05  train_step_timing in s : 7.690323352813721  epoch : 0 

Epoch 0: :  30%|███       | 6/20 [00:42<01:38, v_num=None, reduced_train_loss=12.30, global_step=4.000, consumed_samples=1280.0, train_step_timing in s=7.690]
Epoch 0: :  30%|███       | 6/20 [00:42<01:38, v_num=None, reduced_train_loss=12.30, global_step=5.000, consumed_samples=1536.0, train_step_timing in s=3.930]DLL 2025-06-20 10:20:52.488942 - 5 reduced_train_loss : 12.330609321594238  lr : 3.7500001326407073e-07  global_step : 5.0  consumed_samples : 1536.0  train_backward_timing in s : 4.124641418457031e-05  train_step_timing in s : 3.9342631340026855  epoch : 0 

Epoch 0: :  35%|███▌      | 7/20 [00:46<01:26, v_num=None, reduced_train_loss=12.30, global_step=5.000, consumed_samples=1536.0, train_step_timing in s=3.930]
Epoch 0: :  35%|███▌      | 7/20 [00:46<01:26, v_num=None, reduced_train_loss=12.30, global_step=6.000, consumed_samples=1792.0, train_step_timing in s=3.920]DLL 2025-06-20 10:20:56.409850 - 6 reduced_train_loss : 12.329671859741211  lr : 4.4999998749517545e-07  global_step : 6.0  consumed_samples : 1792.0  train_backward_timing in s : 4.100799560546875e-05  train_step_timing in s : 3.9211737155914306  epoch : 0 

Epoch 0: :  40%|████      | 8/20 [00:50<01:15, v_num=None, reduced_train_loss=12.30, global_step=6.000, consumed_samples=1792.0, train_step_timing in s=3.920]
Epoch 0: :  40%|████      | 8/20 [00:50<01:15, v_num=None, reduced_train_loss=12.30, global_step=7.000, consumed_samples=2048.0, train_step_timing in s=3.920]DLL 2025-06-20 10:21:00.334451 - 7 reduced_train_loss : 12.325464248657227  lr : 5.25000018569699e-07  global_step : 7.0  consumed_samples : 2048.0  train_backward_timing in s : 4.024505615234375e-05  train_step_timing in s : 3.9227892398834228  epoch : 0 

Epoch 0: :  45%|████▌     | 9/20 [00:54<01:06, v_num=None, reduced_train_loss=12.30, global_step=7.000, consumed_samples=2048.0, train_step_timing in s=3.920]
Epoch 0: :  45%|████▌     | 9/20 [00:54<01:06, v_num=None, reduced_train_loss=12.30, global_step=8.000, consumed_samples=2304.0, train_step_timing in s=3.920]DLL 2025-06-20 10:21:04.273599 - 8 reduced_train_loss : 12.316190719604492  lr : 6.000000212225132e-07  global_step : 8.0  consumed_samples : 2304.0  train_backward_timing in s : 3.8957595825195314e-05  train_step_timing in s : 3.9249868869781492  epoch : 0 

Epoch 0: :  50%|█████     | 10/20 [00:58<00:58, v_num=None, reduced_train_loss=12.30, global_step=8.000, consumed_samples=2304.0, train_step_timing in s=3.920]
Epoch 0: :  50%|█████     | 10/20 [00:58<00:58, v_num=None, reduced_train_loss=12.30, global_step=9.000, consumed_samples=2560.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:08.211602 - 9 reduced_train_loss : 12.310684204101562  lr : 6.750000238753273e-07  global_step : 9.0  consumed_samples : 2560.0  train_backward_timing in s : 4.000663757324219e-05  train_step_timing in s : 3.9277627944946287  epoch : 0 

Epoch 0: :  55%|█████▌    | 11/20 [01:02<00:50, v_num=None, reduced_train_loss=12.30, global_step=9.000, consumed_samples=2560.0, train_step_timing in s=3.930]
Epoch 0: :  55%|█████▌    | 11/20 [01:02<00:50, v_num=None, reduced_train_loss=12.30, global_step=10.00, consumed_samples=2816.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:12.140808 - 10 reduced_train_loss : 12.305301666259766  lr : 7.500000265281415e-07  global_step : 10.0  consumed_samples : 2816.0  train_backward_timing in s : 4.076957702636719e-05  train_step_timing in s : 3.9267263889312742  epoch : 0 

Epoch 0: :  60%|██████    | 12/20 [01:05<00:43, v_num=None, reduced_train_loss=12.30, global_step=10.00, consumed_samples=2816.0, train_step_timing in s=3.930]
Epoch 0: :  60%|██████    | 12/20 [01:05<00:43, v_num=None, reduced_train_loss=12.30, global_step=11.00, consumed_samples=3072.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:16.079993 - 11 reduced_train_loss : 12.294624328613281  lr : 8.249999723375367e-07  global_step : 11.0  consumed_samples : 3072.0  train_backward_timing in s : 4.1818618774414064e-05  train_step_timing in s : 3.930397367477417  epoch : 0 

Epoch 0: :  65%|██████▌   | 13/20 [01:09<00:37, v_num=None, reduced_train_loss=12.30, global_step=11.00, consumed_samples=3072.0, train_step_timing in s=3.930]
Epoch 0: :  65%|██████▌   | 13/20 [01:09<00:37, v_num=None, reduced_train_loss=12.30, global_step=12.00, consumed_samples=3328.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:20.015883 - 12 reduced_train_loss : 12.28368091583252  lr : 8.999999749903509e-07  global_step : 12.0  consumed_samples : 3328.0  train_backward_timing in s : 4.2772293090820314e-05  train_step_timing in s : 3.932631587982178  epoch : 0 

Epoch 0: :  70%|███████   | 14/20 [01:13<00:31, v_num=None, reduced_train_loss=12.30, global_step=12.00, consumed_samples=3328.0, train_step_timing in s=3.930]
Epoch 0: :  70%|███████   | 14/20 [01:13<00:31, v_num=None, reduced_train_loss=12.30, global_step=13.00, consumed_samples=3584.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:23.950661 - 13 reduced_train_loss : 12.270427703857422  lr : 9.75000034486584e-07  global_step : 13.0  consumed_samples : 3584.0  train_backward_timing in s : 4.382133483886719e-05  train_step_timing in s : 3.931751012802124  epoch : 0 

Epoch 0: :  75%|███████▌  | 15/20 [01:17<00:25, v_num=None, reduced_train_loss=12.30, global_step=13.00, consumed_samples=3584.0, train_step_timing in s=3.930]
Epoch 0: :  75%|███████▌  | 15/20 [01:17<00:25, v_num=None, reduced_train_loss=12.30, global_step=14.00, consumed_samples=3840.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:27.884055 - 14 reduced_train_loss : 12.25192642211914  lr : 1.050000037139398e-06  global_step : 14.0  consumed_samples : 3840.0  train_backward_timing in s : 4.3964385986328126e-05  train_step_timing in s : 3.9308070659637453  epoch : 0 

Epoch 0: :  80%|████████  | 16/20 [01:21<00:20, v_num=None, reduced_train_loss=12.30, global_step=14.00, consumed_samples=3840.0, train_step_timing in s=3.930]
Epoch 0: :  80%|████████  | 16/20 [01:21<00:20, v_num=None, reduced_train_loss=12.20, global_step=15.00, consumed_samples=4096.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:31.823190 - 15 reduced_train_loss : 12.240994453430176  lr : 1.1250000397922122e-06  global_step : 15.0  consumed_samples : 4096.0  train_backward_timing in s : 4.363059997558594e-05  train_step_timing in s : 3.932813358306885  epoch : 0 

Epoch 0: :  85%|████████▌ | 17/20 [01:25<00:15, v_num=None, reduced_train_loss=12.20, global_step=15.00, consumed_samples=4096.0, train_step_timing in s=3.930]
Epoch 0: :  85%|████████▌ | 17/20 [01:25<00:15, v_num=None, reduced_train_loss=12.20, global_step=16.00, consumed_samples=4352.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:35.760644 - 16 reduced_train_loss : 12.213022232055664  lr : 1.2000000424450263e-06  global_step : 16.0  consumed_samples : 4352.0  train_backward_timing in s : 4.3725967407226564e-05  train_step_timing in s : 3.9324089527130126  epoch : 0 

Epoch 0: :  90%|█████████ | 18/20 [01:29<00:09, v_num=None, reduced_train_loss=12.20, global_step=16.00, consumed_samples=4352.0, train_step_timing in s=3.930]
Epoch 0: :  90%|█████████ | 18/20 [01:29<00:09, v_num=None, reduced_train_loss=12.20, global_step=17.00, consumed_samples=4608.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:39.691288 - 17 reduced_train_loss : 12.19826889038086  lr : 1.2750000450978405e-06  global_step : 17.0  consumed_samples : 4608.0  train_backward_timing in s : 4.315376281738281e-05  train_step_timing in s : 3.9313673973083496  epoch : 0 

Epoch 0: :  95%|█████████▌| 19/20 [01:33<00:04, v_num=None, reduced_train_loss=12.20, global_step=17.00, consumed_samples=4608.0, train_step_timing in s=3.930]
Epoch 0: :  95%|█████████▌| 19/20 [01:33<00:04, v_num=None, reduced_train_loss=12.20, global_step=18.00, consumed_samples=4864.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:43.614636 - 18 reduced_train_loss : 12.18254566192627  lr : 1.3500000477506546e-06  global_step : 18.0  consumed_samples : 4864.0  train_backward_timing in s : 4.167556762695313e-05  train_step_timing in s : 3.929069089889526  epoch : 0 

Epoch 0: : 100%|██████████| 20/20 [01:37<00:00, v_num=None, reduced_train_loss=12.20, global_step=18.00, consumed_samples=4864.0, train_step_timing in s=3.930]
Epoch 0: : 100%|██████████| 20/20 [01:37<00:00, v_num=None, reduced_train_loss=12.20, global_step=19.00, consumed_samples=5120.0, train_step_timing in s=3.930]DLL 2025-06-20 10:21:47.549146 - 19 reduced_train_loss : 12.159055709838867  lr : 1.4250000504034688e-06  global_step : 19.0  consumed_samples : 5120.0  train_backward_timing in s : 4.172325134277344e-05  train_step_timing in s : 3.929321765899658  epoch : 0 
`Trainer.fit` stopped: `max_steps=20` reached.

Epoch 0: : 100%|██████████| 20/20 [01:37<00:00, v_num=None, reduced_train_loss=12.20, global_step=19.00, consumed_samples=5120.0, train_step_timing in s=3.930]
Epoch 0: : 100%|██████████| 20/20 [01:37<00:00, v_num=None, reduced_train_loss=12.20, global_step=19.00, consumed_samples=5120.0, train_step_timing in s=3.930]
[NeMo I 2025-06-20 10:21:47 nemo_logging:393] train_step_timing in s: [22.71, 13.35, 10.2, 8.63, 7.69, 3.93, 3.92, 3.92, 3.92, 3.93, 3.93, 3.93, 3.93, 3.93, 3.93, 3.93, 3.93, 3.93, 3.93, 3.93]
[NeMo I 2025-06-20 10:21:47 nemo_logging:393] FLOPs measurement supported for ['gpt3', 'llama2', 'llama3', 'nemotron', 'mixtral', 'bert']
[NeMo E 2025-06-20 10:21:47 nemo_logging:417] Failed to calculate TFLOPs per sec per GPU.
    'Failed to extract valid model name from or missing FLOPs calculations for qwen3-30b_16gpu_tp1_pp2_cp1_vp1_ep4_gbs256_mbs4'
[NeMo I 2025-06-20 10:21:47 nemo_logging:393] TFLOPs per sec per GPU=-1.00
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
NCCL version 2.26.6+cuda12.8
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Processing events...
Generated:
	No reports were generated
Waiting on Torch PID 1454
Waiting on Torch PID 1455
Waiting on Torch PID 1456
Waiting on Torch PID 1457
Waiting on Torch PID 1458
Waiting on Torch PID 1459
Waiting on Torch PID 1460
Pod on gke-gke-a4-comp-a4-highgpu-8g-a4-high-89f97933-1rkx.us-central1-b.c.supercomputer-testing.internal is exiting