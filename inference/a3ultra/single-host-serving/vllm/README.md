# Single Node Model Serving with vLLM on A3 Ultra GKE Node Pool

This document outlines the steps to serve and benchmark various Large Language Models (LLMs) using [vLLM](https://github.com/vllm-project/vllm) framework on a single [A3 Ultra GKE Node pool](https://cloud.google.com/kubernetes-engine).

<a name="supported-models"></a>

## Supported Models

This recipe supports the deployment of the following models:

1.  [DeepSeek R1 671B](#serving-deepseek-r1-671b-model)
2.  [Llama 4 Maverick & Scout](#serving-llama-4-models)

The recipe is also compatible with similar sized large [models supported by vLLM](https://docs.vllm.ai/en/latest/models/supported_models.html).

<a name="table-of-contents"></a>

## Table of Contents

1.  [Orchestration and Deployment Tools](#orchestration-and-deployment-tools)
2.  [Test Environment](#test-environment)
3.  [Configure Environment](#configure-environment)
    *   [3.1. Clone the `gpu-recipes` Repository](#get-the-gpu-recipes-repository)
    *   [3.2. Configure Environment Settings](#configure-environment-settings)
    *   [3.3. Get Cluster Credentials](#get-cluster-credentials)
    *   [3.4. Get Hugging Face Token](#get-huggingface-token)
    *   [3.5. Create Hugging Face Kubernetes Secret](#create-hugging-face-kubernetes-secret)
    *   [3.6. Build and Push vLLM Docker Image](#build-and-push-vllm-docker-image)
4.  [Run the recipe](#run-the-recipe)
    *   [4.1. Serving DeepSeek R1 671B Model](#serving-deepseek-r1-671b-model)
        *   [4.1.1. Deploy DeepSeek R1 671B](#deploy-deepseek-r1-671b)
        *   [4.1.2. Interact with DeepSeek R1 671B](#interact-with-deepseek-r1-671b)
        *   [4.1.3. Benchmark DeepSeek R1 671B](#benchmark-deepseek-r1-671b)
    *   [4.2. Serving Llama 4 Models](#serving-llama-4-models)
        *   [4.2.1. Compatible Llama 4 Models](#compatible-llama-4-models)
        *   [4.2.2. Deploy Llama 4 Models](#deploy-llama-4-models)
        *   [4.2.3. Interact with Llama 4 Models](#interact-with-llama-4-models)
        *   [4.2.4. Benchmark Llama 4 Models](#benchmark-llama-4-models)
5.  [Monitoring deployment](#monitoring-deployment)
    *   [5.1. View Deployment Logs](#view-deployment-logs)
    *   [5.2. Verify Deployment Status](#verify-deployment-status)
6.  [Cleanup](#cleanup)

<a name="orchestration-and-deployment-tools"></a>

## 1. Orchestration and Deployment Tools

[Back to Top](#table-of-contents)

For this recipe, the following setup is used:

*   **Orchestration**: [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)
*   **Deployment Configuration and Deployment**: A Helm chart is used to configure and deploy the [Kubernetes Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). This deployment encapsulates the inference of the target LLM using vLLM framework. The chart generates manifests adhering to best practices for using RDMA Over Ethernet (RoCE) with Google Kubernetes Engine (GKE) on A3 Ultra node pool.

<a name="test-environment"></a>

## 2. Test environment

[Back to Top](#table-of-contents)

This recipe has been optimized for and tested with the following configuration:

- GKE cluster
    - [A regional standard cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview) version: 1.31.7-gke.1265000 or later.
    - A GPU node pool with 1 [a3-ultragpu-8g](https://cloud.google.com/compute/docs/gpus#h200-gpus) machine.
    - [Workload Identity Federation for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity) enabled.
    - [Cloud Storage FUSE CSI driver for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/cloud-storage-fuse-csi-driver) enabled.
    - [DCGM metrics](https://cloud.google.com/kubernetes-engine/docs/how-to/dcgm-metrics) enabled.
    - [Kueue](https://kueue.sigs.k8s.io/docs/reference/kueue.v1beta1/) and [JobSet](https://jobset.sigs.k8s.io/docs/overview/) APIs installed.
    - Kueue configured to support [Topology Aware Scheduling](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/).
- A regional Google Cloud Storage (GCS) bucket to store logs generated by the recipe runs.

To prepare the required environment, see
[GKE environment setup guide](../../../../docs/configuring-environment-gke-a3-ultra.md).

<a name="configure-environment"></a>

## 3. Configure Environment

[Back to Top](#table-of-contents)

The following steps configure the environment to run the recipe. These steps are common for serving the [models supported by this recipe](#supported-models).

<a name="get-the-gpu-recipes-repository"></a>

### 3.1. Clone the `gpu-recipes` Repository

Clone the `gpu-recipes` repository and set a reference to the recipe folder.

```bash
git clone https://github.com/ai-hypercomputer/gpu-recipes.git
cd gpu-recipes
export REPO_ROOT=`git rev-parse --show-toplevel`
export RECIPE_ROOT=$REPO_ROOT/inference/a3ultra/single-host-serving/vllm
```

<a name="configure-environment-settings"></a>

### 3.3. Configure Environment Settings

From your client, complete the following steps:

Set the environment variables to match your environment:

  ```bash
  export PROJECT_ID=<PROJECT_ID>
  export REGION=<REGION>
  export CLUSTER_REGION=<CLUSTER_REGION>
  export CLUSTER_NAME=<CLUSTER_NAME>
  export GCS_BUCKET=<GCS_BUCKET>
  export ARTIFACT_REGISTRY=<ARTIFACT_REGISTRY>
  export VLLM_IMAGE=vllm-openai
  export VLLM_VERSION=v0.8.5.post1
  ```

  Replace the following values:

  *   `<PROJECT_ID>`: your Google Cloud project ID.
  *   `<REGION>`: the region where you want to run Cloud Build (e.g., `us-central1`) job.
  *   `<CLUSTER_REGION>`: the region where your cluster is located.
  *   `<CLUSTER_NAME>`: the name of your GKE cluster.
  *   `<GCS_BUCKET>`: the name of your Cloud Storage bucket. *Do not include the `gs://` prefix*.
  *   `<ARTIFACT_REGISTRY>`: the full name of your Artifact Registry repository in the format: `LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY` (e.g., `us-central1-docker.pkg.dev/my-project/my-repo`).
  *   `<VLLM_VERSION>`: The vLLM version tag to be used for the image build  (e.g., `v0.8.5post1`).

Set the default project:

  ```bash
  gcloud config set project $PROJECT_ID
  ```

<a name="get-cluster-credentials"></a>

### 3.3. Get Cluster Credentials

From your client, get the credentials for your cluster:

```bash
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION
```

<a name="get-huggingface-token"></a>

### 3.4. Get Hugging Face Token

To access models through Hugging Face, you'll need a Hugging Face token.
  1.  Create a [Hugging Face account](https://huggingface.co/) if you don't have one.
  2.  For **gated models** like Llama 4, ensure you have requested and been granted access on Hugging Face before proceeding.
  3.  Generate an Access Token: Go to **Your Profile > Settings > Access Tokens**.
  4.  Select **New Token**.
  5.  Specify a Name and a Role of at least `Read`.
  6.  Select **Generate a token**.
  7.  Copy the generated token to your clipboard. You'll use this later.

<a name="create-hugging-face-kubernetes-secret"></a>

### 3.5. Create Hugging Face Kubernetes Secret

Create a Kubernetes Secret with your Hugging Face token to enable the job to download model checkpoints from Hugging Face.

```bash
export HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN> # Paste your token here
```

```bash
kubectl create secret generic hf-secret \
--from-literal=hf_api_token=${HF_TOKEN} \
--dry-run=client -o yaml | kubectl apply -f -
```

<a name="build-and-push-vllm-docker-image"></a>

### 3.6. Build and Push vLLM Docker Image to Cloud Artifact Registry

To build the container, complete the following steps from your client:

1.  Navigate to the vLLM docker source directory:
    ```bash
    cd $REPO_ROOT/src/docker/vllm
    ```

2.  Use Cloud Build to build and push the vLLM image to Cloud Artifact Registry repository. Ensure `VLLM_VERSION` is set (see [Configure Environment Settings](#configure-environment-settings)).

    Run the following command:

    ```bash
    gcloud builds submit --region=${REGION} \
        --config cloudbuild.yml \
        --substitutions _ARTIFACT_REGISTRY=$ARTIFACT_REGISTRY,_VLLM_IMAGE=$VLLM_IMAGE,_VLLM_VERSION=$VLLM_VERSION \
        --timeout "2h" \
        --machine-type=e2-highcpu-32 \
        --disk-size=1000 \
        --quiet \
        --async
    ```

    This command outputs the `build ID`.

3.  (Optional) You can monitor the build progress by streaming the logs for the `build ID`. Replace `<BUILD_ID>` with your build ID.

    ```bash
    BUILD_ID=<BUILD_ID>
    gcloud builds log $BUILD_ID --stream --region=$REGION
    ```

You are now ready to run the recipe and deploy the model.

<a name="run-the-recipe"></a>

## 4. Run the recipe

[Back to Top](#table-of-contents)

The following sections detail how to deploy and interact with models.

<a name="serving-deepseek-r1-671b-model"></a>

### 4.1. Serving DeepSeek R1 671B Model

[Back to Top](#table-of-contents)

This recipe serves the [DeepSeek R1 671B model](https://huggingface.co/deepseek-ai/DeepSeek-R1) using vLLM framework on a single A3 Ultra node in native FP8 mode.

Upon launching the vLLM server, it performs the following steps:

1.  Downloads the full DeepSeek R1 671B model checkpoints from Hugging Face.
2.  Loads the model checkpoints and applies vLLM optimizations.
3.  Server is ready to respond to requests.

<a name="deploy-deepseek-r1-671b"></a>

#### 4.1.1. Deploy DeepSeek R1 671B

1.  Install the helm chart to prepare and serve the model using vLLM framework:

    ```bash
    cd $RECIPE_ROOT
    helm install -f values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/vllm-launcher.sh \
    --set-file serving_config=$REPO_ROOT/src/frameworks/a3ultra/vllm-configs/deepseek-r1-671b.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.model.name=deepseek-ai/DeepSeek-R1 \
    --set workload.image=${ARTIFACT_REGISTRY}/${VLLM_IMAGE}:${VLLM_VERSION} \
    --set workload.framework=vllm \
    $USER-serving-deepseek-r1-model \
    $REPO_ROOT/src/helm-charts/a3ultra/inference-templates/deployment
    ```

    The deployment will be named `$USER-serving-deepseek-r1-model-serving` and the service `$USER-serving-deepseek-r1-model-svc`.

To view the deployment logs and verify the deployment status, navigate to [Monitoring Deployment](#monitoring-deployment).

<a name="interact-with-deepseek-r1-671b"></a>

#### 4.1.2. Interact with DeepSeek R1 671B model

1.  **Make API requests:**

    You can make an API request to send a chat message to the deployed DeepSeek R1 671B model and receive a JSON response from the model.

    ```bash
    kubectl exec -it deployment/$USER-serving-deepseek-r1-model -- \
    curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model":"deepseek-ai/DeepSeek-R1",
    "messages":[
        {
            "role":"system",
            "content":"You are a helpful AI assistant"
        },
        {
            "role":"user",
            "content":"How many r are there in strawberry ?"
        }
    ],
    "temperature":0.6,
    "top_p":0.95,
    "max_tokens":2048
    }'
    ```
    You should receive a JSON response from the model.

2.  **Stream chat responses:**

    To send messages and receive streaming text responses from the deployed DeepSeek R1 671B model:

    Open a new terminal session and establish a port-forwarding connection to the model's service to allow your local machine to communicate with the model server:

    ```bash
    kubectl port-forward svc/$USER-serving-deepseek-r1-model-svc 8000:8000
    ```

    Open a separate, second terminal session. In this new session, run the `stream_chat.sh` utility script:

    ```bash
    $RECIPE_ROOT/stream_chat.sh "Which is bigger 9.9 or 9.11 ?" "deepseek-ai/DeepSeek-R1"
    ```

<a name="benchmark-deepseek-r1-671b"></a>

#### 4.1.3. Benchmark DeepSeek R1 671B

To run inference benchmarks, run the [benchmarking tool from vLLM](https://docs.vllm.ai/references/benchmark_and_profiling.html) directly within the running deployment:

```bash
kubectl exec -it deployment/$USER-serving-deepseek-r1-model -- /bin/sh -c \
'mkdir -p /gcs/benchmark_logs/vllm && python3 /workspace/vllm/benchmarks/benchmark_serving.py \
  --model deepseek-ai/DeepSeek-R1 \
  --dataset-name random \
  --ignore-eos \
  --num-prompts 1100 \
  --random-input-len 1000 \
  --random-output-len 1000 \
  --port 8000 \
  --backend vllm'
```

Benchmark results are displayed in the logs.

<a name="serving-llama-4-models"></a>

### 4.2. Serving Llama 4 Models

[Back to Top](#table-of-contents)

This recipe serves various Llama 4 models using vLLM framework on a single A3 Ultra node in full precision (BF16).

<a name="compatible-llama-4-models"></a>

#### 4.2.1. Compatible Llama 4 Models

Llama 4 models are offered in various sizes and precision. This recipe is compatible with:

| Model Name                                                                                       | Total Size | Precision | Context Length |
| :----------------------------------------------------------------------------------------------- | :--------- | :-------- | :------------- |
| [Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)                   | 109B       | BF16      | 3.6M           |
| [Llama-4-Scout-17B-16E-Instruct](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct) | 109B       | BF16      | 3.6M           |
| [Llama-4-Maverick-17B-128E](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E)           | 400B       | BF16      | 1M             |
| [Llama-4-Maverick-17B-128E-Instruct](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct) | 400B       | BF16      | 1M             |

***Note:*** Llama 4 models are gated. Ensure you have access on Hugging Face.

<a name="deploy-llama-4-models"></a>

#### 4.2.2. Deploy Llama 4 Models

1.  Install the helm chart to serve the desired Llama 4 model using vLLM framework:

    a.  To serve `Llama-4-Scout-17B-16E` or `Llama-4-Scout-17B-16E-Instruct` (3.6M context):

      ```bash
      cd $RECIPE_ROOT
      helm install -f values.yaml \
      --set-file workload_launcher=$REPO_ROOT/src/launchers/vllm-launcher.sh \
      --set-file serving_config=$REPO_ROOT/src/frameworks/a3ultra/vllm-configs/llama4-scout.yaml \
      --set queue=${KUEUE_NAME} \
      --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
      --set workload.model.name=meta-llama/Llama-4-Scout-17B-16E-Instruct \
      --set workload.image=${ARTIFACT_REGISTRY}/${VLLM_IMAGE}:${VLLM_VERSION} \
      --set workload.framework=vllm \
      $USER-serving-llama-4-scout-model \
      $REPO_ROOT/src/helm-charts/a3ultra/inference-templates/deployment

      ```

    b.  To serve `Llama-4-Maverick-17B-128E` or `Llama-4-Maverick-17B-128E-Instruct` (1M context):
      ```bash
      cd $RECIPE_ROOT
      helm install -f values.yaml \
      --set-file workload_launcher=$REPO_ROOT/src/launchers/vllm-launcher.sh \
      --set-file serving_config=$REPO_ROOT/src/frameworks/a3ultra/vllm-configs/llama4-maverick.yaml \
      --set queue=${KUEUE_NAME} \
      --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
      --set workload.model.name=meta-llama/Llama-4-Maverick-17B-128E-Instruct \
      --set workload.image=${ARTIFACT_REGISTRY}/${VLLM_IMAGE}:${VLLM_VERSION} \
      --set workload.framework=vllm \
      $USER-serving-llama-4-maverick-model \
      $REPO_ROOT/src/helm-charts/a3ultra/inference-templates/deployment
      ```

    The deployment will be named like `$USER-serving-llama-4-scout-serving` and service as `$USER-serving-llama-4-scout-svc` depending on which model you deployed.

To view the deployment logs and verify the deployment status, navigate to [Monitoring Deployment](#monitoring-deployment).

<a name="interact-with-llama-4-models"></a>

#### 4.2.3. Interact with Llama 4 Models

1.  **Make API requests:**

    You can make an API request to send a chat message to the deployed model and receive a JSON response from the model.

    Here's the example for `Llama-4-Scout-17B-16E-Instruct` (assuming deployed as `$USER-serving-llama-4-scout`):

    ```bash
    kubectl exec -it deployment/$USER-serving-llama-4-scout-model -- \
    curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model":"meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "messages":[
          {
            "role":"system",
            "content":"You are a helpful AI assistant"
          },
          {
            "role":"user",
            "content":"What is the meaning of life?"
          }
      ],
      "temperature":0.6,
      "top_p":0.9,
      "max_tokens":128
    }'
    ```

    To get response from a different Llama 4 variant deployed, you can change the `"model"` field in the JSON payload.

2.  **Stream chat responses:**

    To send messages and receive streaming text responses from the deployed Llama4 model variant, for example Llama 4 Scout Instruct:

    Open a new terminal session and establish a port-forwarding connection to the model's service to allow your local machine to communicate with the model server:

    ```bash
    kubectl port-forward svc/$USER-serving-llama-4-scout-model-svc 8000:8000
    ```

    Open a separate, second terminal session. In this new session, run the `stream_chat.sh` utility script. Provide the model name as the second argument.

    ```bash
    $RECIPE_ROOT/stream_chat.sh "what is the meaning of life ?" "meta-llama/Llama-4-Scout-17B-16E-Instruct"
    ```

<a name="benchmark-llama-4-models"></a>

#### 4.2.4. Benchmark Llama 4 Models

To run inference benchmarks, run the [benchmarking tool from vLLM](https://docs.vllm.ai/references/benchmark_and_profiling.html) directly within the running deployment.
For example, to run benchmarks for the model `Llama-4-Scout-17B-16E-Instruct` with deployment `$USER-serving-llama-4-scout-serving`:

```bash
kubectl exec -it deployment/$USER-serving-llama-4-scout-model -- /bin/sh -c \
'mkdir -p /gcs/benchmark_logs/vllm && python3 /workspace/vllm/benchmarks/benchmark_serving.py \
  --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
  --dataset-name random \
  --ignore-eos \
  --num-prompts 1100 \
  --random-input-len 1000 \
  --random-output-len 1000 \
  --port 8000 \
  --backend vllm'
```

Benchmark results are displayed in the logs.

<a name="monitoring-deployment"></a>

## 5. Monitoring Deployment

[Back to Top](#table-of-contents)

After the model is deployed via Helm as described in the sections [above](#run-the-recipe), use the following steps to monitor the deployment and interact with the model. Replace `<deployment-name>` and `<service-name>` with the appropriate names from the model-specific deployment instructions (e.g., `$USER-serving-deepseek-r1-model-serving` and `$USER-serving-deepseek-r1-model-svc`).

<a name="view-deployment-logs"></a>

### 5.1. View Deployment Logs

```bash
kubectl logs -f deployment/<deployment-name>
```

You should see logs indicating vLLM server downloading/loading the model, and then starting the API server, similar to this:

```bash
INFO 03-03 11:46:53 api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000
INFO 03-03 11:46:53 launcher.py:23] Available routes are:
... (list of routes)
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

<a name="verify-deployment-status"></a>

### 5.2. Verify Deployment Status

```bash
kubectl get deployment/<deployment-name>
```

Check if the deployment `READY` column shows `1/1`.

<a name="cleanup"></a>

## 6. Cleanup

[Back to Top](#table-of-contents)

To clean up the resources created by the recipe:

1. **List deployed models** in the cluster:

    ```bash
    # list deployed models
    helm list $USER-serving-
    ```

2.  **Uninstall the helm charts** for any deployed models:

    ```bash
    # uninstall the deployed model
    helm uninstall <release_name>
    ```
    Replace `<release_name>` with the helm release names listed.

2.  **Delete the Kubernetes Secret:**

    ```bash
    kubectl delete secret hf-secret --ignore-not-found=true
    ```

3.  (Optional) Delete the built Docker image from Artifact Registry if no longer needed.
4.  (Optional) Delete Cloud Build logs.
5.  (Optional) Clean up files in your GCS bucket if benchmarking was performed.
6.  (Optional) Delete the [test environment](#test-environment) provisioned including GKE cluster.