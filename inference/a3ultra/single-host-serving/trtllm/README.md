# Single Node Model Serving with NVIDIA TensorRT-LLM (TRT-LLM) on A3 Ultra GKE Node Pool

This document outlines the steps to serve and benchmark various Large Language Models (LLMs) using [NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) on an [A3 Ultra GKE Node pool](https://cloud.google.com/kubernetes-engine).

<a name="supported-models"></a>

## Supported Models

This recipe supports the deployment of the following models:

1.  [Llama 3.1 405B](#serving-llama-3.1-405b-model)

The recipe is also compatible with similar sized large [models supported by NVIDIA TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html).

<a name="table-of-contents"></a>

## Table of Contents

1.  [Orchestration and Deployment Tools](#orchestration-and-deployment-tools)
2.  [Test Environment](#test-environment)
3.  [Configure Environment](#configure-environment)
    *   [3.1. Clone the `gpu-recipes` repository](#get-the-gpu-recipes-repository)
    *   [3.2. Configure Environment Settings](#configure-environment-settings)
    *   [3.3. Get Cluster Credentials](#get-cluster-credentials)
    *   [3.4. Get Hugging Face Token](#get-huggingface-token)
    *   [3.5. Create Hugging Face Kubernetes Secret](#create-hugging-face-kubernetes-secret)
    *   [3.6. Build and Push TensorRT-LLM Docker Image](#build-and-push-trtllm-docker-image)
4.  [Run the recipe](#run-the-recipe)
    *   [4.1. Inference benchmark for Llama 3.1 405B](#benchmarking-llama-3.1-405b-model)
5.  [Monitoring Deployment](#monitoring-deployment)
    *   [5.1. View Deployment Logs](#view-deployment-logs)
    *   [5.2. Verify Deployment Status](#verify-deployment-status)
6.  [Cleanup](#cleanup)

<a name="orchestration-and-deployment-tools"></a>

## 1. Orchestration and Deployment Tools

[Back to Top](#table-of-contents)

For this recipe, the following setup is used:

*   **Orchestration**: [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)
*   **Deployment Configuration and Deployment**: A Helm chart is used to configure and deploy the [Kubernetes Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). This deployment encapsulates the inference of the target LLM using NVIDIA TensorRT-LLM. The chart generates manifests adhering to best practices for using RDMA Over Ethernet (RoCE) with Google Kubernetes Engine (GKE) on A3 Ultra node pool.

<a name="test-environment"></a>

## 2. Test environment

[Back to Top](#table-of-contents)

This recipe has been optimized for and tested with the following configuration:

- GKE cluster
    - [A regional standard cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview) version: 1.31.7-gke.1265000 or later.
    - A GPU node pool with 1 [a3-ultragpu-8g](https://cloud.google.com/compute/docs/gpus#h200-gpus) machine.
    - [Workload Identity Federation for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity) enabled.
    - [Cloud Storage FUSE CSI driver for GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/cloud-storage-fuse-csi-driver) enabled.
    - [DCGM metrics](https://cloud.google.com/kubernetes-engine/docs/how-to/dcgm-metrics) enabled.
    - [Kueue](https://kueue.sigs.k8s.io/docs/reference/kueue.v1beta1/) and [JobSet](https://jobset.sigs.k8s.io/docs/overview/) APIs installed.
    - Kueue configured to support [Topology Aware Scheduling](https://kueue.sigs.k8s.io/docs/concepts/topology_aware_scheduling/).
- A regional Google Cloud Storage (GCS) bucket to store logs generated by the recipe runs.

To prepare the required environment, see
[GKE environment setup guide](../../../../docs/configuring-environment-gke-a3-ultra.md).

<a name="configure-environment"></a>

## 3. Configure Environment

[Back to Top](#table-of-contents)

The following steps configure the environment to run the recipe. These steps are common for serving the [models supported by this recipe](#supported-models).

<a name="get-the-gpu-recipes-repository"></a>

### 3.1. Clone the `gpu-recipes` Repository

Clone the `gpu-recipes` repository and set a reference to the recipe folder.

```bash
git clone https://github.com/ai-hypercomputer/gpu-recipes.git
cd gpu-recipes
export REPO_ROOT=`git rev-parse --show-toplevel`
export RECIPE_ROOT=$REPO_ROOT/inference/a3ultra/single-host-serving/trtllm
```

<a name="configure-environment-settings"></a>

### 3.2. Configure environment settings

From your client, complete the following steps:

Set the environment variables to match your environment:

  ```bash
  export PROJECT_ID=<PROJECT_ID>
  export REGION=<REGION>
  export CLUSTER_REGION=<CLUSTER_REGION>
  export CLUSTER_NAME=<CLUSTER_NAME>
  export GCS_BUCKET=<GCS_BUCKET>
  export ARTIFACT_REGISTRY=<ARTIFACT_REGISTRY>
  export TRTLLM_IMAGE=trtllm
  export TRTLLM_VERSION=v0.17.0
  export TRITON_SERVER_VERSION=25.02-trtllm-python-py3
  ```

  Replace the following values:

  *   `<PROJECT_ID>`: your Google Cloud project ID.
  *   `<REGION>`: the region where you want to run Cloud Build (e.g., `us-central1`) job.
  *   `<CLUSTER_REGION>`: the region where your cluster is located.
  *   `<CLUSTER_NAME>`: the name of your GKE cluster.
  *   `<GCS_BUCKET>`: the name of your Cloud Storage bucket. *Do not include the `gs://` prefix*.
  *   `<ARTIFACT_REGISTRY>`: the full name of your Artifact Registry repository in the format: `LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY` (e.g., `us-central1-docker.pkg.dev/my-project/my-repo`).
  *   `<TRTLLM_VERSION>`: The TensorRT-LLM version tag to be used for the image build  (e.g., `0.19.0`).
  *   `<TRITON_SERVER_VERSION>`: The NVIDIA Triton inference server version tag to be used as base image to build TensorRT-LLM  (e.g., `25.04-trtllm-python-py3`).

Set the default project:

  ```bash
  gcloud config set project $PROJECT_ID
  ```

<a name="get-cluster-credentials"></a>

### 3.3. Get Cluster Credentials

From your client, get the credentials for your cluster:

```bash
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION
```

<a name="get-huggingface-token"></a>

### 3.4. Get Hugging Face Token

To access models through Hugging Face, you'll need a Hugging Face token.
  1.  Create a [Hugging Face account](https://huggingface.co/) if you don't have one.
  2.  For **gated models** like Llama 4, ensure you have requested and been granted access on Hugging Face before proceeding.
  3.  Generate an Access Token: Go to **Your Profile > Settings > Access Tokens**.
  4.  Select **New Token**.
  5.  Specify a Name and a Role of at least `Read`.
  6.  Select **Generate a token**.
  7.  Copy the generated token to your clipboard. You will use this later.

<a name="create-hugging-face-kubernetes-secret"></a>

### 3.5. Create Hugging Face Kubernetes Secret

Create a Kubernetes Secret with your Hugging Face token to enable the job to download model checkpoints from Hugging Face.

```bash
export HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN> # Paste your token here
```

```bash
kubectl create secret generic hf-secret \
--from-literal=hf_api_token=${HF_TOKEN} \
--dry-run=client -o yaml | kubectl apply -f -
```

<a name="build-and-push-trtllm-docker-image"></a>

### 3.6. Build and Push TensorRT-LLM Docker Image to the Artifact Registry

To build the container, complete the following steps from your client:

1.  Navigate to the TensorRT-LLM docker source directory:
    ```bash
    cd $REPO_ROOT/src/docker/trtllm
    ```

2.  Use Cloud Build to build and push the TensorRT-LLM image to the Artifact Registry repository. Ensure `TRTLLM_VERSION` and `TRITON_SERVER_VERSION` are set (see [Configure Environment Settings](#configure-environment-settings)).

    Run the following command:

    ```bash
    gcloud builds submit --region=${REGION} \
        --config cloudbuild.yml \
        --substitutions _ARTIFACT_REGISTRY=$ARTIFACT_REGISTRY,_TRTLLM_IMAGE=$TRTLLM_IMAGE,_TRTLLM_VERSION=$TRTLLM_VERSION,_TRITON_SERVER_VERSION=$TRITON_SERVER_VERSION \
        --timeout "2h" \
        --machine-type=e2-highcpu-32 \
        --disk-size=1000 \
        --quiet \
        --async
    ```

    This command outputs the `build ID`.

3.  (Optional) You can monitor the build progress by streaming the logs for the `build ID`. Replace `<BUILD_ID>` with your build ID.

    ```bash
    BUILD_ID=<BUILD_ID>
    gcloud builds log $BUILD_ID --stream --region=$REGION
    ```

<a name="run-the-recipe"></a>

## 4. Run the recipe

[Back to Top](#table-of-contents)

The following sections detail how to run inference benchmark recipes.

<a name="benchmarking-llama-3.1-405b-model"></a>

### 4.1. Inference benchmark for Llama 3.1 405B Model

[Back to Top](#table-of-contents)

The recipe runs inference benchmark for [Llama 3.1 405B model](https://huggingface.co/meta-llama/Llama-3.1-405B) converting the Hugging Face checkpoint to [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) optimized format with FP8 quantization on a single A3 Ultra node.

The recipe does the following steps to run the benchmarking:

1. Download the full Llama 3.1 405B model checkpoints from [Hugging Face](https://huggingface.co/meta-llama/Llama-3.1-405B).
2. Convert the model checkpoints to TensorRT-LLM optimized format.
3. Build TensorRT-LLM engines for the model with FP8 quantization.
4. Run the throughput and/or latency benchmarking.

The recipe uses [`trtllm-bench`](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md), a command-line tool from NVIDIA to benchmark the performance of TensorRT-LLM engine. For more information about `trtllm-bench`, see the [TensorRT-LLM documentation](https://github.com/NVIDIA/TensorRT-LLM).

Install the helm chart to prepare and benchmark the model using [`trtllm-bench`](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md) tool:

    ```bash
    cd $RECIPE_ROOT
    helm install -f values.yaml \
    --set-file workload_launcher=$REPO_ROOT/src/launchers/trtllm-launcher.sh \
    --set-file serving_config=$REPO_ROOT/src/frameworks/a3ultra/trtllm-configs/llama-3.1-405b.yaml \
    --set queue=${KUEUE_NAME} \
    --set volumes.gcsMounts[0].bucketName=${GCS_BUCKET} \
    --set workload.model.name=meta-llama/Llama-3.1-405B \
    --set workload.image=${ARTIFACT_REGISTRY}/${TRTLLM_IMAGE}:${TRTLLM_VERSION} \
    --set workload.framework=trtllm \
    $USER-serving-llama-3-1-405b-model \
    $REPO_ROOT/src/helm-charts/a3ultra/inference-templates/deployment
    ```

The deployment will be named `$USER-serving-llama-3-1-405b-model` and the service `$USER-serving-llama-3-1-405b-model-svc`.

To view the deployment logs and verify the deployment status, navigate to [Monitoring Deployment](#monitoring-deployment).

**NOTE:** This helm chart is configured to run only a single benchmarking experiment for 30k requests for 128 tokens of input/output lengths. To run other experiments, you can add the various combinations provided in the [values.yaml](values.yaml) file.

<a name="monitoring-deployment"></a>

## 5. Monitoring Deployment

[Back to Top](#table-of-contents)

After the benchmarking job is deployed via Helm as described in the sections [above](#run-the-recipe), use the following steps to monitor the deployment. Replace `<deployment-name>` and `<service-name>` with the appropriate names from the model-specific deployment instructions (e.g., `$USER-serving-llama-3-1-405b-model` and `$USER-serving-llama-3-1-405b-model-svc`).

<a name="view-deployment-logs"></a>

### 5.1. View Deployment Logs

```bash
kubectl logs -f deployment/<deployment-name>
```

You should see logs indicating preparing the model, and then running the throughput benchmark test, similar to this:

  ```bash
  Running benchmark for meta-llama/Llama-3.1-405B with ISL=128, OSL=128, TP=8
  [TensorRT-LLM] TensorRT-LLM version: 0.16.0
  Parse safetensors files: 100%|██████████| 191/191 [00:01<00:00, 152.55it/s]
  [05/28/2025-22:40:28] [TRT-LLM] [I] Found dataset.
  [05/28/2025-22:40:29] [TRT-LLM] [I]
  ===========================================================
  = DATASET DETAILS
  ===========================================================
  Max Input Sequence Length:      128
  Max Output Sequence Length:     128
  Max Sequence Length:    256
  Target (Average) Input Sequence Length: 128
  Target (Average) Output Sequence Length:        128
  Number of Sequences:    30000
  ===========================================================


  [05/28/2025-22:40:29] [TRT-LLM] [I] Max batch size and max num tokens are not provided, use tuning heuristics or pre-defined setting from trtllm-bench.
  [05/28/2025-22:40:29] [TRT-LLM] [I] Estimated total available memory for KV cache: 717.36 GB
  [05/28/2025-22:40:29] [TRT-LLM] [I] Estimated total KV cache memory: 681.49 GB
  [05/28/2025-22:40:29] [TRT-LLM] [I] Estimated max number of requests in KV cache memory: 11076.91
  [05/28/2025-22:40:29] [TRT-LLM] [I] Estimated max batch size (after fine-tune): 4096
  [05/28/2025-22:40:29] [TRT-LLM] [I] Estimated max num tokens (after fine-tune): 8192
  [05/28/2025-22:40:29] [TRT-LLM] [I] Set dtype to bfloat16.
  [05/28/2025-22:40:29] [TRT-LLM] [I] Set multiple_profiles to True.
  [05/28/2025-22:40:29] [TRT-LLM] [I] Set use_paged_context_fmha to True.
  [05/28/2025-22:40:29] [TRT-LLM] [I] Set use_fp8_context_fmha to True.
  [05/28/2025-22:40:29] [TRT-LLM] [I] Specified dtype 'auto'; inferred dtype 'bfloat16'.
  [05/28/2025-22:40:29] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False
  [05/28/2025-22:40:29] [TRT-LLM] [I] Set nccl_plugin to None.
  [05/28/2025-22:40:29] [TRT-LLM] [I] start MpiSession with 8 workers
  [01/21/2025-02:18:20] [TRT-LLM] [I]
  Loading Model: [1/2]    Loading HF model to memory
  Loading checkpoint shards: 100%|██████████| 191/191 [02:56<00:00,  1.08it/s]
  Inserted 2649 quantizers
  current rank: 0, tp rank: 0, pp rank: 0
  Time: 1663.266s
  Loading Model: [2/2]    Building TRT-LLM engine
  Time: 555.845s
  Loading model done.
  Total latency: 2219.111s

  [05/28/2025-23:22:44] [TRT-LLM] [I]
  ===========================================================
  = ENGINE BUILD INFO
  ===========================================================
  Model Name:             meta-llama/Llama-3.1-405B
  Model Path:             /ssd/meta-llama/Llama-3.1-405B
  Workspace Directory:    /ssd
  Engine Directory:       /ssd/meta-llama/Llama-3.1-405B/tp_8_pp_1

  ===========================================================
  = ENGINE CONFIGURATION DETAILS
  ===========================================================
  Max Sequence Length:            256
  Max Batch Size:                 4096
  Max Num Tokens:                 8192
  Quantization:                   FP8
  KV Cache Dtype:                 FP8
  ===========================================================

  [05/28/2025-23:22:44] [TRT-LLM] [I]

  ===========================================================
  ENGINE SAVED: /ssd/meta-llama/Llama-3.1-405B/tp_8_pp_1
  ===========================================================

  [TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1

  [05/28/2025-23:24:31] [TRT-LLM] [I] Setting up for warmup...
  [05/28/2025-23:24:31] [TRT-LLM] [I] Running warmup.
  [05/28/2025-23:24:31] [TRT-LLM] [I] Starting benchmarking async task.
  [05/28/2025-23:24:31] [TRT-LLM] [I] Starting benchmark...
  [05/28/2025-23:24:31] [TRT-LLM] [I] Request submission complete. [count=2, time=0.0000s, rate=175330.94 req/s]
  [05/28/2025-23:24:36] [TRT-LLM] [I] Benchmark complete.
  [05/28/2025-23:24:36] [TRT-LLM] [I] Stopping LLM backend.
  [05/28/2025-23:24:36] [TRT-LLM] [I] Cancelling all 0 tasks to complete.
  [05/28/2025-23:24:36] [TRT-LLM] [I] All tasks cancelled.
  [05/28/2025-23:24:36] [TRT-LLM] [I] LLM Backend stopped.
  [05/28/2025-23:24:36] [TRT-LLM] [I] Warmup done.
  [05/28/2025-23:24:36] [TRT-LLM] [I] Starting benchmarking async task.
  [05/28/2025-23:24:36] [TRT-LLM] [I] Starting benchmark...
  [05/28/2025-23:24:36] [TRT-LLM] [I] Request submission complete. [count=30000, time=0.0121s, rate=2484491.39 req/s]
  [05/28/2025-23:40:58] [TRT-LLM] [I] Benchmark complete.
  [05/28/2025-23:40:58] [TRT-LLM] [I] Stopping LLM backend.
  [05/28/2025-23:40:58] [TRT-LLM] [I] Cancelling all 0 tasks to complete.
  [05/28/2025-23:40:58] [TRT-LLM] [I] All tasks cancelled.
  [05/28/2025-23:40:58] [TRT-LLM] [I] LLM Backend stopped.
  [05/28/2025-23:40:59] [TRT-LLM] [I]

  ===========================================================
  = ENGINE DETAILS
  ===========================================================
  Model:                  meta-llama/Llama-3.1-405B
  Engine Directory:       /ssd/meta-llama/Llama-3.1-405B/tp_8_pp_1
  TensorRT-LLM Version:   0.17.0.post1
  Dtype:                  bfloat16
  KV Cache Dtype:         FP8
  Quantization:           FP8
  Max Input Length:       256
  Max Sequence Length:    256

  ===========================================================
  = WORLD + RUNTIME INFORMATION
  ===========================================================
  TP Size:                8
  PP Size:                1
  Max Runtime Batch Size: 4096
  Max Runtime Tokens:     8192
  Scheduling Policy:      Guaranteed No Evict
  KV Memory Percentage:   95.00%
  Issue Rate (req/sec):   3.1860E+14

  ===========================================================
  = PERFORMANCE OVERVIEW
  ===========================================================
  Number of requests:             30000
  Average Input Length (tokens):  128.0000
  Average Output Length (tokens): 128.0000
  Token Throughput (tokens/sec):  3914.4509
  Request Throughput (req/sec):   30.5816
  Total Latency (ms):             980980.5121

  ===========================================================

  [05/28/2025-23:40:59] [TRT-LLM] [I] Thread proxy_dispatch_result_thread stopped.
  ```

<a name="verify-deployment-status"></a>

### 5.2. Verify Deployment Status

```bash
kubectl get deployment/<deployment-name>
```

Check if the deployment `READY` column shows `1/1`.

<a name="cleanup"></a>

## 6. Cleanup

[Back to Top](#table-of-contents)

To clean up the resources created by the recipe:

1. **List deployed models** in the cluster:

    ```bash
    # list deployed models
    helm list $USER-serving-
    ```

2.  **Uninstall the helm charts** for any deployed models:

    ```bash
    # uninstall the deployed model
    helm uninstall <release_name>
    ```
    Replace `<release_name>` with the helm release names listed.

2.  **Delete the Kubernetes Secret:**

    ```bash
    kubectl delete secret hf-secret --ignore-not-found=true
    ```

3.  (Optional) Delete the built Docker image from Artifact Registry if no longer needed.
4.  (Optional) Delete Cloud Build logs.
5.  (Optional) Clean up files in your GCS bucket if benchmarking was performed.
6.  (Optional) Delete the [test environment](#test-environment) provisioned including GKE cluster.